<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico">
  <link rel="mask-icon" href="/images/avatar.jpeg" color="#222">
  <meta name="google-site-verification" content="d2tAnXrr34Y8JpdeBBtXb3s6KJuDmbZk7Kq1mlFW3qs">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"vnicl.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.19.2","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":true,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="一个理想主义者 · 空想家 · LOSER">
<meta property="og:type" content="website">
<meta property="og:title" content="攻城狮也文艺">
<meta property="og:url" content="https://vnicl.github.io/index.html">
<meta property="og:site_name" content="攻城狮也文艺">
<meta property="og:description" content="一个理想主义者 · 空想家 · LOSER">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Iceberg">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://vnicl.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>攻城狮也文艺 - 渺小但执着</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">攻城狮也文艺</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">渺小但执着</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Iceberg"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">Iceberg</p>
  <div class="site-description" itemprop="description">一个理想主义者 · 空想家 · LOSER</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLW5kLzQuMC9kZWVkLnpoLWhhbnM="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_nd.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2018/10/10/%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/10/10/%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">曲线拟合原理和实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2018-10-10 16:37:43" itemprop="dateCreated datePublished" datetime="2018-10-10T16:37:43+08:00">2018-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2018-10-11 16:01:12" itemprop="dateModified" datetime="2018-10-11T16:01:12+08:00">2018-10-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">算法应用</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.8k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><hr>
<p>  现实世界中，变量间未必都有线性关系，如服药后血药浓度与时间的关系、疾病疗效与疗程长短的关系和毒物剂量与致死率的关系等常呈曲线关系。曲线拟合（Curve Fitting）是指选择适当的曲线类型来拟合观测数据，并用拟合的<code>曲线方程</code>分析两变量间的关系。</p>
<p>  实际工作中，我们通过统计或实验观察得到一组可以在二维坐标系中表示的点$(x_i, y_i)$，其中$x_i$为自变量，$y_i$为因变量，用二维散点图的方式绘制出来如下</p>
<center><img data-src="/2018/10/10/%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/sdt.png" class="" title="以散点图的方式表示在二维坐标系中的数据"></center>

<p>曲线拟合的目的即寻找可以连续并近似地刻画或比拟该二维坐标系中所有离散点的一条曲线，表示所有离散点和坐标系之间直接的一种函数关系$y_i&#x3D;f(x_i, c_m)$。该曲线函数即我们需要找到的拟合函数，包括自变量$x_i$和$m$个拟合系数$c_m$组成，不同的拟合方式会有不同的拟合系数产生，所以曲线拟合就是求最优的拟合系数的问题。该拟合函数也被称为<code>拟合模型</code>。</p>
<p>  在日常工作生产中一般使用的拟合函数为<code>线形拟合</code>、<code>对数</code>、<code>多项式</code>、<code>幂函数</code>、<code>指数</code>和<code>移动平均</code>等几种，不同的拟合方式最终的结果亦不相同，需要根据提供离散数据组的分布和业务状况进行选择。其实很多数据软件中通常已经封装了曲线拟合的功能，以下展示分别为样例离散数据在<code>MacOs</code>系统中的<code>Number表格</code>实现的效果。</p>
<center><img data-src="/2018/10/10/%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/slqxnh.png" class="" title="曲线拟合示例"></center>

<p>  由于该拟合模型会有不同个数的拟合系数$c_m$，并且最终的拟合模型也只能近似的拟合所有离散数据点，所以就涉及到一个最优化解的过程，故曲线拟合也是一个求最优结果的优化问题。最优化理论在人工智能领域频繁的被提及，所有机器学习相关算法其实都是最优化问题。在工程上常用的求最优解的方式有两种，<code>梯度</code>和<code>最小二乘法</code>，梯度的概念在之前的几篇文章中都有提及，如<a href="/2017/05/09/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" title="支持向量机 (SVM)">支持向量机 (SVM)</a>等。在做拟合实践之前我们先来了解一下<code>最小二乘法</code>。</p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><hr>
<p>  <code>最小二乘法</code>是一种优化技术，其中<code>最小</code>表现为在所求内容（可以理解为算法预测结果）与真实数据的误差达到最小，该优化技术也由该点出发，常用的误差计算方式为<code>残差平方和</code>，即<br>$$<br>\epsilon &#x3D; \sum_{i&#x3D;1}^n{(f(x_i) - y_i)^2}<br>$$<br>其中$f(x_i)$表示预测值，最小二乘法是通过最小化误差的平方和寻找数据的最佳函数匹配；而<code>二乘</code>的意思个人理解为在该优化技术最后会将待求解的最佳函数以矩阵的方式表现为两个矩阵的乘法，以方便最后<code>超参数</code>或<code>拟合系数</code>的计算。</p>
<p>  假定我们当前有一个超定方程组<br>$$<br>\begin{equation}<br>\sum_{j&#x3D;1}^nx_i^j\alpha_j &#x3D; y_i \<br>\begin{pmatrix}<br>i&#x3D;1,2,3\cdots m \<br>j&#x3D;1,2,3\cdots n \<br>\end{pmatrix}<br>\end{equation}<br>$$<br>可以这样理解该超定方程组，$x_i^j$表示一个包含$n$个特征的$m$个样本数据如下：<br>$$<br>\begin{pmatrix}<br>x_1^1 &amp; x_1^2 &amp; \cdots &amp; x_1^n \<br>x_2^1 &amp; x_2^2 &amp; \cdots &amp; x_2^n \<br>\vdots &amp; \ddots &amp; \vdots  &amp; \vdots \<br>x_m^1 &amp; x_m^2 &amp; \cdots &amp; x_m^n \<br>\end{pmatrix}<br>$$<br>$\alpha_j$表示待求的最优系数，$y_i$则是因变量，方程组如下：<br>$$<br>\begin{pmatrix}<br>x_1^1\times\alpha_1 + x_1^2\times\alpha_2 &amp; \cdots &amp; x_1^n\times\alpha_n &#x3D; y_1 \<br>x_2^1\times\alpha_1 + x_2^2\times\alpha_2 &amp; \cdots &amp; x_2^n\times\alpha_n &#x3D; y_2 \<br>\vdots &amp; \ddots  &amp; \vdots \<br>x_m^1\times\alpha_1 + x_m^2\times\alpha_2 &amp; \cdots &amp; x_m^n\times\alpha_n &#x3D; y_m \<br>\end{pmatrix}<br>$$<br>其中$m$表示有$m$个等式，$n$表示有$n$个未知数$\alpha$，并且$m \gt n$，根据向量的特性，将该超定方程组向量化后为$x\alpha&#x3D;y$（二乘）<br>$$<br>x&#x3D;\begin{bmatrix}<br>x_1^1 &amp; x_1^2 &amp; \cdots &amp; x_1^n \<br>x_2^1 &amp; x_2^2 &amp; \cdots &amp; x_2^n \<br>\vdots &amp; \ddots &amp; \vdots  &amp; \vdots \<br>x_m^1 &amp; x_m^2 &amp; \cdots &amp; x_m^n \<br>\end{bmatrix}<br>,\alpha&#x3D;\begin{bmatrix}<br>\alpha_1 \<br>\alpha_2 \<br>\vdots \<br>\alpha_n \<br>\end{bmatrix}<br>,y&#x3D;\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n \<br>\end{bmatrix}<br>$$<br>接下来我们用最小二乘法求解该方程组，根据上面提到的<code>最小</code>的最优理论，运用残差平方和则有<br>$$<br>\epsilon(\alpha) &#x3D; ||x\alpha-y||^2<br>$$<br>我们的目的是使的$\epsilon(\alpha)$最小，对该函数进行微分求极值则有<br>$$<br>x^Tx\times\hat \alpha&#x3D;x^Ty<br>$$<br>其中$\hat \alpha$表示当$\alpha&#x3D;\hat \alpha$时，函数$\epsilon(\alpha) $取最小值，<br>$$<br>\hat \alpha &#x3D; argmin(\epsilon(\alpha) )<br>$$<br>根据矩阵的特点，如果矩阵$x^Tx$非奇异则$\hat \alpha$有唯一解<br>$$<br>\hat \alpha&#x3D;(x^Tx)^{-1}x^Ty<br>$$</p>
<h2 id="线性拟合"><a href="#线性拟合" class="headerlink" title="线性拟合"></a>线性拟合</h2><hr>
<h1 id="所谓线性拟合意思为考虑最终的拟合模型表达式为-f-a-b-a-bx-假设给定一批离散数据如-begin-equation-x-i-y-i-i-1-2-3-cdots-m-end-equation-即表示以-m-个数据点做曲线拟合。运用上面提到的最小二乘法的最优技术，使平方误差最小-begin-equation-begin-split-epsilon-a-b-sum-i-1-m-f-a-b-y-i-2-sum-i-1-m-a-bx-i-y-i-2-end-split-end-equation-运用微积分知识，计算-epsilon-a-b-的极小值要满足如下（分别对-a-b-求偏导数）-begin-equation-begin-split-frac-partial-epsilon-a-b-partial-a-2-sum-i-1-m-a-bx-i-y-i-sum-i-1-m-a-bx-i-y-i-0-frac-partial-epsilon-a-b-partial-a-2-sum-i-1-m-a-bx-i-y-i-x-i-sum-i-1-m-a-bx-i-y-i-x-i-0-end-split-end-equation-将上面的内容整理成矩阵的形式（二乘）如下-begin-pmatrix-m-sum-i-1-m-x-i-sum-i-1-mx-i-sum-i-1-mx-i-2-end-pmatrix-times-begin-pmatrix-a-b-end-pmatrix"><a href="#所谓线性拟合意思为考虑最终的拟合模型表达式为-f-a-b-a-bx-假设给定一批离散数据如-begin-equation-x-i-y-i-i-1-2-3-cdots-m-end-equation-即表示以-m-个数据点做曲线拟合。运用上面提到的最小二乘法的最优技术，使平方误差最小-begin-equation-begin-split-epsilon-a-b-sum-i-1-m-f-a-b-y-i-2-sum-i-1-m-a-bx-i-y-i-2-end-split-end-equation-运用微积分知识，计算-epsilon-a-b-的极小值要满足如下（分别对-a-b-求偏导数）-begin-equation-begin-split-frac-partial-epsilon-a-b-partial-a-2-sum-i-1-m-a-bx-i-y-i-sum-i-1-m-a-bx-i-y-i-0-frac-partial-epsilon-a-b-partial-a-2-sum-i-1-m-a-bx-i-y-i-x-i-sum-i-1-m-a-bx-i-y-i-x-i-0-end-split-end-equation-将上面的内容整理成矩阵的形式（二乘）如下-begin-pmatrix-m-sum-i-1-m-x-i-sum-i-1-mx-i-sum-i-1-mx-i-2-end-pmatrix-times-begin-pmatrix-a-b-end-pmatrix" class="headerlink" title="  所谓线性拟合意思为考虑最终的拟合模型表达式为$$f(a,b)&#x3D;a+bx$$假设给定一批离散数据如$$\begin{equation}(x_i,y_i) \i&#x3D;1,2,3\cdots m \\end{equation}$$即表示以$m$个数据点做曲线拟合。运用上面提到的最小二乘法的最优技术，使平方误差最小$$\begin{equation}\begin{split}\epsilon(a,b) &amp;&#x3D;\sum_{i&#x3D;1}^m(f(a,b) - y_i)^2 \&amp;&#x3D;\sum_{i&#x3D;1}^m(a+bx_i- y_i)^2 \\end{split}\end{equation}$$运用微积分知识，计算$\epsilon(a,b)$的极小值要满足如下（分别对$a,b$求偏导数）$$\begin{equation}\begin{split}\frac {\partial \epsilon(a,b)} {\partial a} &amp;&#x3D;2\sum_{i&#x3D;1}^m(a+bx_i-y_i) \&amp;&#x3D;\sum_{i&#x3D;1}^m(a+bx_i-y_i) \&amp;&#x3D;0 \\frac {\partial \epsilon(a,b)} {\partial a} &amp;&#x3D;2\sum_{i&#x3D;1}^m(a+bx_i-y_i)x_i \&amp;&#x3D;\sum_{i&#x3D;1}^m(a+bx_i-y_i)x_i \&amp;&#x3D;0 \\end{split}\end{equation}$$将上面的内容整理成矩阵的形式（二乘）如下$$\begin{pmatrix}m &amp; \sum_{i&#x3D;1}^m  x_i  \\sum_{i&#x3D;1}^mx_i &amp; \sum_{i&#x3D;1}^mx_i^2 \\end{pmatrix}\times\begin{pmatrix}a \b \\end{pmatrix}"></a>  所谓<code>线性拟合</code>意思为考虑最终的拟合模型表达式为<br>$$<br>f(a,b)&#x3D;a+bx<br>$$<br>假设给定一批离散数据如<br>$$<br>\begin{equation}<br>(x_i,y_i) \<br>i&#x3D;1,2,3\cdots m \<br>\end{equation}<br>$$<br>即表示以$m$个数据点做曲线拟合。运用上面提到的最小二乘法的最优技术，使平方误差最小<br>$$<br>\begin{equation}\begin{split}<br>\epsilon(a,b) &amp;&#x3D;\sum_{i&#x3D;1}^m(f(a,b) - y_i)^2 \<br>&amp;&#x3D;\sum_{i&#x3D;1}^m(a+bx_i- y_i)^2 \<br>\end{split}\end{equation}<br>$$<br>运用微积分知识，计算$\epsilon(a,b)$的极小值要满足如下（分别对$a,b$求偏导数）<br>$$<br>\begin{equation}\begin{split}<br>\frac {\partial \epsilon(a,b)} {\partial a} &amp;&#x3D;2\sum_{i&#x3D;1}^m(a+bx_i-y_i) \<br>&amp;&#x3D;\sum_{i&#x3D;1}^m(a+bx_i-y_i) \<br>&amp;&#x3D;0 \<br>\frac {\partial \epsilon(a,b)} {\partial a} &amp;&#x3D;2\sum_{i&#x3D;1}^m(a+bx_i-y_i)x_i \<br>&amp;&#x3D;\sum_{i&#x3D;1}^m(a+bx_i-y_i)x_i \<br>&amp;&#x3D;0 \<br>\end{split}\end{equation}<br>$$<br>将上面的内容整理成矩阵的形式（二乘）如下<br>$$<br>\begin{pmatrix}<br>m &amp; \sum_{i&#x3D;1}^m  x_i  \<br>\sum_{i&#x3D;1}^mx_i &amp; \sum_{i&#x3D;1}^mx_i^2 \<br>\end{pmatrix}<br>\times<br>\begin{pmatrix}<br>a \<br>b \<br>\end{pmatrix}</h1><p>\begin{pmatrix}<br>\sum_{i&#x3D;1}^my_i  \<br>\sum_{i&#x3D;1}^mx_iy_i \<br>\end{pmatrix}<br>$$<br>用消元法或克莱姆方法解出方程<br>$$<br>\begin{equation}\begin{split}<br>a &#x3D; &amp; {\sum_{i&#x3D;1}^my_i\sum_{i&#x3D;1}^mx_i^2 - \sum_{i&#x3D;1}^mx_i\sum_{i&#x3D;1}^mx_iy_i} \div {m\sum_{i&#x3D;1}^mx_i^2-(\sum_{i&#x3D;1}^mx_i)^2} \<br>b &#x3D; &amp; {m\sum_{i&#x3D;1}^mx_iy_i - \sum_{i&#x3D;1}^mx_i\sum_{i&#x3D;1}^my_i} \div {m\sum_{i&#x3D;1}^mx_i^2-(\sum_{i&#x3D;1}^mx_i)^2}<br>\end{split}\end{equation}<br>$$<br>其中<br>$$<br>\begin{equation}\begin{split}<br>m&#x3D;&amp; 离散点的数量\<br>\sum_{i&#x3D;1}^mx_i&#x3D;&amp; x_1+x_2+x_3 \cdots+x_i \<br>\sum_{i&#x3D;1}^my_i&#x3D;&amp; y_1+y_2+y_3+\cdots+y_i\<br>\sum_{i&#x3D;1}^mx_i^2&#x3D;&amp; x_1^2 + x_2^2+x_3^2 + \cdots+x_i^2\<br>\sum_{i&#x3D;1}^mx_iy_i&#x3D;&amp; x_1y_1+x_2y_2+x_3y_3+\cdots + x_iy_i\<br>\end{split}\end{equation}<br>$$</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">double</span>[] x = <span class="keyword">new</span> <span class="title class_">double</span>[]&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, ··· ···, m&#125;;</span><br><span class="line"><span class="type">double</span>[] y = <span class="keyword">new</span> <span class="title class_">double</span>[]&#123;<span class="number">0.628289474</span>, <span class="number">0.537257824</span>, <span class="number">0.510469314</span>, ··· ···, <span class="number">0.441730635</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">double</span> <span class="variable">sumX</span> <span class="operator">=</span> <span class="number">0d</span>;</span><br><span class="line"><span class="type">double</span> <span class="variable">sumY</span> <span class="operator">=</span> <span class="number">0d</span>;</span><br><span class="line"><span class="type">double</span> <span class="variable">sumXX</span> <span class="operator">=</span> <span class="number">0d</span>;</span><br><span class="line"><span class="type">double</span> <span class="variable">sumXY</span> <span class="operator">=</span> <span class="number">0d</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; x.length; i ++)&#123;</span><br><span class="line">    sumX += x[i];</span><br><span class="line">  	sumY += y[i];</span><br><span class="line">  	sumXX += Math.pow(x[i], <span class="number">2d</span>);</span><br><span class="line">  	sumXY += x[i] * y[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">double</span> <span class="variable">a</span> <span class="operator">=</span> (sumY * sumXX - sumX * sumXY) / (x.length * sumXX - Math.pow(sumX, <span class="number">2d</span>));</span><br><span class="line"><span class="type">double</span> <span class="variable">b</span> <span class="operator">=</span> (x.length * sumXY - sumX * sumY) / (x.length * sumXX - Math.pow(sumX, <span class="number">2d</span>));</span><br><span class="line"><span class="comment">//f(x) = a + bx</span></span><br><span class="line">System.out.println(<span class="string">&quot;f(x) = &quot;</span> + a + <span class="string">&quot; + &quot;</span> + b + <span class="string">&quot;x&quot;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="幂函数拟合"><a href="#幂函数拟合" class="headerlink" title="幂函数拟合"></a>幂函数拟合</h2><hr>
<p>待出稿</p>
<h2 id="多项式拟合"><a href="#多项式拟合" class="headerlink" title="多项式拟合"></a>多项式拟合</h2><hr>
<p>待出稿</p>
<h2 id="拟合优度"><a href="#拟合优度" class="headerlink" title="拟合优度"></a>拟合优度</h2><hr>
<p>待出稿</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/09/15/Play-Framework%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/09/15/Play-Framework%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">Play Framework入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-09-15 14:44:30" itemprop="dateCreated datePublished" datetime="2017-09-15T14:44:30+08:00">2017-09-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2017-09-20 14:20:34" itemprop="dateModified" datetime="2017-09-20T14:20:34+08:00">2017-09-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/JAVA/" itemprop="url" rel="index"><span itemprop="name">JAVA</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>690</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>博主环境（本文只讨论Play在Java下的开发）</p>
<p>JDK：1.8</p>
<p>Play：2.6</p>
</blockquote>
<h2 id="Play是什么？"><a href="#Play是什么？" class="headerlink" title="Play是什么？"></a>Play是什么？</h2><hr>
<p>  Play是一种高效率的Java和Scala程序语言的Web应用开发框架，Play自身继承了所有Web应用程序开发需要的组件和API。</p>
<p>  Play框架被官方描述为轻量级、无状态、网络友好的框架，并且具有可预测和消耗最少资源，适用于可扩展的应用程序。</p>
<h2 id="创建一个新应用"><a href="#创建一个新应用" class="headerlink" title="创建一个新应用"></a>创建一个新应用</h2><hr>
<p>  Play框架在<code>2.6</code>版本中是由sbt管理的，并且sbt工具版本需要<code>0.13.13</code>或更高，并且JDK版本必须为<code>1.8</code>。</p>
<figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbt <span class="keyword">new</span> playframework/<span class="keyword">play</span>-java-seed.g8</span><br></pre></td></tr></table></figure>

<p>初次运行该命令可能会下载许多第三方依赖，如果失败需要重试，直到下面的步骤</p>
<figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">This template generates a <span class="keyword">Play</span> Java project </span><br><span class="line"># 项目名称</span><br><span class="line"><span class="keyword">name</span> [<span class="keyword">play</span>-java-seed]: HelloPlay</span><br><span class="line"># 组织名称</span><br><span class="line">organization [<span class="keyword">com</span>.example]: <span class="keyword">com</span>.vnicl</span><br><span class="line"># 使用scala的版本，直接回车则使用默认版本</span><br><span class="line">scala_version [<span class="number">2.12</span>.<span class="number">2</span>]: </span><br><span class="line"># 使用<span class="keyword">play</span>的版本，直接回车则使用默认版本</span><br><span class="line">play_version [<span class="number">2.6</span>.<span class="number">5</span>]: </span><br></pre></td></tr></table></figure>

<p>然后进入项目根目录，运行<code>run</code>命令即可在浏览器中预览</p>
<figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ./HelloPlay</span><br><span class="line"># 当前环境JDK版本必须为<span class="number">1.8</span>，否则会抛出错误：Unsupported major.minor version <span class="number">52.0</span></span><br><span class="line">sbt <span class="keyword">run</span></span><br></pre></td></tr></table></figure>

<p>也可以进入sbt控制台进行预览</p>
<figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ./HelloPlay</span><br><span class="line">sbt</span><br><span class="line"># 指定Web服务端口，默认端口为<span class="number">9000</span></span><br><span class="line">[HelloPlay] $ <span class="keyword">run</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure>

<h2 id="项目结构解析"><a href="#项目结构解析" class="headerlink" title="项目结构解析"></a>项目结构解析</h2><hr>
<h3 id="app"><a href="#app" class="headerlink" title="app"></a>app</h3><p>  该目录包含可执行的项目源代码，默认里面包括三个包</p>
<table>
<thead>
<tr>
<th>目录</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>app&#x2F;controllers</td>
<td>所有类库源代码</td>
</tr>
<tr>
<td>app&#x2F;models</td>
<td>模型文件</td>
</tr>
<tr>
<td>app&#x2F;views</td>
<td>视图文件</td>
</tr>
</tbody></table>
<h3 id="public"><a href="#public" class="headerlink" title="public"></a>public</h3><p>  该目录包括所有的Web服务需要的静态资源，默认爆款三个文件夹</p>
<table>
<thead>
<tr>
<th>目录</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>public&#x2F;images</td>
<td>静态图片资源</td>
</tr>
<tr>
<td>public&#x2F;javascripts</td>
<td>视图文件需要引入的JS资源</td>
</tr>
<tr>
<td>public&#x2F;stylesheets</td>
<td>视图文件需要引入的CSS样式表资源</td>
</tr>
</tbody></table>
<h3 id="conf"><a href="#conf" class="headerlink" title="conf"></a>conf</h3><p>  该目录包含应用程序的配置文件，主要为下面两个配置文件</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>conf&#x2F;application.conf</td>
<td>应用的主配置文件</td>
</tr>
<tr>
<td>conf&#x2F;routes</td>
<td>静态路由定义文件</td>
</tr>
</tbody></table>
<h3 id="lib"><a href="#lib" class="headerlink" title="lib"></a>lib</h3><p>  此目录包含没有通过sbt托管的依赖库，即任何第三方开发的Jar包。该目录下的所有Jar文件会自动添加到应用程序的类路径中。</p>
<h2 id="Play中的MVC"><a href="#Play中的MVC" class="headerlink" title="Play中的MVC"></a>Play中的MVC</h2><hr>
<p>  跟常规的MVC框架类似，在Play中，一个自定义的扩展类被称为一个控制器，该类中可以定义处理Web请求的方法并返回一个请求结果返回。</p>
<p>  一个控制器须要继承自<code>play.mvc.Controller</code>，<code>Controller</code>类中实现了常用的以下方法供使用</p>
<p>继承了<code>play.mvn.Results</code>，</p>
<p>控制器中的一个操作必须返回一个<code>play.mvc.Result</code>对象作为该操作的响应。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" class="post-title-link" itemprop="url">机器学习中的特征工程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-06-13 15:03:37" itemprop="dateCreated datePublished" datetime="2017-06-13T15:03:37+08:00">2017-06-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2017-07-13 10:29:00" itemprop="dateModified" datetime="2017-07-13T10:29:00+08:00">2017-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.2k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  在机器学习算法中，算法的实现过程固然重要，但是作为算法训练的输入也决定算法好坏和算法预测结果准确性的重要因素。我们知道作为机器学习算法的输入是训练样本集，训练样本集中包含很多样本，并且每个样本都会有很多的特征作为机器学习算法的计算依据，所以在样本特征的选取和处理也显得格外重要。</p>
<p>  特征工程（Feature Engineering）需要解决的问题就是将传统的数据内容处理为对算法有用的特征内容，包括文档的切词、值的二值化、分箱／分区、进行离散化等处理方式，本文重点介绍在机器学习领域常见的一些特征处理，包括特征的提取、转化和选择。</p>
<p>  在进行机器学习算法训练前的特征选择时，我们需要尽可能的考虑业务规则，即基于业务的理解去选择是否使用一个特征进行机器学习算法的训练，因为在算法训练的过程中，我们除了需要考虑算法的准确性外还应该考虑算法的训练和计算性能，如果选择一个无关的特征可能会对算法结果有影响和影响算法性能。此外在特征选择时我们还应该考虑特征在算法训练样本集中的覆盖率、准确率、信息量评估、差别性评估等，如果我们打算使用的特征需要及时去获取，那么我们还应该考虑该特征的获取难度，即对选择特征的可用性评估，我们不能捡了芝麻而丢了西瓜啊。</p>
<p>  接下来我们要做的就是进行特征的处理，包括数据的清洗和数据的预处理，根据我们选择的算法模型进行特征的处理工作，使特征更适合我们选择的算法模型。下面介绍几种常见的特征处理方式和实现方案：</p>
<h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><hr>
<p>  分词是机器学习算法中简单常用的操作，例如将文档样本生成向量特征的时候经常会使用分词器进行文档的分词切割，常用的方法就是以空格作为分隔符将文档分隔成单词群。</p>
<h3 id="Spark-Mllib中实现"><a href="#Spark-Mllib中实现" class="headerlink" title="Spark Mllib中实现"></a>Spark Mllib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkTokenizer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkTokenizer&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="string">&quot;Hi I heard about Spark&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="string">&quot;I wish Java could use case classes&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="string">&quot;Logistic regression models are neat&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;sentence&quot;</span>, DataTypes.StringType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; documentDF = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 使用 Tokenizer 分词器以空格作为分隔符分隔文档</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="type">Tokenizer</span> <span class="variable">tokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Tokenizer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;sentence&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;words&quot;</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 使用 RegexTokenizer 分词器分词</span></span><br><span class="line"><span class="comment">         * 可以使用正则表达式的方式作为分隔符</span></span><br><span class="line"><span class="comment">         * 当设置 gaps 参数为 false 时正则表达式作用为提取关键词</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="type">RegexTokenizer</span> <span class="variable">regexTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RegexTokenizer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;sentence&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;words&quot;</span>)</span><br><span class="line">                <span class="comment">//设置 gaps 参数</span></span><br><span class="line">                .setGaps(<span class="literal">false</span>)</span><br><span class="line">                .setPattern(<span class="string">&quot;\\\\W&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; wordsDF = tokenizer.transform(documentDF);</span><br><span class="line">        wordsDF.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="停用词过滤"><a href="#停用词过滤" class="headerlink" title="停用词过滤"></a>停用词过滤</h2><hr>
<p>  停用词是在进行分词的时候去除一些在文档中频繁出现，但是未携带太多意义的词语，因为这些没意义的单词不应该参与算法运算。</p>
<h3 id="Spark-MLlib中实现"><a href="#Spark-MLlib中实现" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkStopWordsRemover</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkStopWordsRemover&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="string">&quot;Hi I heard about Spark&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="string">&quot;I wish Java could use case classes&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="string">&quot;Logistic regression models are neat&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;sentence&quot;</span>, DataTypes.StringType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; documentDF = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">Tokenizer</span> <span class="variable">tokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Tokenizer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;sentence&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;words&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; wordsDF = tokenizer.transform(documentDF);</span><br><span class="line"></span><br><span class="line">        <span class="type">StopWordsRemover</span> <span class="variable">stopWordsRemover</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StopWordsRemover</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;words&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;filtered&quot;</span>)</span><br><span class="line">                <span class="comment">//设置停用词</span></span><br><span class="line">                <span class="comment">//.setStopWords(new String[]&#123;&quot;i&quot;, &quot;the&quot;, &quot;are&quot;&#125;)</span></span><br><span class="line">                <span class="comment">//设置是否区分大小写</span></span><br><span class="line">                .setCaseSensitive(<span class="literal">true</span>);</span><br><span class="line">        Dataset&lt;Row&gt; filteredDF = stopWordsRemover.transform(wordsDF);</span><br><span class="line">        filteredDF.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><hr>
<p>  TF-IDF（Term Frequency-Inverse Document Frequency）被叫做词频-逆向文档频率。是一种在文本挖掘中广泛使用的特征向量化方法，用来反映一个单词在语料库文档中的重要性。</p>
<p>  我们把很多文档的集合叫做语料库，我们需要计算语料库文档中每个单词$t$在整个语料库文档中的权重度量用来表示该单词对于该类文档的重要性，通常我们使用概率来确定该度量信息，然后通过机器学习算法进行预测文档类别的时候，通过单词的权重来确定该文档更相似于语料库中哪篇文档来达到预测的目的。</p>
<h3 id="TF"><a href="#TF" class="headerlink" title="TF"></a>TF</h3><p>  TF（Term Frequency）即词频，表示单词$t$在文档$d$中的的出现频率，所以有<br>$$<br>TF(t,d) &#x3D; \frac{n_{t,d}}{\sum_k n_{k,d}}<br>$$</p>
<h3 id="IDF"><a href="#IDF" class="headerlink" title="IDF"></a>IDF</h3><p>  IDF（Inverse Document Frequency）即逆向文档频率，表示如果单词$t$在语料库中出现的频率极高，那么该词及有可能没有携带针对文档的某些特殊信息，例如的、是和我等词，所以我们需要降低该词的权重信息，我们将语料库表示为$D$，所以有<br>$$<br>IDF(t,D) &#x3D; log \frac {\mid D\mid}{\mid DF(t,D)\mid}<br>$$<br>其中$\mid D\mid$表示为语料库中文档总数，$\mid DF(t,D)\mid$表示为单词$t$在语料库中出现的次数，为了避免分母为0的情况，所以公式更新为<br>$$<br>IDF(t,D) &#x3D; log \frac {\mid D\mid+1}{\mid DF(t,D)\mid+1}<br>$$<br>因为取对数的缘故，当单词$t$出现在所有文档中时，$IDF(t,D)$的值为0，合理的下降了单词$t$的权重。</p>
<h3 id="TF-IDF-1"><a href="#TF-IDF-1" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>  最终我们TF-IDF的结果会被表示为<br>$$<br>TFIDF(t,d,D) &#x3D; TF(t,d)·IDF(t,D)<br>$$</p>
<h3 id="Spark-MLlib中实现-1"><a href="#Spark-MLlib中实现-1" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkTFIDF</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession.builder()</span><br><span class="line">                .appName(<span class="string">&quot;SparkTFIDF&quot;</span>)</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0.0</span>, <span class="string">&quot;Hi I heard about Spark&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">0.0</span>, <span class="string">&quot;I wish Java could use case classes&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">1.0</span>, <span class="string">&quot;Logistic regression models are neat&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;label&quot;</span>, DataTypes.DoubleType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;sentence&quot;</span>, DataTypes.StringType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; sentenceData = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">Tokenizer</span> <span class="variable">tokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Tokenizer</span>().setInputCol(<span class="string">&quot;sentence&quot;</span>).setOutputCol(<span class="string">&quot;words&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; wordsData = tokenizer.transform(sentenceData);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 使用 HashingTF 进行词频统计</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * HashingTF 运用 Hashing trick 方法进行进行特征降维处理</span></span><br><span class="line"><span class="comment">         * 哈希函数使用 murmurHash 3</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">HashingTF</span> <span class="variable">hashingTF</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HashingTF</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;words&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;rawFeatures&quot;</span>)</span><br><span class="line">                <span class="comment">//设置为以二进制方式进行词频统计，即单词出现被记录为1，不出现记录为0</span></span><br><span class="line">                <span class="comment">//.setBinary(true)</span></span><br><span class="line">                <span class="comment">//设置输出维数，默认维数为 262144</span></span><br><span class="line">                .setNumFeatures(<span class="number">20</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 使用 CountVectorizer 进行词频统计</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * CountVectorizer 是使用文档中单词计数的方式进行特征提取</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">CountVectorizerModel</span> <span class="variable">countVectorizerModel</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountVectorizer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;words&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;rawFeatures&quot;</span>)</span><br><span class="line">                <span class="comment">//设置为以二进制方式进行词频统计，即单词出现被记录为1，不出现记录为0</span></span><br><span class="line">                <span class="comment">//.setBinary(true)</span></span><br><span class="line">                <span class="comment">//设置输出向量的结果维数，或理解为Top n的词列表</span></span><br><span class="line">                .setVocabSize(<span class="number">20</span>)</span><br><span class="line">                <span class="comment">//设置最小输出</span></span><br><span class="line">                .setMinDF(<span class="number">0</span>)</span><br><span class="line">                .fit(wordsData);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; featurizedData = hashingTF.transform(wordsData);</span><br><span class="line">        <span class="type">IDF</span> <span class="variable">idf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IDF</span>().setInputCol(<span class="string">&quot;rawFeatures&quot;</span>).setOutputCol(<span class="string">&quot;features&quot;</span>);</span><br><span class="line">        <span class="type">IDFModel</span> <span class="variable">idfModel</span> <span class="operator">=</span> idf.fit(featurizedData);</span><br><span class="line">        Dataset&lt;Row&gt; rescaledData = idfModel.transform(featurizedData);</span><br><span class="line">        rescaledData.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><hr>
<p>  word2vec是将一个文档用分布式特征向量的方式表示出来，便于在机器学习算法中进行文档相似度等计算。该方式将每个单词映射到一个唯一的固定维度大小的向量中，向量被表示为每个词在空间向量中的距离，可以理解为语义相似度。</p>
<h3 id="Spark-MLlib中实现-2"><a href="#Spark-MLlib中实现-2" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkWord2Vec</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkWord2Vec&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="string">&quot;Hi I heard about Spark&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="string">&quot;I wish Java could use case classes&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="string">&quot;Logistic regression models are neat&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;sentence&quot;</span>, DataTypes.StringType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; documentDF = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">Tokenizer</span> <span class="variable">tokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Tokenizer</span>().setInputCol(<span class="string">&quot;sentence&quot;</span>).setOutputCol(<span class="string">&quot;words&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; wordsDF = tokenizer.transform(documentDF);</span><br><span class="line">        <span class="type">Word2Vec</span> <span class="variable">word2Vec</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Word2Vec</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;words&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;result&quot;</span>)</span><br><span class="line">                <span class="comment">//设置最终输出的向量长度</span></span><br><span class="line">                .setVectorSize(<span class="number">3</span>)</span><br><span class="line">                <span class="comment">//因为有使用梯度下降算法计算最大似然估计，所以可以设置迭代次数</span></span><br><span class="line">                .setMaxIter(<span class="number">100</span>)</span><br><span class="line">                .setMinCount(<span class="number">0</span>);</span><br><span class="line">        <span class="type">Word2VecModel</span> <span class="variable">model</span> <span class="operator">=</span> word2Vec.fit(wordsDF);</span><br><span class="line">        Dataset&lt;Row&gt; result = model.transform(wordsDF);</span><br><span class="line">        result.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="N-Gram（N元模型）"><a href="#N-Gram（N元模型）" class="headerlink" title="N-Gram（N元模型）"></a>N-Gram（N元模型）</h2><hr>
<p>  n-gram被表示为一个长度为n的单词的序列，即将输入的特征值重新组合为以空格为分隔符的两个单词为单位的特征，常用的在Word2Vec中被用作上下文表示。例如[a, b, c]会生成[a b, b c]，即输出某个单词在相对的语义环境中的表示。</p>
<h3 id="Spark-MLlib中实现-3"><a href="#Spark-MLlib中实现-3" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkNGram</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkNGram&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="string">&quot;Hi I heard about Spark&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="string">&quot;I wish Java could use case classes&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="string">&quot;Logistic regression models are neat&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;sentence&quot;</span>, DataTypes.StringType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; documentDF = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">Tokenizer</span> <span class="variable">tokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Tokenizer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;sentence&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;words&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; wordsDF = tokenizer.transform(documentDF);</span><br><span class="line">        <span class="type">NGram</span> <span class="variable">ngram</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NGram</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;words&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;ngrams&quot;</span>)</span><br><span class="line">          		<span class="comment">//设置n的值</span></span><br><span class="line">                .setN(<span class="number">2</span>);</span><br><span class="line">        Dataset&lt;Row&gt; ngramDataFrame = ngram.transform(wordsDF);</span><br><span class="line">        ngramDataFrame.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="二值化（Binarizer）"><a href="#二值化（Binarizer）" class="headerlink" title="二值化（Binarizer）"></a>二值化（Binarizer）</h2><hr>
<p>  二值化是将特征值转化为二进制（0&#x2F;1）的过程，在实际使用中我们需要确定一个阈值，如果特征值大于我们确定的阈值则特征被二值化为1，否则为0。</p>
<h3 id="Spark-MLlib中实现-4"><a href="#Spark-MLlib中实现-4" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkBinarizer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkBinarizer&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0</span>, <span class="number">0.1</span>),</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, <span class="number">0.8</span>),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, <span class="number">0.2</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;feature&quot;</span>, DataTypes.DoubleType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; continuousDataFrame = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">Binarizer</span> <span class="variable">binarizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Binarizer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;feature&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;binarized_feature&quot;</span>)</span><br><span class="line">                .setThreshold(<span class="number">0.5</span>);</span><br><span class="line">        Dataset&lt;Row&gt; binarizedDataFrame = binarizer.transform(continuousDataFrame);</span><br><span class="line">        binarizedDataFrame.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="PCA（主元分析）"><a href="#PCA（主元分析）" class="headerlink" title="PCA（主元分析）"></a>PCA（主元分析）</h2><hr>
<p>  PCA（Principal Components Analysis）即主成分分析，也叫主元分析。PCA在机器学习算法的经常在特征值降维中使用，就是将原本复杂的特征在不影响原信息表示的前提下从高维降至低维特征，使特征可以更方便的进行机器学习算法的计算。</p>
<blockquote>
<p>关于PCA原理和推导的文档：</p>
<p>[PCA]: <span class="exturl" data-url="aHR0cDovL3d3dy4zNjBkb2MuY29tL2NvbnRlbnQvMTMvMTEyNC8wMi85NDgyXzMzMTY4ODg4OS5zaHRtbA==">http://www.360doc.com/content/13/1124/02/9482_331688889.shtml<i class="fa fa-external-link-alt"></i></span>	“PCA数学原理”</p>
</blockquote>
<h3 id="Spark-MLlib中实现-5"><a href="#Spark-MLlib中实现-5" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkPCA</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkPCA&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(Vectors.sparse(<span class="number">5</span>, <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">1</span>, <span class="number">3</span>&#125;, <span class="keyword">new</span> <span class="title class_">double</span>[]&#123;<span class="number">1.0</span>, <span class="number">7.0</span>&#125;)),</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">2.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>)),</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">4.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>))</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;features&quot;</span>, <span class="keyword">new</span> <span class="title class_">VectorUDT</span>(), <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">PCAModel</span> <span class="variable">pca</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PCA</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;pcaFeatures&quot;</span>)</span><br><span class="line">                .setK(<span class="number">3</span>)</span><br><span class="line">                .fit(df);</span><br><span class="line">        Dataset&lt;Row&gt; result = pca.transform(df);</span><br><span class="line">        result.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="多项式扩展"><a href="#多项式扩展" class="headerlink" title="多项式扩展"></a>多项式扩展</h2><hr>
<p>  多项式扩展（Polynomial expansion）也叫做多项式展开，是将特征从低维扩展为高维空间的过程。</p>
<h3 id="Spark-MLlib中实现-6"><a href="#Spark-MLlib中实现-6" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkPoly</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkPoly&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">2.0</span>, <span class="number">1.0</span>)),</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">0.0</span>, <span class="number">0.0</span>)),</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">3.0</span>, -<span class="number">1.0</span>))</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;features&quot;</span>, <span class="keyword">new</span> <span class="title class_">VectorUDT</span>(), <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(data, schema);</span><br><span class="line"></span><br><span class="line">        <span class="type">PolynomialExpansion</span> <span class="variable">polyExpansion</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PolynomialExpansion</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;polyFeatures&quot;</span>)</span><br><span class="line">                .setDegree(<span class="number">3</span>);</span><br><span class="line">        Dataset&lt;Row&gt; polyDF = polyExpansion.transform(df);</span><br><span class="line">        polyDF.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="DCT"><a href="#DCT" class="headerlink" title="DCT"></a>DCT</h2><hr>
<p>  DCT（Discrete Cosine Transform）即离散余弦变换，是将时域的N维实数序列转换为频域的N维实数序列</p>
<h3 id="Spark-MLlib中实现-7"><a href="#Spark-MLlib中实现-7" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkDCT</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkDCT&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">0.0</span>, <span class="number">1.0</span>, -<span class="number">2.0</span>, <span class="number">3.0</span>)),</span><br><span class="line">                RowFactory.create(Vectors.dense(-<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>, -<span class="number">7.0</span>)),</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">14.0</span>, -<span class="number">2.0</span>, -<span class="number">5.0</span>, <span class="number">1.0</span>))</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;features&quot;</span>, <span class="keyword">new</span> <span class="title class_">VectorUDT</span>(), <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(data, schema);</span><br><span class="line"></span><br><span class="line">        <span class="type">DCT</span> <span class="variable">dct</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DCT</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;featuresDCT&quot;</span>)</span><br><span class="line">                .setInverse(<span class="literal">false</span>);</span><br><span class="line">        Dataset&lt;Row&gt; dctDf = dct.transform(df);</span><br><span class="line">        dctDf.select(<span class="string">&quot;featuresDCT&quot;</span>).show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="索引变换"><a href="#索引变换" class="headerlink" title="索引变换"></a>索引变换</h2><hr>
<p>  索引变换有两种，一种是字符串到索引的变换，一种是索引到字符串的变换，简单理解就是将复杂的特征值变换为简单的按照特征值出现的频率排序的索引列表。</p>
<h3 id="Spark-MLlib中实现-8"><a href="#Spark-MLlib中实现-8" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkIndex</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkIndex&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line"></span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0</span>, <span class="string">&quot;a&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, <span class="string">&quot;b&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, <span class="string">&quot;c&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">3</span>, <span class="string">&quot;a&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">4</span>, <span class="string">&quot;a&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">5</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;category&quot;</span>, DataTypes.StringType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 字符串到索引的变换</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="type">StringIndexer</span> <span class="variable">stringIndexer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringIndexer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;category&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;categoryIndex&quot;</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 索引到字符串的变换</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="type">IndexToString</span> <span class="variable">indexToString</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IndexToString</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;categoryIndex&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;index&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; categoryIndexDF = stringIndexer.fit(df).transform(df);</span><br><span class="line">        Dataset&lt;Row&gt; indexDF = indexToString.transform(categoryIndexDF);</span><br><span class="line">        indexDF.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h2><hr>
<p>  独热编码（One-hot encoding）是将一列标签索引映射到一列二进制向量，并且最多只有一个单值。</p>
<p>  独热编码往往被用来处理不是连续值的特征，如分类值等，因为算法分类器默认数据是连续的并且有序的，所以当输入特征随机分配的属性值或分类值时，我们需要找一种特征解决方案将这样的特征值进行变换。</p>
<p>  独热编码使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意的时候其中只有一位有效，例如有以下特征<br>$$<br>[male, female]<br>$$<br>如果将上述特征用数字表示，在算法处理时效率会高很多，我们对该特征做简单的索引映射<br>$$<br>[0,1]<br>$$<br>因为分类器默认的算法数据是连续有序的，所以上述经过映射过的特征也不能直接在分类器中使用，我们采用独热编码的方式进行特征转换<br>$$<br>[00, 10]<br>$$<br>因此原来的特征内容被转换为一个稀疏特征，直接输入到分类器进行模型训练。</p>
<h3 id="Spark-MLlib中实现-9"><a href="#Spark-MLlib中实现-9" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkOnehotEncode</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkOnehotEncode&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0</span>, <span class="string">&quot;a&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, <span class="string">&quot;b&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, <span class="string">&quot;c&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">3</span>, <span class="string">&quot;a&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">4</span>, <span class="string">&quot;a&quot;</span>),</span><br><span class="line">                RowFactory.create(<span class="number">5</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;category&quot;</span>, DataTypes.StringType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">StringIndexerModel</span> <span class="variable">indexer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringIndexer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;category&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;categoryIndex&quot;</span>)</span><br><span class="line">                .fit(df);</span><br><span class="line">        Dataset&lt;Row&gt; indexed = indexer.transform(df);</span><br><span class="line">        <span class="type">OneHotEncoder</span> <span class="variable">encoder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OneHotEncoder</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;categoryIndex&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;categoryVec&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; encoded = encoder.transform(indexed);</span><br><span class="line">        encoded.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="向量类型索引化"><a href="#向量类型索引化" class="headerlink" title="向量类型索引化"></a>向量类型索引化</h2><hr>
<p>  向量类型索引化（Vector Index）可以帮助指定向量数据集中的分类特征，它可以自动确定哪些特征值是分类的，并将原始值转换为类别索引。</p>
<p>  向量类型索引化的功能就是将特征中的类别特征重新进行编号，将向量内部原始的值离散化为索引值，用来提高决策树或随机森林等算法的模型分类效果。</p>
<h3 id="Spark-MLlib中实现-10"><a href="#Spark-MLlib中实现-10" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><p>  在Spark MLlib中，指定一列输入的向量列后需要指定maxCategrries参数，表明如果某个特征值不重复个数小于等于maxCategrries参数，那么VectorIndexer会将该特征进行从索引0开始的离散化操作。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkVectorIndexer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkVectorIndexer&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">2.0</span>, <span class="number">1.0</span>)),</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">0.0</span>, <span class="number">0.0</span>)),</span><br><span class="line">                RowFactory.create(Vectors.dense(<span class="number">3.0</span>, -<span class="number">1.0</span>))</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;features&quot;</span>, <span class="keyword">new</span> <span class="title class_">VectorUDT</span>(), <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">VectorIndexerModel</span> <span class="variable">vectorIndexerModel</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">VectorIndexer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;feat&quot;</span>)</span><br><span class="line">                .setMaxCategories(<span class="number">2</span>)</span><br><span class="line">                .fit(df);</span><br><span class="line">        Dataset&lt;Row&gt; featDF = vectorIndexerModel.transform(df);</span><br><span class="line">        featDF.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="相互作用"><a href="#相互作用" class="headerlink" title="相互作用"></a>相互作用</h2><hr>
<p>  该特征变换采用两个向量类型的特征值进行变换后生成一个单个向量的特征，即将每个输入列中值的组合乘积后生成一个新的向量特征。</p>
<h3 id="Spark-MLlib中实现-11"><a href="#Spark-MLlib中实现-11" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkInteraction</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkInteraction&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>),</span><br><span class="line">                RowFactory.create(<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>),</span><br><span class="line">                RowFactory.create(<span class="number">4</span>, <span class="number">10</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">                RowFactory.create(<span class="number">5</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">7</span>, <span class="number">3</span>),</span><br><span class="line">                RowFactory.create(<span class="number">6</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id1&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id2&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id3&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id4&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id5&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id6&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id7&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">VectorAssembler</span> <span class="variable">assembler1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">VectorAssembler</span>()</span><br><span class="line">                .setInputCols(<span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;id2&quot;</span>, <span class="string">&quot;id3&quot;</span>, <span class="string">&quot;id4&quot;</span>&#125;)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;vec1&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; assembled1 = assembler1.transform(df);</span><br><span class="line">        <span class="type">VectorAssembler</span> <span class="variable">assembler2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">VectorAssembler</span>()</span><br><span class="line">                .setInputCols(<span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;id5&quot;</span>, <span class="string">&quot;id6&quot;</span>, <span class="string">&quot;id7&quot;</span>&#125;)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;vec2&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; assembled2 = assembler2.transform(assembled1).select(<span class="string">&quot;id1&quot;</span>, <span class="string">&quot;vec1&quot;</span>, <span class="string">&quot;vec2&quot;</span>);</span><br><span class="line">        <span class="type">Interaction</span> <span class="variable">interaction</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Interaction</span>()</span><br><span class="line">                .setInputCols(<span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;id1&quot;</span>,<span class="string">&quot;vec1&quot;</span>,<span class="string">&quot;vec2&quot;</span>&#125;)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;interactedCol&quot;</span>);</span><br><span class="line">        Dataset&lt;Row&gt; interacted = interaction.transform(assembled2);</span><br><span class="line">        interacted.show(<span class="literal">false</span>);</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="泛数p-norm规范化"><a href="#泛数p-norm规范化" class="headerlink" title="泛数p-norm规范化"></a>泛数p-norm规范化</h2><hr>
<p>  p-norm即p-泛数，机器学习算法中对特征进行P-泛数规范化操作，使得算法有更好的表现。</p>
<p>  p-泛数的定义在自定义p值的前提下进行泛数计算，常用的为2-泛数，即$L^2$，单位P-泛数的定义为<br>$$<br>\mid\mid x \mid\mid_p &#x3D; (\mid x_1\mid^p + \mid x_2 \mid^p+\cdots+\mid x_n\mid^p)^{\frac1p}<br>$$<br>当p取1，2和$\infty$时的三种简单情形<br>$$<br>\begin{align}<br>&amp; 1-泛数(L^1):\mid\mid x \mid\mid &#x3D; \mid x_1\mid + \mid x_2 \mid+\cdots+\mid x_n\mid<br>\<br>&amp; 2-泛数(L^2):\mid\mid x \mid\mid_2 &#x3D; (\mid x_1\mid^2 + \mid x_2 \mid^2+\cdots+\mid x_n\mid^2)^{\frac12}<br>\<br>&amp; \infty-泛数(L^\infty):\mid\mid x \mid\mid_\infty &#x3D;max (\mid x_1\mid, \mid x_2 \mid,\cdots+\mid x_n\mid)<br>\end{align}<br>$$<br>  对某个特征进行P-泛数规范即将该特征组成的特征向量计算其P-泛数，然后对特征向量中的每个元素除以P-泛数后组成新的特征向量。</p>
<h3 id="Spark-MLlib中实现-12"><a href="#Spark-MLlib中实现-12" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkNormalizer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkNormalizer&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0</span>, Vectors.dense(<span class="number">1.0</span>, <span class="number">0.1</span>, -<span class="number">8.0</span>)),</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, Vectors.dense(<span class="number">2.0</span>, <span class="number">1.0</span>, -<span class="number">4.0</span>)),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, Vectors.dense(<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">8.0</span>))</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;features&quot;</span>, <span class="keyword">new</span> <span class="title class_">VectorUDT</span>(), <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; l1NormData = normalizer.transform(dataFrame);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 1.0 / ( |1.0| + |0.1| + |-8.0| ) 0.1 / ( |1.0| + |0.1| + |-8.0| ) -0.8 / ( |1.0| + |0.1| + |-8.0| )</span></span><br><span class="line"><span class="comment">         * 2.0 / ( |2.0| + |1.0| + |-4.0| ) 1.0 / ( |2.0| + |1.0| + |-4.0| ) -4.0 / ( |2.0| + |1.0| + |-4.0| )</span></span><br><span class="line"><span class="comment">         * 4.0 / ( |4.0| + |10.0| + |8.0| ) 10.0/ ( |4.0| + |10.0| + |8.0| )  8.0 / ( |4.0| + |10.0| + |8.0| )</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        l1NormData.show(<span class="literal">false</span>);</span><br><span class="line">        Dataset&lt;Row&gt; lInfNormData = normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 1.0 / max(1.0, 0.1, -8.0) ...</span></span><br><span class="line"><span class="comment">         * ...</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        lInfNormData.show(<span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="z-score规范化"><a href="#z-score规范化" class="headerlink" title="z-score规范化"></a>z-score规范化</h2><hr>
<p>  z-score规范化或称作零均值规范化是将特征进行基于特征值均值和标准差进行规范化的操作。特征v的z-score规范化被表示为<br>$$<br>v&#96;&#x3D;\frac{v_i-avg_v}{\delta_v}<br>$$<br>其中$avg_v$为特征v的平均值，$\delta_v$为特征的标准差。</p>
<p>  当特征v的最大值和最小值未知或有离群点影响最大最小规范化时z-score规范化是一个有效的特征规范化手段。</p>
<h3 id="Spark-MLlib中实现-13"><a href="#Spark-MLlib中实现-13" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkStandardScaler</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkStandardScaler&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0</span>, Vectors.dense(<span class="number">1.0</span>, <span class="number">0.1</span>, -<span class="number">8.0</span>)),</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, Vectors.dense(<span class="number">2.0</span>, <span class="number">1.0</span>, -<span class="number">4.0</span>)),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, Vectors.dense(<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">8.0</span>))</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;features&quot;</span>, <span class="keyword">new</span> <span class="title class_">VectorUDT</span>(), <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; dataFrame = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">StandardScalerModel</span> <span class="variable">standardScalerModel</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StandardScaler</span>()</span><br><span class="line">                <span class="comment">//是否启用均值计算，将均值移到0，对稀疏矩阵不可用</span></span><br><span class="line">                .setWithMean(<span class="literal">false</span>)</span><br><span class="line">                <span class="comment">//是否将方差缩放到1</span></span><br><span class="line">                .setWithStd(<span class="literal">true</span>)</span><br><span class="line">          		.setInputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;scalerFeat&quot;</span>)</span><br><span class="line">                .fit(dataFrame);</span><br><span class="line">        Dataset&lt;Row&gt; scalerDF = standardScalerModel.transform(dataFrame);</span><br><span class="line">        scalerDF.show(<span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="最大-最小规范化"><a href="#最大-最小规范化" class="headerlink" title="最大-最小规范化"></a>最大-最小规范化</h2><hr>
<p>  最大-最小规范化是将特征向量的值线性地变换到指定最大-最小值之间。经常的我们在使用算法进行模型训练的时候，因为每个特征的取值大小不同或差距很大，这导致在算法训练的时候数值较大的特征对模型结果起决定性作用，为了解决这种问题所以引入最大最小规范化的特征处理方式，该方式使得特征值最后落在$[0, 1]$范围内。</p>
<p>  常用的最大-最小规范化方式为<br>$$<br>newValue&#x3D;\frac{oldValue-minValue}{maxValue-minValue}\times (E_{max}-E_{min})+E_{min}<br>$$<br>其中$E_{max}$和$E_{min}$是指定的区间范围。</p>
<h3 id="Spark-MLlib中实现-14"><a href="#Spark-MLlib中实现-14" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkMinMaxScaler</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkMinMaxScaler&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0</span>, Vectors.dense(<span class="number">1.0</span>, <span class="number">0.1</span>, -<span class="number">8.0</span>)),</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, Vectors.dense(<span class="number">2.0</span>, <span class="number">1.0</span>, -<span class="number">4.0</span>)),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, Vectors.dense(<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">8.0</span>))</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;features&quot;</span>, <span class="keyword">new</span> <span class="title class_">VectorUDT</span>(), <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; dataFrame = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">MinMaxScalerModel</span> <span class="variable">minMaxScalerModel</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MinMaxScaler</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;scalerFeat&quot;</span>)</span><br><span class="line">                .fit(dataFrame);</span><br><span class="line">        Dataset&lt;Row&gt; scalerDF = minMaxScalerModel.transform(dataFrame);</span><br><span class="line">        scalerDF.show(<span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="绝对值规范化"><a href="#绝对值规范化" class="headerlink" title="绝对值规范化"></a>绝对值规范化</h2><hr>
<p>  绝对值规范化是将各个特征值除以该特征的最大绝对值，因此可以将特征值缩放到$[-1,1]$之间，并且不会移动和居中中心点，所以不会破坏向量的稀疏性。但是唯一需要考虑的就是如果该最大绝对值是一个利群点，这种规范化方式就会很不合理。</p>
<h3 id="Spark-MLlib中实现-15"><a href="#Spark-MLlib中实现-15" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkMaxAbsScaler</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkMaxAbsScaler&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0</span>, Vectors.dense(<span class="number">1.0</span>, <span class="number">0.1</span>, -<span class="number">8.0</span>)),</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, Vectors.dense(<span class="number">2.0</span>, <span class="number">1.0</span>, -<span class="number">4.0</span>)),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, Vectors.dense(<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">8.0</span>))</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;features&quot;</span>, <span class="keyword">new</span> <span class="title class_">VectorUDT</span>(), <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; dataFrame = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">MaxAbsScalerModel</span> <span class="variable">maxAbsScalerModel</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MaxAbsScaler</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;scalerFeat&quot;</span>)</span><br><span class="line">                .fit(dataFrame);</span><br><span class="line">        Dataset&lt;Row&gt; scalerDF = maxAbsScalerModel.transform(dataFrame);</span><br><span class="line">        scalerDF.show(<span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="分箱器"><a href="#分箱器" class="headerlink" title="分箱器"></a>分箱器</h2><hr>
<p>  分箱器是将连续的特征值按照我们提供的数值区间按照$[x,y)$进行切割，输出区间索引并生成新的特征。</p>
<h3 id="Spark-MLlib中实现-16"><a href="#Spark-MLlib中实现-16" class="headerlink" title="Spark MLlib中实现"></a>Spark MLlib中实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparkBinarizer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;SparkBinarizer&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        List&lt;Row&gt; data = Arrays.asList(</span><br><span class="line">                RowFactory.create(<span class="number">0</span>, <span class="number">0.1</span>),</span><br><span class="line">                RowFactory.create(<span class="number">1</span>, <span class="number">0.8</span>),</span><br><span class="line">                RowFactory.create(<span class="number">2</span>, <span class="number">0.2</span>)</span><br><span class="line">        );</span><br><span class="line">        <span class="type">StructType</span> <span class="variable">schema</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructType</span>(<span class="keyword">new</span> <span class="title class_">StructField</span>[]&#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">false</span>, Metadata.empty()),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">StructField</span>(<span class="string">&quot;feature&quot;</span>, DataTypes.DoubleType, <span class="literal">false</span>, Metadata.empty())</span><br><span class="line">        &#125;);</span><br><span class="line">        Dataset&lt;Row&gt; continuousDataFrame = spark.createDataFrame(data, schema);</span><br><span class="line">        <span class="type">Binarizer</span> <span class="variable">binarizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Binarizer</span>()</span><br><span class="line">                .setInputCol(<span class="string">&quot;feature&quot;</span>)</span><br><span class="line">                .setOutputCol(<span class="string">&quot;binarized_feature&quot;</span>)</span><br><span class="line">                .setThreshold(<span class="number">0.5</span>);</span><br><span class="line">        Dataset&lt;Row&gt; binarizedDataFrame = binarizer.transform(continuousDataFrame);</span><br><span class="line">        binarizedDataFrame.show();</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Hadamard乘积"><a href="#Hadamard乘积" class="headerlink" title="Hadamard乘积"></a>Hadamard乘积</h2><hr>
<p>  将我们输入的每个向量乘以一个指定的权重向量来生成一个新的向量值。</p>
<h2 id="向量切片机"><a href="#向量切片机" class="headerlink" title="向量切片机"></a>向量切片机</h2><hr>
<h2 id="R模型公式"><a href="#R模型公式" class="headerlink" title="R模型公式"></a>R模型公式</h2><hr>
<h2 id="卡方特征选择器"><a href="#卡方特征选择器" class="headerlink" title="卡方特征选择器"></a>卡方特征选择器</h2><hr>
<h2 id="局部敏感哈希"><a href="#局部敏感哈希" class="headerlink" title="局部敏感哈希"></a>局部敏感哈希</h2><hr>
<h2 id="LSH运算"><a href="#LSH运算" class="headerlink" title="LSH运算"></a>LSH运算</h2><hr>
<h2 id="LSH算法"><a href="#LSH算法" class="headerlink" title="LSH算法"></a>LSH算法</h2><hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/06/03/Spark%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/06/03/Spark%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">Spark入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-06-03 15:17:01" itemprop="dateCreated datePublished" datetime="2017-06-03T15:17:01+08:00">2017-06-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2017-06-12 14:21:00" itemprop="dateModified" datetime="2017-06-12T14:21:00+08:00">2017-06-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.1k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  在编写Spark应用程序之前我们应该清楚一点，Spark应用是构建在Scala语言的基础上，但是Apache官方封装了其他语言的版本，如Java、Python等，本文是基于Java语言实现的。在Spark应用程序实现中，我们需要有一个应用入口，在Java语言中即main函数，也就是说在Java版本的Spark应用是从main函数开始执行的。</p>
<h2 id="SparkConf"><a href="#SparkConf" class="headerlink" title="SparkConf"></a>SparkConf</h2><hr>
<p>  在编写Spark应用开始，我们必须获取Spark集群的一些配置信息，即new一个SparkConf对象，该对象存储了Spark应用运行过程中所需要的所有配置内容</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br></pre></td></tr></table></figure>

<p>  接下来会设置一些Spark应用运行时需要的参数，通过调用SparkConf中的方法去指定。</p>
<p>  指定Spark应用的名称</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.setAppName(<span class="string">&quot;My Frist Spark App.&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>  配置Spark应用需要链接的Spark集群，这里被设置为集群中Master几点的URL</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.setMaster(<span class="string">&quot;spark://spark1:7077&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>  如果Spark应用在本地测试运行的时候，可以指定URL为local则表示该Spark应用在本地执行</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h2><hr>
<p>  在Spark中，SparkContext时所有Spark功能的一个入口，所以无论使用哪种编程语言都需要一个SparkContext对象，它作用于Spark应用的初始化所需的一些核心组件，包括资源调度器（DAGSchedule、TaskScheduler）和在Master节点注册应用。</p>
<p>  根据Spark应用需要实现的任务和编程语言的不同，SparkContext对象也有不同的名字，如果使用Scala或Python开发，则默认为SparkContext；使用Java语言开发，则SparkContext被定义为JavaSparkContext。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">JavaSparkContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line">\\ ....</span><br><span class="line">\\ 所有计算操作</span><br><span class="line">\\ ....</span><br><span class="line">sc.close();</span><br></pre></td></tr></table></figure>

<p>对象参数为我们构建的SparkConf对象，并在应用完成后进行关闭操作。</p>
<h2 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h2><hr>
<p>  Spark应用的运行配置和初始化完成后，我们需要创建初始的RDD，即从数据源中获取数据并完成RDD的创建工作。输入源的数据会被打散并分配到RDD的每个partition中形成一个初始的分布式数据集。</p>
<h3 id="并行化集合创建"><a href="#并行化集合创建" class="headerlink" title="并行化集合创建"></a>并行化集合创建</h3><p>  使用并行化集合的方式可以使Spark应用在程序实现过程中手动创造一个数据集作为Spark应用的数据输入源</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers);</span><br></pre></td></tr></table></figure>

<h3 id="文本文件创建"><a href="#文本文件创建" class="headerlink" title="文本文件创建"></a>文本文件创建</h3><p>  以文本文件作为Spark应用的数据源时，RDD中的每一个元素相当于文本文件中每一行的内容</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;spark.txt&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>textFile方法会读取文件内数据生成我们的初始RDD。</p>
<h3 id="Hdfs创建"><a href="#Hdfs创建" class="headerlink" title="Hdfs创建"></a>Hdfs创建</h3><p>  以Hdfs作为Spark应用的数据源和文本文件类似，不同的是输入参数被替换为Hdfs链接的格式</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;hdfs://spark1:9000/spark.txt&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="textFile"><a href="#textFile" class="headerlink" title="textFile"></a>textFile</h3><p>  SparkContext对象的textFile方法支持目录、压缩文件和通配符进行RDD创建并且默认会为每一个文件创建partition，并支持使用第二个参数手动设置partition数量。</p>
<h2 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h2><hr>
<p>  在Spark应用中，其实我们最主要的部分就是对RDD进行运算的部分，其中包括RDD的筛选、合并等操作，即transformation和action操作。</p>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>  map操作将自定义函数分别作用于RDD中每个元素并获取一个新的元素，然后将所有新元素组成一个新的RDD。对任何类型的RDD，map算子都可以调用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;hdfs://spark1:9000/spark.txt&quot;</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; lineLength = lines.map(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Function</span>&lt;String, Integer&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Integer <span class="title function_">call</span><span class="params">(String v1)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> v1.length();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>  Function类中第一个泛型参数为传入自定义函数的类型，第二个泛型参数为自定义函数返回的数据类型，map操作将自定义函数的返回数据作为新RDD的元素。</p>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>  filter算子是将自定义函数作用于RDD中每个元素进行运算且自定义函数中call()方法的返回类型必须为Boolean，如果需要保留RDD中某个元素，那么在将该元素调用自定义函数的时候返回true，否则返回false不保留该元素</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers);</span><br><span class="line">JavaRDD&lt;Integer&gt; evenNumberRDD = numberRDD.filter(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Integer, Boolean&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Integer v1)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> v1 % <span class="number">2</span> == <span class="number">0</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">);</span><br><span class="line">evenNumberRDD.foreach(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">VoidFunction</span>&lt;Integer&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">(Integer t)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        System.out.println(t);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>  跟map算子形同，Function类中第一个泛型参数为传入自定义函数的类型，第二个泛型参数为自定义函数返回的数据类型，filter操作将根据自定义函数的返回值确定是否保留当前元素。</p>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>  flatMap通常在将RDD中数据拆分成多个元素的场景使用，该操作会将我们提供的RDD中每一个元素都调用我们传入的自定义函数并返回一个新的元素或多个新的元素组成一个新的RDD等待下一步处理</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;hdfs://spark1:9000/spark.txt&quot;</span>);</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title function_">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">&quot;&quot;</span>));</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>  FlatMapFunction类有两个泛型参数分别代表输入和输出类型，这里将每行文本作为输入然后做字符串分隔并返回多个元素组成新的RDD。</p>
<h3 id="mapToPair"><a href="#mapToPair" class="headerlink" title="mapToPair"></a>mapToPair</h3><p>  mapToPair通常在将RDD元素变换为（word，1）这种Tuple格式时使用</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;hdfs://spark1:9000/spark.txt&quot;</span>);</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title function_">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">&quot;&quot;</span>));</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;String, String, Integer&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Tuple2&lt;String, integer&gt; <span class="title function_">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>  JavaPairRDD也是RDD，它的两个泛型参数表示其中Tuple元素中第一个值和第二个值的类型。mapToPair要求和PairFunction搭配使用，PairFunction中的三个泛型参数分别表述输入类型和返回元素Tuple中两个子元素类型。</p>
<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><p>  groupByKey算子是一个聚合类型的算子，是对RDD中元素进行分组。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Integer&gt;(<span class="string">&quot;class1&quot;</span>, <span class="number">80</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Integer&gt;(<span class="string">&quot;class2&quot;</span>, <span class="number">75</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Integer&gt;(<span class="string">&quot;class1&quot;</span>, <span class="number">90</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Integer&gt;(<span class="string">&quot;class2&quot;</span>, <span class="number">65</span>),</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line">JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupedScores = scores.groupByKey();</span><br></pre></td></tr></table></figure>

<p>  使用groupByKey算子返回一个新的对key相同的元素进行聚合后的RDD。</p>
<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><p>  reduceByKey作用于RDD元素的每个key并进行reduce操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;hdfs://spark1:9000/spark.txt&quot;</span>);</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title function_">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">&quot;&quot;</span>));</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;String, String, Integer&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Tuple2&lt;String, integer&gt; <span class="title function_">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Integer, Integer, Integer&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Integer <span class="title function_">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> v1 + v2;</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>  该计算会将所有RDD元素传入自定义函数，并对所有key相同的元素进行第二个元素的相加操作，Function2的三个泛型参数分别表示输入两个相同key的value值和返回后的结果。</p>
<h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3><p>  sortByKey是一个排序算子，是对输入RDD中所有的key进行排序后返回新的RDD，新的RDD和就的RDD中元素没用变换，只是顺序不同而已。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">80</span>, <span class="string">&quot;class1&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">75</span>, <span class="string">&quot;class2&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">90</span>, <span class="string">&quot;class1&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">65</span>, <span class="string">&quot;class2&quot;</span>),,</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; sortedScores = scores.sortByKey(<span class="literal">false</span>);</span><br></pre></td></tr></table></figure>

<p>  在Spark应用的排序操作中，有时会遇到复杂的排序需求，即不只是简单的数字排序，那么此时我们需要自定义我们的排序key来完成RDD中元素的排序。</p>
<p>  自定义的排序key必须实现Ordered和Serializable接口，完成$greater、$greater$eq、$less、$less$eq、compare、compareTo和getter、setter、hascode、equals方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SecondarySortKey</span> <span class="keyword">implements</span> <span class="title class_">Ordered</span>&lt;SecondarySortKey&gt;, Serializable&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">int</span> first;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">int</span> second;</span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">SecondarySortKey</span><span class="params">(<span class="type">int</span> first, <span class="type">int</span> second)</span>&#123;</span><br><span class="line">    <span class="built_in">this</span>.first = first;</span><br><span class="line">    <span class="built_in">this</span>.second = second;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">boolean</span> $greater(SecondarySortkey other)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.first &gt; other.getFirst())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.first != other.getFirst())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.second &gt; other.getSecond())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">boolean</span> $greater$eq(SecondarySortKey other)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.$greater(other))</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.first != other.getFirst())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.second == other.getSecond())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">boolean</span> $less(SecondarySortKey other)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.first &lt; other.getFirst())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.first != other.getFirst())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.second &lt; other.getSecond())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">boolean</span> $less$eq(SecondarySortKey other)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.$less(other))</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.frist != other.getFirst())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.second == other.getSecond())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(SecondarySortKey other)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.first - other.getFirst() != <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">this</span>.first - other.getFirst();</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">this</span>.second - other.getSecond();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(SecondarySortKey other)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span>.first - other.getFirst() != <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">this</span>.first - other.getFirst();</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">this</span>.second - other.getSecond();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getFirst</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> first;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setFirst</span><span class="params">(<span class="type">int</span> first)</span>&#123;</span><br><span class="line">    <span class="built_in">this</span>.first = first;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getSecond</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> second;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSecond</span><span class="params">(<span class="type">int</span> second)</span>&#123;</span><br><span class="line">    <span class="built_in">this</span>.second = second;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hasCode</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">prime</span> <span class="operator">=</span> <span class="number">31</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">    result = prime * result + first;</span><br><span class="line">    result = prime * result + second;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object obj)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">this</span> == obj) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span>(obj == <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(getClass() != obj.getClass()) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="type">SecondarySortKey</span> <span class="variable">other</span> <span class="operator">=</span> (SecondarySortKey) obj;</span><br><span class="line">    <span class="keyword">if</span>(first != other.getFirst()) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(second != other.getSecond()) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用时直接将自定义好的key类型作为待排序RDD中元素的key。</p>
<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>  join算子实现了两个RDD的链接操作，进行join操作时，根据两个RDD中的key进行join</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; studentList = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">1</span>, <span class="string">&quot;leo&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">2</span>, <span class="string">&quot;jack&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">3</span>, <span class="string">&quot;tom&quot;</span>)</span><br><span class="line">);</span><br><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">1</span>, <span class="number">100</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">2</span>, <span class="number">90</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">3</span>, <span class="number">60</span>)</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; students = sc.parallelizePairs(studentList);</span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line">JavaPairRDD&lt;Integer, Tuple2&lt;String, Integer&gt;&gt; studentScores = students.join(scores);</span><br></pre></td></tr></table></figure>

<p>  join操作后返回一个新的RDD，新RDD的第一个泛型参数为原RDD的key的类型，第二个泛型参数中包括两个原RDD中所有的值的类型。假如有一个RDD（1，1）（1，2）（1，3）和（1，4）（2，1）（2，2），进行join以后会得到（1，（1，4））（1，（2，4））（1，（3，4））。</p>
<h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><p>  cogroup和join类似，不同的是cogroup会将所有关联到的元素都放到一个Iterable的对象中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; studentList = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">1</span>, <span class="string">&quot;leo&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">2</span>, <span class="string">&quot;jack&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, String&gt;(<span class="number">3</span>, <span class="string">&quot;tom&quot;</span>)</span><br><span class="line">);</span><br><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">1</span>, <span class="number">100</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">2</span>, <span class="number">90</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">3</span>, <span class="number">60</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">1</span>, <span class="number">70</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">2</span>, <span class="number">80</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Integer, Integer&gt;(<span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; students = sc.parallelizePairs(studentList);</span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line">JavaPairRDD&lt;Integer, Tuple2&lt;Iterable&lt;String&gt;, Iterable&lt;Integer&gt;&gt;&gt; studentScores = students.cogroup(scores);</span><br></pre></td></tr></table></figure>

<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p>  reduce操作是对RDD中每个元素进行reduce操作即对输入RDD中所有元素进行聚合操作并获取一个最终结果。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers);</span><br><span class="line"><span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> numberRDD.reduce(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Integer, Integer, Integer&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Integer <span class="title function_">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> v1 + v2;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>  collect操作的作用是将远程集群上面的RDD拉取到本地进行操作，考虑到数据量的问题，一般不会使用到该操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line">List&lt;Integer&gt; doubleNumberList = numbers.collect();</span><br></pre></td></tr></table></figure>

<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>  count是一个统计操作，对一个RDD进行count操作会返回一个数值型数据表示该RDD中有多少个元素</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"><span class="type">long</span> <span class="variable">count</span> <span class="operator">=</span> numbers.count();</span><br></pre></td></tr></table></figure>

<h3 id="take-n"><a href="#take-n" class="headerlink" title="take(n)"></a>take(n)</h3><p>  take(n)操作和collect类似，只是take(n)操作只获取RDD中前n个元素，collect为获取所有RDD中的元素</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line">javaRDD&lt;Integer&gt; sortedNumb = numbers.sortByKey(<span class="literal">true</span>);</span><br><span class="line">List&lt;Integer&gt; topNumber = sortedNumb.take(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>

<h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><p>  saveAsTextFile操作会将RDD中的数据保存在一个文件中，也可以是hdfs系统文件，并且我们需要指定一个具体的保存文件的目录而不是文件名</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line">javaRDD&lt;Integer&gt; sortedNumb = numbers.sortByKey(<span class="literal">true</span>);</span><br><span class="line">sortedNumb.saveAsTextFile(<span class="string">&quot;hdfs://spark1:9000/test&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>  countByKey操作时统计RDD中每个key对应的元素个数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, String&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(<span class="string">&quot;class1&quot;</span>, <span class="string">&quot;leo&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(<span class="string">&quot;class2&quot;</span>, <span class="string">&quot;jack&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(<span class="string">&quot;class1&quot;</span>, <span class="string">&quot;marry&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(<span class="string">&quot;class2&quot;</span>, <span class="string">&quot;tom&quot;</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(<span class="string">&quot;class2&quot;</span>, <span class="string">&quot;david&quot;</span>)</span><br><span class="line">); </span><br><span class="line">JavaPairRDD&lt;String, String&gt; students = sc.parallelizePairs(scoreList);</span><br><span class="line">Map&lt;String, Object&gt; studentCounts = students.countByKey();</span><br></pre></td></tr></table></figure>

<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><p>  foreach是一个action操作，用来遍历所有RDD数据集</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;hdfs://spark1:9000/spark.txt&quot;</span>);</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title function_">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">&quot;&quot;</span>));</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;String, String, Integer&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Tuple2&lt;String, integer&gt; <span class="title function_">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Integer, Integer, Integer&gt;()&#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="keyword">public</span> Integer <span class="title function_">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="keyword">return</span> v1 + v2;</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">);</span><br><span class="line">wordCounts.foreach(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">VoidFunction</span>&lt;Tuple2&lt;String, Integer&gt;&gt;()&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">(Tuple2&lt;String, Integer&gt; wordCount)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">      System.out.println(wordCount._1 + <span class="string">&quot;:&quot;</span> + wordCount._2);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h2 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h2><hr>
<p>  因为在Spark应用中，对当前RDD进行一个算子操作后为了保障系统资源的合理利用和减少中间结果数据的资源消耗原RDD就会被及时销毁，所以如果想再次使用之前被构建的RDD则必须重新读取数据进行RDD构建或提前将RDD缓存下来避开Spark集群的销毁策略，同样的考虑Spark应用的性能和效率，所以我们在大数据处理场景下应该避免RDD的重复构建而使用将未来可能会用到的RDD缓存下来的方法，这里我们叫RDD的持久化操作。</p>
<p>  Spark应用中使用cache()和persist()两个方法进行RDD的持久化操作，cache()方法调用的是persist()的无参版本，默认会将RDD持久化到内存中persist(MEMORY_ONLY)，如果需要使用其他的持久化策略，则修改具体的参数即可。在Spark应用中销毁我们持久化后的RDD，使用unpersist()方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;spark.txt&quot;</span>).cache();</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;spark.txt&quot;</span>).persist(MEMORY_ONLY);</span><br></pre></td></tr></table></figure>

<p>  需要注意的是RDD的持久化操作必须在该RDD创建完成后马上进行。</p>
<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><hr>
<p>  Spark为Spark应用提供两个实现Spark集群共享变量的方案，分别为广播变量和递增变量。</p>
<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">int</span> <span class="variable">factor</span> <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;Integer&gt; factorBroadcast = sc.broadcast(factor);</span><br><span class="line">list&lt;Integer&gt; numberList = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line">JavaRDD&lt;Integer&gt; multipleNumbers = numbers.map(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Integer, Integer&gt;()&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">call</span><span class="params">(Integer v1)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">factor</span> <span class="operator">=</span> factorBroadcast.value();</span><br><span class="line">      <span class="keyword">return</span> v1 * factor;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="递增变量"><a href="#递增变量" class="headerlink" title="递增变量"></a>递增变量</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> Accumulator&lt;Integer&gt; sum = sc.accumulator(<span class="number">0</span>);</span><br><span class="line">list&lt;Integer&gt; numberList = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line">numbers.foreach(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">VoidFunction</span>&lt;Integer&gt;()&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">(Integer t)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">      sum.add(t);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">);</span><br><span class="line">System.out.println(sum.value());</span><br></pre></td></tr></table></figure>

<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><hr>
<p>  Spark可以使用类似Hive的方式进行开发Spark应用，Spark SQL支持多种数据源的数据输入且性能比Hive高出很多。</p>
<h3 id="SQLContext"><a href="#SQLContext" class="headerlink" title="SQLContext"></a>SQLContext</h3><p>  开发Spark SQL应用，除了要构建一个SparkContext对象外，还需要构建Spark SQL特有的SQLContext对象</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">SQLContext</span> <span class="variable">sqlContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SQLContext</span>(sc);</span><br></pre></td></tr></table></figure>

<h3 id="HiveContext"><a href="#HiveContext" class="headerlink" title="HiveContext"></a>HiveContext</h3><p>  开发Spark SQL应用时也可以使用SQLContext对象的子类HiveContext，HiveContext除了包括SQLContext提供的所有功能外，还可以使用HiveQL语法来编写和执行SQL、使用Hive中的UDF函数、从Hive表读取数据等。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">HiveContext</span> <span class="variable">hiveContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HiveContext</span>(sc.sc());</span><br></pre></td></tr></table></figure>

<p>当然使用HiveContext的前提是必须已经安装了Hive且HiveContext构造函数只接收SparkContext对象。</p>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><hr>
<p>  DataFrame时Spark SQL中的抽象化数据表示形式，可以形象理解为数据库中的表。DataFrame支持多种数据源来构建。DataFrame的创建有共同的load和save操作，在Save操作的时候，我们可以使用save方法的第二个参数来指定写操作的策略</p>
<table>
<thead>
<tr>
<th align="center">Sava Mode</th>
<th align="center">简介</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SaveMode.ErrorIfExists（默认）</td>
<td align="center">如果目标位置已经存在数据，那么抛出一个异常</td>
</tr>
<tr>
<td align="center">SaveMode.Append</td>
<td align="center">如果目标位置已经存在数据，那么将新数据追加进去</td>
</tr>
<tr>
<td align="center">SaveMode.Overwrite</td>
<td align="center">如果目标位置已经存在数据，那么将已经存在的数据删除，用新数据进行覆盖</td>
</tr>
<tr>
<td align="center">SaveMode.Ignore</td>
<td align="center">如果目标位置已经存在数据就忽略不做任何操作</td>
</tr>
</tbody></table>
<h3 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h3><p>加载数据</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrame</span> <span class="variable">df</span> <span class="operator">=</span> sqlContext.read().format(<span class="string">&quot;parquet&quot;</span>).load(<span class="string">&quot;spark.parquet&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrame</span> <span class="variable">df</span> <span class="operator">=</span> sqlContext.read().parquet(<span class="string">&quot;spark.parquet&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>使用save方法写数据</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write().format(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;spark1&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="Json"><a href="#Json" class="headerlink" title="Json"></a>Json</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrame</span> <span class="variable">df</span> <span class="operator">=</span> sqlContext.read().format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;spark.json&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrame</span> <span class="variable">df</span> <span class="operator">=</span> sqlContext.read().json(<span class="string">&quot;hdfs://spark1:9000/spark.json&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; studentInfoJSONs = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;();</span><br><span class="line">studentInfoJSONs.add(<span class="string">&quot;&#123;\&quot;name\&quot;:\&quot;Leo\&quot;, \&quot;age\&quot;:18&#125;&quot;</span>);  </span><br><span class="line">studentInfoJSONs.add(<span class="string">&quot;&#123;\&quot;name\&quot;:\&quot;Marry\&quot;, \&quot;age\&quot;:17&#125;&quot;</span>);  </span><br><span class="line">studentInfoJSONs.add(<span class="string">&quot;&#123;\&quot;name\&quot;:\&quot;Jack\&quot;, \&quot;age\&quot;:19&#125;&quot;</span>);</span><br><span class="line">JavaRDD&lt;String&gt; studentInfoJSONsRDD = sc.parallelize(studentInfoJSONs);</span><br><span class="line"><span class="type">DataFrame</span> <span class="variable">studentInfosDF</span> <span class="operator">=</span> sqlContext.read().json(studentInfoJSONsRDD);</span><br></pre></td></tr></table></figure>

<p>使用save方法写数据</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write().format(<span class="string">&quot;json&quot;</span>).save(<span class="string">&quot;spark1.json&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>使用Json数据构建DataFrame，会使用Json中对象的键值作为表字段名来显示数据。</p>
<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>  Spark SQL支持对Hive中存储的数据进行读写。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrame</span> <span class="variable">df</span> <span class="operator">=</span> hiveContext.sql(<span class="string">&quot;select * from table&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrame</span> <span class="variable">df</span> <span class="operator">=</span> hiveContext.table(<span class="string">&quot;table&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>使用saveAsTable方法写数据</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.saveAsTable(<span class="string">&quot;table&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><p>  Spark SQL支持从关系型数据库中读取数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, String&gt; options = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;String, String&gt;();</span><br><span class="line">options.put(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://spark1:3306/testdb&quot;</span>);</span><br><span class="line">options.put(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;spark&quot;</span>);</span><br><span class="line"><span class="type">DataFrame</span> <span class="variable">df</span> <span class="operator">=</span> sqlContext.read().format(<span class="string">&quot;jdbc&quot;</span>).options(options).load();</span><br></pre></td></tr></table></figure>

<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>  将RDD直接转换为DataFrame，可以使用任何数据源构建的RDD进行Spark SQL操作，但是构建DataFrame我们需要知道数据的元数据，但是多数场景下我们并没有预先提供数据的元数据信息，所以需要动态绑定的方式构建RDD。</p>
<p>  当我们知道RDD的元数据时，我们使用反射来判断特定数据类型的RDD元数据，用作反射的JavaBean必须实现Serializable接口，表示可以被序列化的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Student</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">int</span> id;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">int</span> age;</span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">Student</span><span class="params">(<span class="type">int</span> id, String name, <span class="type">int</span> age)</span>&#123;</span><br><span class="line">    <span class="built_in">this</span>.id = id;</span><br><span class="line">    <span class="built_in">this</span>.name = name;</span><br><span class="line">    <span class="built_in">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getId</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> id;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setId</span><span class="params">(<span class="type">int</span> id)</span>&#123;</span><br><span class="line">    <span class="built_in">this</span>.id = id;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> String <span class="title function_">getName</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> name;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setName</span><span class="params">(String name)</span>&#123;</span><br><span class="line">    <span class="built_in">this</span>.name = name;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getAge</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> age;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setAge</span><span class="params">(<span class="type">int</span> age)</span>&#123;</span><br><span class="line">    <span class="built_in">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RDD2DataFrame</span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">    <span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;RDD2DataFrame&quot;</span>);</span><br><span class="line">    <span class="type">JavaSparkContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line">    <span class="type">SQLContext</span> <span class="variable">sqlContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SQLContext</span>(sc);</span><br><span class="line">    JavaRDD&lt;String&gt; lins = sc.textFile(<span class="string">&quot;spark.txt&quot;</span>);</span><br><span class="line">    JavaRDD&lt;Student&gt; studends = lines.map(</span><br><span class="line">      <span class="keyword">new</span> <span class="title class_">Function</span>&lt;String, Student&gt;()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Student <span class="title function_">call</span><span class="params">(String lin)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Student</span>(Integer.valueOf(line.split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>]), line.split(<span class="string">&quot;,&quot;</span>)[<span class="number">1</span>], Integer.valueOf(line.split(<span class="string">&quot;,&quot;</span>)[<span class="number">2</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    );</span><br><span class="line">    <span class="type">DataFrame</span> <span class="variable">studentDF</span> <span class="operator">=</span> sqlContext.createDataFrame(students, Student.class);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>  同样的，考虑这么一个场景，根据输入数据的不同，我们可以会在其他地方动态的获取RDD的元数据信息，然后去动态的构建一个DataFrame</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RDD2DataFrame</span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">    <span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;RDD2DataFrame&quot;</span>);</span><br><span class="line">    <span class="type">JavaSparkContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line">    <span class="type">SQLContext</span> <span class="variable">sqlContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SQLContext</span>(sc);</span><br><span class="line">    JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">&quot;spark.txt&quot;</span>);</span><br><span class="line">    JavaRDD&lt;Row&gt; studentRDD = lines.map(</span><br><span class="line">      <span class="keyword">new</span> <span class="title class_">Function</span>&lt;String, Row&gt;()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Row <span class="title function_">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">          String[] linePart = line.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">          <span class="keyword">return</span> RowFactory.create(Integer.valueOf(linePart[<span class="number">0</span>]), linePart[<span class="number">1</span>], Integer.valueOf(linePart[<span class="number">2</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    );</span><br><span class="line">    List&lt;StructField&gt; structFields = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;StructField&gt;();</span><br><span class="line">    structField.add(DataTypes.createStructField(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType, <span class="literal">true</span>));</span><br><span class="line">    structField.add(DataTypes.createStructField(<span class="string">&quot;name&quot;</span>, DataTypes.StringType, <span class="literal">true</span>));</span><br><span class="line">    structField.add(DataTypes.createStructField(<span class="string">&quot;age&quot;</span>, DataTypes.IntegerType, <span class="literal">true</span>));</span><br><span class="line">    <span class="type">StructType</span> <span class="variable">structType</span> <span class="operator">=</span> DataTypes.createStructType(structFields);</span><br><span class="line">    <span class="type">DataFrame</span> <span class="variable">studentDF</span> <span class="operator">=</span> sqlContext.createDataFrame(studentRDD, structType);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="DataFrame操作"><a href="#DataFrame操作" class="headerlink" title="DataFrame操作"></a>DataFrame操作</h2><hr>
<p>  在Spark SQL应用开发中，构建好DataFrame后可以类似操作数据表一样操作和查询DataFrame，也可以使用DataFrame创建临时表进行SQL语句的查询。</p>
<h3 id="临时表"><a href="#临时表" class="headerlink" title="临时表"></a>临时表</h3><p>  使用DataFrame可以注册实现一个临时的数据表供SQL语句的查询，该临时表并不是物理存在的，在应用运行结束后会动态销毁。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.registerTempTable(<span class="string">&quot;tempTable&quot;</span>);</span><br><span class="line"><span class="type">DataFrame</span> <span class="variable">newDf</span> <span class="operator">=</span> sqlContext.sql(<span class="string">&quot;select * from tempTable&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="show"><a href="#show" class="headerlink" title="show"></a>show</h3><p>  对DataFrame进行show操作是展示所有DataFrame中的数据，类似于对数据表数据的展示。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.show();</span><br></pre></td></tr></table></figure>

<h3 id="printSchema"><a href="#printSchema" class="headerlink" title="printSchema"></a>printSchema</h3><p>  打印DataFrame的元数据信息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema();</span><br></pre></td></tr></table></figure>

<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><p>  查询DataFrame中的列数据，查询一列</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;name&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>或者查询多列</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(df.col(<span class="string">&quot;name&quot;</span>), df.col(<span class="string">&quot;age&quot;</span>));</span><br></pre></td></tr></table></figure>

<h3 id="filter-1"><a href="#filter-1" class="headerlink" title="filter"></a>filter</h3><p>  DataFrame数据过滤</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.filter(df.col(<span class="string">&quot;age&quot;</span>).gt(<span class="number">18</span>));</span><br></pre></td></tr></table></figure>

<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><p>  数据分组</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(df.col(<span class="string">&quot;age&quot;</span>));</span><br></pre></td></tr></table></figure>

<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><hr>
<p>  Spark Streaming是Spark提供的一个实时计算的模块，它可以应用于大规模、高吞吐量、容错的实时数据流处理。</p>
<h3 id="SparkStreamingContext"><a href="#SparkStreamingContext" class="headerlink" title="SparkStreamingContext"></a>SparkStreamingContext</h3><hr>
<p>  要运行Spark Streaming应用，必须创建一个SparkStreamingContext对象，SparkStreamingContext对象类似于SparkContext，和SparkContext不同的是，SparkStreamingContext对象除了接收一个SparkConf实例外，还需要接收一个整型参数，定义每收集多长时间的数据划分为一个batch</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">JavaStreamingContext</span> <span class="variable">jsc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaStreamingContext</span>(conf, Durations.seconds(<span class="number">1</span>));</span><br></pre></td></tr></table></figure>

<p>并且在实现计算过程后需要手动启动SparkStreamingContext进行数据流的监控</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jsc.start();</span><br><span class="line">jsc.awaitTermination();</span><br></pre></td></tr></table></figure>

<h3 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h3><hr>
<p>  DStream表示一个从数据源来的持续不断的实时数据流，根据SparkStreamingContext中设置的数据划分参数，将每隔一个单位时间接收到的数据流数据封装成一个DStream，并交给应用进行实时计算</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaReceiverInputDStream&lt;String&gt; lines = jsc.socketTextStream(<span class="string">&quot;local&quot;</span>, <span class="number">80</span>);</span><br></pre></td></tr></table></figure>

<p>使用socket的方式监听数据流，并封装到JavaReceiverInputDStream对象，可以理解JavaReceiverInputDStream为一个元素为String类型的RDD，然后可以直接进行RDD运算</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaDStream&lt;String&gt; words = lines.flatMap(</span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;()&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title function_">call</span><span class="params">(String t)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">      <span class="keyword">return</span> Arrays.asList(t.split(<span class="string">&quot;&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>注意的是这里的RDD对象不再是之前的RDD，都变成DStream的形式。</p>
<h3 id="Hdfs"><a href="#Hdfs" class="headerlink" title="Hdfs"></a>Hdfs</h3><p>  将Hdfs作为Spark Streaming的输入源比较特殊，该方式没有Receiver的参与，是对一个Hdfs目录的监控，只要出现新的文件就进行实时处理，并且目录中的每个文件只会处理一次</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaDStream&lt;String&gt; lines = jsc.textFileStream(<span class="string">&quot;hdfs://spark1:9000/spark&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>  将其他的数据源作为Spark Streaming的输入源时往往需要借助一些第三方开发的类包。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/06/02/Spark%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/06/02/Spark%E6%A6%82%E8%BF%B0/" class="post-title-link" itemprop="url">Spark概述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-06-02 18:17:58" itemprop="dateCreated datePublished" datetime="2017-06-02T18:17:58+08:00">2017-06-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2017-06-03 13:58:02" itemprop="dateModified" datetime="2017-06-03T13:58:02+08:00">2017-06-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.2k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类似Hadoop MapReduce的通用并行框架，它拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是Job中间输出结果可以保存在内存中，从而不需要读写Hdfs，因此Spark能更好的适用于数据挖掘与机器学习等需要迭代的MapReduce算法。</p>
<p>  Spark是在Scala语言中实现的，它将Scala用作其应用程序的框架。与Hadoop不同的是，Spark和Scala能够紧密集成，其中的Scala可以像操作本地集合对象一样轻松的操作分布式数据集。Spark其实是对Hadoop的补充，可以在Hadoop文件系统中并行运行，通过Mesos的第三方集群框架可以支持此行为。</p>
<p>  在Spark项目开源后，在2013年Spark成为了Apache基金会下的项目，由于其代码量少、轻量级和使用内存的特性受到大数据行业的关注，在众多第三方开发者的维护下，Spark进度高速发展期而成为了Apache基金会的顶级项目。</p>
<p>  Apache对Spark的定义为：通用的大数据快速处理引擎。Spark是一种专为大规模数据处理而设计的快速通用的计算引擎，使用一个技术堆栈就完美地解决大数据领域的各种计算任务。</p>
<h2 id="Spark组成"><a href="#Spark组成" class="headerlink" title="Spark组成"></a>Spark组成</h2><hr>
<p>  Spark是一个一站式的大数据计算框架，其中包括Spark Core、Spark Sql、Spark Streaming、MLlib和GraphX，分别在大数据领域中解决了离线批处理、交互式查询、实时流计算、机器学习与图计算的所有任务和问题。另外由于Spark基于内存存储的特性，使得Spark的计算性能超过MapReduce和Hive的的数倍以上，所以越来越多的IT公司使用Spark+Hadoop的组合来使用Spark进行大数据的计算任务，用Hadoop解决大数据的存储问题。</p>
<h2 id="Spark运行原理"><a href="#Spark运行原理" class="headerlink" title="Spark运行原理"></a>Spark运行原理</h2><hr>
<ol>
<li>我们将写好的Spark应用在Spark集群上的某个节点提交并开始执行；</li>
<li>应用被提交后，节点生成一个Driver，Driver启动后，对该Spark应用进行初始化操作，并请求集群Master节点进行Spark应用的注册，通知Master进行资源的分配；</li>
<li>Master节点在收到应用的注册请求后发送给所有节点的Worker，Worker进行资源的调整和分配，即启动Executor；</li>
<li>节点启动Executor后会与Driver通信进行反注册，即告知Driver在集群中哪些Executor是为该Driver服务的；</li>
<li>之后Driver开始读取应用中数据源中的数据来创建RDD；</li>
<li>Driver根据Spark应用中定义的对RDD的操作生成一大推Task并发送给Executor；</li>
<li>Executor接收到Task后，启动多个线程去执行收到的Task；</li>
<li>对RDD的partition数据执行完指定操作后形成新的RDD partition；</li>
</ol>
<h2 id="Spark中的RDD"><a href="#Spark中的RDD" class="headerlink" title="Spark中的RDD"></a>Spark中的RDD</h2><hr>
<p>  RDD是Spark提供的核心抽象概念，是Resillient Distributed Dataset的缩写，即弹性分布式数据集。</p>
<p>  RDD在抽象上说是一种元素的集合，包含了数据且是被分区的，其中每个分区被分布在集群中的不同节点上使得RDD中的数据可以被并行操作。</p>
<p>  RDD具备像MapReduce等数据流模型的容错特性，并允许开发人员在大型集群上执行基于内存的计算，因此有效解决了大数据计算中的迭代算法问题和交互式数据挖掘。</p>
<p>  RDD是只读的数据集合，是一种高度受限的共享内存，并且只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。</p>
<h2 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h2><hr>
<p>  Spark支持两种RDD操作：transformation和action。transformation操作会针对已有的RDD创建一个新的RDD；而action则主要是对RDD进行最后的操作，如遍历、reduce和保存到文件等。</p>
<p>  transformation操作有lazy特性，该特性表示如果一个Spark应用中只定义了transformation操作，那么即使你执行该该应用，这些操作也不会实际执行。换句话说transformation操作时不会触发Spark程序的执行，它们只是记录了对RDD的操作，只有当transformation操作之后接着执行一个action操作，那么Spark应用才会被实际执行。lazy这种特性使得Spark应用不会产生过多的中间结果。</p>
<p>  常用的transformation操作如下：</p>
<table>
<thead>
<tr>
<th align="center">操作</th>
<th align="left">简介</th>
</tr>
</thead>
<tbody><tr>
<td align="center">map</td>
<td align="left">将RDD中的每个元素传入自定义函数并获取一个新的元素，然后用所有的新元素组成新的RDD</td>
</tr>
<tr>
<td align="center">filter</td>
<td align="left">对RDD中每个元素进行判断，如果返回true则保留，否则被剔除</td>
</tr>
<tr>
<td align="center">flatMap</td>
<td align="left">与map类似，但是对每一个元素都可以返回一个或多个新的元素</td>
</tr>
<tr>
<td align="center">groupByKey</td>
<td align="left">根据key进行分组，然后每个key对应一个Iterable<value></td>
</tr>
<tr>
<td align="center">reduceByKey</td>
<td align="left">对每个key对应的value进行reduce操作</td>
</tr>
<tr>
<td align="center">sortByKey</td>
<td align="left">对每个key对应的value进行排序操作</td>
</tr>
<tr>
<td align="center">join</td>
<td align="left">对两个包含&lt;key, value&gt;对的RDD进行join操作，每个key join上的pair，都会传入自定义函数进行处理</td>
</tr>
<tr>
<td align="center">cogroup</td>
<td align="left">同join，但是每个key对应的Iterable<value>都会传入自定义函数进行处理</td>
</tr>
</tbody></table>
<p>  常用action操作如下：</p>
<table>
<thead>
<tr>
<th align="center">操作</th>
<th>简介</th>
</tr>
</thead>
<tbody><tr>
<td align="center">reduce</td>
<td>将RDD中的所有元素进行聚合操作，第一个和第二个元素聚合后产生的值与第三个元素聚合，以此类推</td>
</tr>
<tr>
<td align="center">collect</td>
<td>将RDD中所有元素获取到本地客户端</td>
</tr>
<tr>
<td align="center">count</td>
<td>获取RDD元素</td>
</tr>
<tr>
<td align="center">take(n)</td>
<td>获取RDD中前n个元素</td>
</tr>
<tr>
<td align="center">saveAsTextFile</td>
<td>将RDD元素保存到文件中，对每个元素调用toString方法</td>
</tr>
<tr>
<td align="center">countByKey</td>
<td>对每个key对应的值进行count计数</td>
</tr>
<tr>
<td align="center">foreach</td>
<td>遍历RDD中的每个元素</td>
</tr>
</tbody></table>
<h2 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h2><hr>
<p>  RDD的持久化功能室Spark中非常重要的一个功能特性，由于Spark底层的固有特性，当创建了一个RDD之后，使用transformation对该RDD执行下一步操作后，之前的RDD是会被销毁的，也就是Spark默认当对现有的一个RDD进行transformation操作后上一个RDD被处理为没用的RDD而被马上销毁，但是如果在特定业务场景下如果对现有的RDD会有两次以上的操作时，销毁会被认为是没必要的，如果要对RDD进行第二次transformation操作时由于该RDD已经被Spark销毁，所以必须重新构建，如果在一个很大的数据中去构建一个RDD两次以上，这对Spark应用的性能上来说无非是灾难性的，所以Spark会提供一个RDD持久化的功能，这样就保证二次以上的RDD操作无需重新构建RDD，而是直接从被持久化的RDD中进行transformation操作。</p>
<p>  对RDD执行持久化操作时，每个节点都会将自己操作RDD的partition持久化到内存或硬盘中供之后对该RDD进行反复操作，使二次以上的RDD操作不需要重新构建该RDD而是直接从缓存中读取对应节点操作的partition部分。</p>
<h2 id="RDD持久化策略"><a href="#RDD持久化策略" class="headerlink" title="RDD持久化策略"></a>RDD持久化策略</h2><hr>
<p>  RDD持久化是可以手动指定不同的持久化策略的，如果可以将RDD持久化到内存中、到磁盘中、使用序列化的方式等。</p>
<p>  RDD持久化的策略列表如下：</p>
<table>
<thead>
<tr>
<th align="center">持久化策略</th>
<th>简介</th>
</tr>
</thead>
<tbody><tr>
<td align="center">MEMORY_ONLY</td>
<td>以非序列化的Java对象持久化在JVM内存中，如果内存无法完全存储RDD所有的partition，那么那些没用持久化的partition就会在下一次需要使用它的时候重新被计算</td>
</tr>
<tr>
<td align="center">MEMORY_AND_DISK</td>
<td>同上MEMORY_ONLY，但是当某些partition无法存储在内存中时，会持久化到磁盘中，下次需要使用这些partition时从磁盘读取</td>
</tr>
<tr>
<td align="center">MEMORY_ONLY_SER</td>
<td>同上MEMORY_ONLY，但是会使用Java序列化的方式，将Java对象序列化之后进行持久化，可以减少内存开销，但是在使用时会进行反序列化操作，所以会加大CPU的开销</td>
</tr>
<tr>
<td align="center">MEMORY_AND_DSK_SER</td>
<td>同MEMORY_AND_DISK，但是使用序列化方式持久化Java对象</td>
</tr>
<tr>
<td align="center">DISK_ONLY</td>
<td>使用非序列化Java对象的方式持久化，完全存储到磁盘上。</td>
</tr>
<tr>
<td align="center">MEMORY_ONLY_2、MEMORY_AND_DISK_2、等等</td>
<td>如果尾部加了2的持久化策略，表示会将持久化数据复制一份保存到其他节点而保障数据的容错性能</td>
</tr>
</tbody></table>
<h2 id="Spark中的共享变量"><a href="#Spark中的共享变量" class="headerlink" title="Spark中的共享变量"></a>Spark中的共享变量</h2><hr>
<p>  在日常的生产开发中，经常会使用全局变量的情况，在Spark应用中也会有使用全局变量的业务场景，比如全局范围内统计等等，这里的全局变量就是Spark中的共享变量。</p>
<p>  在Spark中如果要使用一个外部变量时的默认的做法是将该变量拷贝到每个节点中的每个task中，也就是说每个task操作的只是该变量的一个拷贝副本，还达不到共享的功能。当然Spark也解决了使用共享变量的特性。Spark提供了两种共享变量的使用，一种时广播变量（Broadcast Variable），另一种时累加变量（Accumulator Variable）。广播变量会将使用到的变量为每个节点拷贝一份，而累加变量则可以让多个task共同操作一份变量。</p>
<p>  广播变量是只读的，并且每个节点只会存在一份副本，而不会为每个task都拷贝一份，这样最大限度的减少了变量到每个节点的网络传输消耗和节点上的内存消耗。</p>
<p>  累加变量用于多个节点对一个变量进行共享性的操作，值得注意的是Spark中对累加变量的操作只限与累加并且不能读取该累加变量的值，只有Driver程序可以。所以Spark中的累加变量类似于Hadoop MapReduce中的计数器组件。</p>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><hr>
<p>  提到Spark SQL的由来我们必须先说说Hive，Hadoop的Hive是让那些不熟悉Java的数据分析师在无法深入进行MapReduce编程时可以使用他们熟悉的关系型数据库的SQL模型来进行操作Hdfs数据完成数据仓库的建模和建设并针对数据仓库的数据进行统计和分析使用，但是Hive的底层基于MapReduce，因为MapReduce计算是基于磁盘IO和网络IO的，所以导致Hive性能的底下。</p>
<p>  Spark起初在Hive的基础上推出了Shark，Shark基于Hive的语法解析器和查询优化器等组件但是修改了内存管理、物理计划和执行三个模块并底层使用Spark基于内存的计算模型使得Shark的计算性能较Hive有很大的提升，但是由于Shark和Hive的关系紧密，所以对Shark性能的提升还是造成了制约，所以Spark推出了全新的Spark SQL，并且Spark SQL可以支持更多的数据源查询，如Hive、RDD、Parquet、Json和JDBC等。</p>
<p>  Spark SQL是Spark中的一个模块，主要用于进行结构化数据的处理。</p>
<h2 id="Spark-SQL中的DataFrame"><a href="#Spark-SQL中的DataFrame" class="headerlink" title="Spark SQL中的DataFrame"></a>Spark SQL中的DataFrame</h2><hr>
<p>  DataFrame是Spark SQL中最核心的编程抽象，是以列的形式组织的分布式的数据结合，DataFrame和关系型数据库中的表类似。DataFrame可以通过很多的数据源进行构建。</p>
<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><hr>
<p>  Spark Streaming是Spark Core的一种拓展，它可以用于进行大规模、高吞吐量、容错的实时数据流的处理。Spark Streaming支持从很多数据源中读取数据。常见的如Kafka、Flume、Twitter、ZeroMQ、Kinesis以及Tcp Socket。</p>
<p>  Spark Streaming的工作流程为接收一个实时输入数据流，然后用时间关系将接收到的数据拆分为多个batch，比如每收集1秒然后将数据封装为一个batch，然后将每个batch交给Spark进行计算处理，最后会生成一个结果数据流并且也是由一个一个batch组成。</p>
<h2 id="Spark-Streaming中的DStream"><a href="#Spark-Streaming中的DStream" class="headerlink" title="Spark Streaming中的DStream"></a>Spark Streaming中的DStream</h2><hr>
<p>  DStream类似于Spark SQL中的DataFrame，也是一种数据的高级抽象概念，通俗的被叫做离散流，表示为一个持续不断的数据流。DStream可以通过输入数据源来创建，也可以通过其他DStream应用的高阶函数来创建。其实在DStream的内部是一系列持续不断产生的RDD，每个RDD中都办函了一个时间段内的数据。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/05/17/%E6%A0%91%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/17/%E6%A0%91%E5%9B%9E%E5%BD%92/" class="post-title-link" itemprop="url">树回归</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-17 16:37:43" itemprop="dateCreated datePublished" datetime="2017-05-17T16:37:43+08:00">2017-05-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2018-10-10 16:00:16" itemprop="dateModified" datetime="2018-10-10T16:00:16+08:00">2018-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.3k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  在机器学习算法中，树的算法模型在分类判定和回归问题都可以使用，ID3算法就是一个构造决策树模型的分类判定算法，这里我们讨论决策树中的CART算法。ID3和CART不同在ID3算法是每次选取当前最佳的特征来做数据分隔，并且会按照该特征的所有特征值做数据集切分，在切分过程中，如果按照一个特征切分后，该特征在之后的算法切分中不会再起作用，所以ID3算法不能直接处理连续型数据的特征，在连续型特征的样本集中使用ID3算法，必须提前将样本集数据处理为离散型数据且这么做会破坏样本集数据的某种潜在规律。</p>
<p>  还有种切分数据的方式就是二元切分法，实现为每次将数据集切分为两部分，根据特征值的切分条件，所有数据分别切分为数据的左子树和右子树。</p>
<h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><hr>
<p>  回归树的原理和ID3算法的构建相似，在树的叶子节点位置存储算法的结果。在回归树中，因为处理的是连续型样本集数据，所以在构建树后，叶子节点为一个单值。</p>
<p>  回归树的实现原理是：迭代使用数据集特征和特质值做数据切分，根据数据切分方式将数据切分为左子树和右子树，然后分别计算左子树和右子树的数据集误差，选择误差小的一个做为最佳切分特征和特征值并且保持切分过程信息，直到切分后的子树数据集中值相等或者是达到我们的停止条件，然后计算数据集的均值作为叶子节点，这样迭代完成后就构成一个可供使用的回归树。</p>
<h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><hr>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择最佳分隔特征和特征值实现方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestSplit</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 最小子树误差，即迭代停止条件</span></span><br><span class="line">    tolS = <span class="number">1</span> </span><br><span class="line">    <span class="comment"># 最小子树大小，迭代停止条件</span></span><br><span class="line">    tolN = <span class="number">4</span></span><br><span class="line">    <span class="comment"># 如果给定样本集所有值相等，则表示到达叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">set</span>(dataSet[:,-<span class="number">1</span>].T.tolist()[<span class="number">0</span>])) == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 返回分隔特征为空和当前样本集中的平均值</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, mean(dataSet[:,-<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 样本集规格</span></span><br><span class="line">    m,n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 样本集原始误差，此处使用样本集均方差，乘以样本集个数表示样本集误差</span></span><br><span class="line">    S = var(dataSet[:,-<span class="number">1</span>]) * shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 最小误差，初始化为无限大，因为要找最小</span></span><br><span class="line">    bestS = inf</span><br><span class="line">    <span class="comment"># 最佳分隔特征</span></span><br><span class="line">    bestIndex = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 最佳分隔特征的特征值</span></span><br><span class="line">    bestValue = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环所有特征</span></span><br><span class="line">    <span class="keyword">for</span> featIndex <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 循环所有特征值</span></span><br><span class="line">        <span class="keyword">for</span> splitVal <span class="keyword">in</span> <span class="built_in">set</span>(dataSet[:,featIndex]):</span><br><span class="line">            <span class="comment"># 按照特征和特征值做数据集二元切分</span></span><br><span class="line">            mat0 = dataSet[nonzero(dataSet[:,featIndex] &gt; splitVal)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">            mat1 = dataSet[nonzero(dataSet[:,featIndex] &lt;= splitVal)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 判断终止条件</span></span><br><span class="line">            <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN): </span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 计算子集误差值和作为新的误差值</span></span><br><span class="line">            newS = var(mat0[:,-<span class="number">1</span>]) * shape(mat0)[<span class="number">0</span>] + var(mat1[:,-<span class="number">1</span>]) * shape(mat1)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 选择一个最小误差</span></span><br><span class="line">            <span class="keyword">if</span> newS &lt; bestS: </span><br><span class="line">                bestIndex = featIndex</span><br><span class="line">                bestValue = splitVal</span><br><span class="line">                bestS = newS</span><br><span class="line">    <span class="comment"># 如果误差值小于我们提供的最小误差，则表示到叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> (S - bestS) &lt; tolS: </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, var(dataSet[:,-<span class="number">1</span>]) * shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 根据最佳分隔特征和特征值做数据集切分</span></span><br><span class="line">    mat0 = dataSet[nonzero(dataSet[:,bestIndex] &gt; bestValue)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">    mat1 = dataSet[nonzero(dataSet[:,bestIndex] &lt;= bestValue)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 判断终止切割数据集条件</span></span><br><span class="line">    <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, var(dataSet[:,-<span class="number">1</span>]) * shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> bestIndex,bestValue</span><br><span class="line"><span class="comment"># 创建决策树方法</span></span><br><span class="line"><span class="comment"># dataSet：样本集数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 选取最佳分隔特征和特征值</span></span><br><span class="line">    feat, val = chooseBestSplit(dataSet)</span><br><span class="line">    <span class="comment"># 如果没有最佳切割特征，则表示到叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> feat == <span class="literal">None</span>: </span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line">    <span class="comment"># 树词典</span></span><br><span class="line">    retTree = &#123;&#125;</span><br><span class="line">    <span class="comment"># 保存树分隔过程信息</span></span><br><span class="line">    retTree[<span class="string">&#x27;spInd&#x27;</span>] = feat</span><br><span class="line">    retTree[<span class="string">&#x27;spVal&#x27;</span>] = val</span><br><span class="line">    lSet = dataSet[nonzero(dataSet[:,feat] &gt; val)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">    rSet = dataSet[nonzero(dataSet[:,feat] &lt;= val)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 迭代调用树创建方法，进行树枝干部分的构建</span></span><br><span class="line">    retTree[<span class="string">&#x27;left&#x27;</span>] = createTree(lSet)</span><br><span class="line">    retTree[<span class="string">&#x27;right&#x27;</span>] = createTree(rSet)</span><br><span class="line">    <span class="keyword">return</span> retTree  </span><br></pre></td></tr></table></figure>

<h2 id="树剪枝"><a href="#树剪枝" class="headerlink" title="树剪枝"></a>树剪枝</h2><hr>
<p>  树剪枝指降低构造的决策树复杂度，避免拟合过程的过拟合现象，过拟合表现为决策树的节点过多。</p>
<h2 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h2><hr>
<p>  预剪枝表示为在算法训练的过程中进行决策树剪枝，方法就是调整算法训练中的停止条件，即算法参数来完成。但是我们并不知道要将算法训练结果调整到一个什么样的状态。并且调整上述算法参数也是很费时间的，所以我们还可以使用后剪枝。</p>
<h2 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h2><hr>
<p>  后剪枝顾名思义就是在算法训练完成以后进行树剪枝，后剪枝方法的实现原理是使用一个测试样本集对算法训练生成的决策树进行测试，然后通过对测试样本集根据已经训练好的决策树进行分隔，并且迭代到叶子节点处，然后计算两个叶子节点合并后的误差是不是比不合并的误差要小，小的话即进行两个叶子节点的合并完成树剪枝。</p>
<h2 id="算法实现-1"><a href="#算法实现-1" class="headerlink" title="算法实现"></a>算法实现</h2><hr>
<h3 id="Python-1"><a href="#Python-1" class="headerlink" title="Python"></a>Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isTree</span>(<span class="params">obj</span>):</span><br><span class="line">    <span class="keyword">return</span> (<span class="built_in">type</span>(obj).__name__==<span class="string">&#x27;dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getMean</span>(<span class="params">tree</span>):</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;right&#x27;</span>] = getMean(tree[<span class="string">&#x27;right&#x27;</span>])</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;left&#x27;</span>] = getMean(tree[<span class="string">&#x27;left&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> (tree[<span class="string">&#x27;left&#x27;</span>]+tree[<span class="string">&#x27;right&#x27;</span>])/<span class="number">2.0</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prune</span>(<span class="params">tree, testData</span>):</span><br><span class="line">    <span class="keyword">if</span> shape(testData)[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> getMean(tree)</span><br><span class="line">    <span class="keyword">if</span> (isTree(tree[<span class="string">&#x27;right&#x27;</span>]) <span class="keyword">or</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>])):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">&#x27;spInd&#x27;</span>], tree[<span class="string">&#x27;spVal&#x27;</span>])</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;left&#x27;</span>] = prune(tree[<span class="string">&#x27;left&#x27;</span>], lSet)</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;right&#x27;</span>] = prune(tree[<span class="string">&#x27;right&#x27;</span>], rSet)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]) <span class="keyword">and</span> <span class="keyword">not</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">&#x27;spInd&#x27;</span>], tree[<span class="string">&#x27;spVal&#x27;</span>])</span><br><span class="line">        errorNoMerge = <span class="built_in">sum</span>(power(lSet[:,-<span class="number">1</span>] - tree[<span class="string">&#x27;left&#x27;</span>],<span class="number">2</span>)) + <span class="built_in">sum</span>(power(rSet[:,-<span class="number">1</span>] - tree[<span class="string">&#x27;right&#x27;</span>],<span class="number">2</span>))</span><br><span class="line">        treeMean = (tree[<span class="string">&#x27;left&#x27;</span>]+tree[<span class="string">&#x27;right&#x27;</span>])/<span class="number">2.0</span></span><br><span class="line">        errorMerge = <span class="built_in">sum</span>(power(testData[:,-<span class="number">1</span>] - treeMean,<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">if</span> errorMerge &lt; errorNoMerge: </span><br><span class="line">            <span class="keyword">return</span> treeMean</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> tree</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> tree</span><br></pre></td></tr></table></figure>

<h2 id="模型树"><a href="#模型树" class="headerlink" title="模型树"></a>模型树</h2><hr>
<p>  用树来对数据建模，除了把叶节点简单地设定为常数值外，我们还可以把叶节点设定为分段的线性函数，分段线性表示该模型由多个线性片段组成。</p>
<h2 id="算法实现-2"><a href="#算法实现-2" class="headerlink" title="算法实现"></a>算法实现</h2><hr>
<h3 id="Python-2"><a href="#Python-2" class="headerlink" title="Python"></a>Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># 线性函数，即叶子节点的模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linearSolve</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 数据子集规格</span></span><br><span class="line">    m,n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 构造线性函数的样本集</span></span><br><span class="line">    X = mat(ones((m,n)))</span><br><span class="line">    <span class="comment"># 构造线性函数样本集对应的类别</span></span><br><span class="line">    Y = mat(ones((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 根据子集填充线性函数样本集</span></span><br><span class="line">    X[:,<span class="number">1</span>:n] = dataSet[:,<span class="number">0</span>:n-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 根据子集填充线性函数样本集类别</span></span><br><span class="line">    Y = dataSet[:,-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 根据线性函数公式计算回归系数</span></span><br><span class="line">    xTx = X.T*X</span><br><span class="line">    <span class="comment"># 检测矩阵是否可逆</span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">&#x27;This matrix is singular, cannot do inverse,\n try increasing the second value of ops&#x27;</span>)</span><br><span class="line">    <span class="comment"># 根据线性回归公式计算回归系数</span></span><br><span class="line">    ws = xTx.I * (X.T * Y)</span><br><span class="line">    <span class="keyword">return</span> ws,X,Y</span><br><span class="line"><span class="comment"># 叶子节点内容，即该叶子节点对应的线性回归函数的最佳回归系数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">modelLeaf</span>(<span class="params">dataSet</span>):</span><br><span class="line">    ws,X,Y = linearSolve(dataSet)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"><span class="comment"># 计算子集误差，差值平方求和</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">modelErr</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 根据当前子集调用线性回归函数</span></span><br><span class="line">    ws,X,Y = linearSolve(dataSet)</span><br><span class="line">    <span class="comment"># 预测结果</span></span><br><span class="line">    yHat = X * ws</span><br><span class="line">    <span class="comment"># 计算误差</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(power(Y - yHat,<span class="number">2</span>))</span><br><span class="line"><span class="comment"># 选择最佳分隔特征和特征值实现方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestSplit</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 最小子树误差，即迭代停止条件</span></span><br><span class="line">    tolS = <span class="number">1</span> </span><br><span class="line">    <span class="comment"># 最小子树大小，迭代停止条件</span></span><br><span class="line">    tolN = <span class="number">4</span></span><br><span class="line">    <span class="comment"># 如果给定样本集所有值相等，则表示到达叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">set</span>(dataSet[:,-<span class="number">1</span>].T.tolist()[<span class="number">0</span>])) == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 返回分隔特征为空和当前样本集中的平均值</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, mean(dataSet[:,-<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 样本集规格</span></span><br><span class="line">    m,n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 样本集原始误差</span></span><br><span class="line">    S = modelErr(dataSet)</span><br><span class="line">    <span class="comment"># 最小误差，初始化为无限大，因为要找最小</span></span><br><span class="line">    bestS = inf</span><br><span class="line">    <span class="comment"># 最佳分隔特征</span></span><br><span class="line">    bestIndex = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 最佳分隔特征的特征值</span></span><br><span class="line">    bestValue = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环所有特征</span></span><br><span class="line">    <span class="keyword">for</span> featIndex <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 循环所有特征值</span></span><br><span class="line">        <span class="keyword">for</span> splitVal <span class="keyword">in</span> <span class="built_in">set</span>(dataSet[:,featIndex]):</span><br><span class="line">            <span class="comment"># 按照特征和特征值做数据集二元切分</span></span><br><span class="line">            mat0 = dataSet[nonzero(dataSet[:,featIndex] &gt; splitVal)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">            mat1 = dataSet[nonzero(dataSet[:,featIndex] &lt;= splitVal)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 判断终止条件</span></span><br><span class="line">            <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 计算子集误差值和作为新的误差值</span></span><br><span class="line">            newS = modelErr(mat0) + modelErr(mat1)</span><br><span class="line">            <span class="comment"># 选择一个最小误差</span></span><br><span class="line">            <span class="keyword">if</span> newS &lt; bestS: </span><br><span class="line">                bestIndex = featIndex</span><br><span class="line">                bestValue = splitVal</span><br><span class="line">                bestS = newS</span><br><span class="line">    <span class="comment"># 如果误差值小于我们提供的最小误差，则表示到叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> (S - bestS) &lt; tolS: </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, modelLeaf(dataSet)</span><br><span class="line">    <span class="comment"># 根据最佳分隔特征和特征值做数据集切分</span></span><br><span class="line">    mat0 = dataSet[nonzero(dataSet[:,bestIndex] &gt; bestValue)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">    mat1 = dataSet[nonzero(dataSet[:,bestIndex] &lt;= bestValue)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 判断终止切割数据集条件</span></span><br><span class="line">    <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, modelLeaf(dataSet)</span><br><span class="line">    <span class="keyword">return</span> bestIndex,bestValue</span><br><span class="line"><span class="comment"># 创建决策树方法</span></span><br><span class="line"><span class="comment"># dataSet：样本集数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 选取最佳分隔特征和特征值</span></span><br><span class="line">    feat, val = chooseBestSplit(dataSet)</span><br><span class="line">    <span class="comment"># 如果没有最佳切割特征，则表示到叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> feat == <span class="literal">None</span>: </span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line">    <span class="comment"># 树词典</span></span><br><span class="line">    retTree = &#123;&#125;</span><br><span class="line">    <span class="comment"># 保存树分隔过程信息</span></span><br><span class="line">    retTree[<span class="string">&#x27;spInd&#x27;</span>] = feat</span><br><span class="line">    retTree[<span class="string">&#x27;spVal&#x27;</span>] = val</span><br><span class="line">    lSet = dataSet[nonzero(dataSet[:,feat] &gt; val)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">    rSet = dataSet[nonzero(dataSet[:,feat] &lt;= val)[<span class="number">0</span>],:][<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 迭代调用树创建方法，进行树枝干部分的构建</span></span><br><span class="line">    retTree[<span class="string">&#x27;left&#x27;</span>] = createTree(lSet)</span><br><span class="line">    retTree[<span class="string">&#x27;right&#x27;</span>] = createTree(rSet)</span><br><span class="line">    <span class="keyword">return</span> retTree</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/05/16/%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/16/%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%9B%9E%E5%BD%92/" class="post-title-link" itemprop="url">预测数值型数据：回归</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-16 14:42:40" itemprop="dateCreated datePublished" datetime="2017-05-16T14:42:40+08:00">2017-05-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2017-05-17 16:36:58" itemprop="dateModified" datetime="2017-05-17T16:36:58+08:00">2017-05-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.8k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  回归算法适用于好多算法环境，比如做分类预测的Logistic回归，下面我们介绍的所有内容都是进行数值型预测的回归算法。</p>
<p>  回归算法就是对给定数据样本集数据进行拟合，我们将拟合的过程叫做回归。回归算法的目的就是找到一组最佳回归系数，然后通过回归系数构建一个预测函数，通过输入待预测样本来求解预测结果。回归算法中常用的函数基本形式为<br>$$<br>y&#x3D;w_0+w_1x_1+w_2x_2+ \cdots +w_nx_n<br>$$<br>预测函数中$w$的值就表示我们需要训练的回归系数，使用矩阵的方式，预测函数可以被表示为<br>$$<br>y&#x3D;w^Tx\<br>s.t. \quad x_1&#x3D;1<br>$$</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><hr>
<p>  在数值型预测的回归算法中，我们给定训练数据样本集，然后通过算法训练出使得预测函数的预测误差最小的最佳回归系数$w$，预测结果$y$和真实结果$y$之间的差值为我们评测回归系数好坏的唯一标准，我们采用平方误差计算样本集训练误差<br>$$<br>\epsilon &#x3D; \sum_{i&#x3D;1}^n(y_i-x_i^Tw)^2<br>$$<br>同样，使用矩阵的方式表示为<br>$$<br>\epsilon &#x3D; (y-x^Tw)^T(y-x^Tw)<br>$$<br>我们要找到$\epsilon$的最小值，所以$\epsilon$对$w$求导且等于0可以得出$w$的最优解方程<br>$$<br>\hat w &#x3D; (x^Tx)^{-1}x^Ty<br>$$<br>由于方程中$(x^Tx)^{-1}$是一个对矩阵求逆的计算，我们知道并不是所有的矩阵都可以求逆，所以在算法训练中需要确认是否可以对该矩阵求逆。</p>
<h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><hr>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 样本集数据</span></span><br><span class="line">xMat = mat([[...], []])</span><br><span class="line"><span class="comment"># 对应样本集数据的类别</span></span><br><span class="line">yMat = mat([[...], []])</span><br><span class="line"><span class="comment"># 根据系数求解方程计算回归系数</span></span><br><span class="line">xTx = xMat.T*xMat</span><br><span class="line"><span class="comment"># 检测矩阵是否可以求逆</span></span><br><span class="line"><span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"><span class="comment"># 根据系数求解方程计算回归系数</span></span><br><span class="line">ws = xTx.I * (xMat.T*yMat)</span><br></pre></td></tr></table></figure>

<center><img data-src="/2017/05/16/%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%9B%9E%E5%BD%92/QQ20170516-152457@2x.png" class="" title="线性回归结果"></center>

<p>  观察上图中线性回归中的算法训练结果，虽然样本集经过拟合，已经完全可以做结果预测，但是直观感受上好像现在的回归算法结果有点“鲁莽”，就是不能取得一个很好的预测结果，这样的拟合情况我们称之为<code>欠拟合</code>，产生欠拟合是因为线性算法求的是具有最小均方误差的无偏估计，接下来我们研究一些可以更好拟合的回归算法。</p>
<h2 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h2><hr>
<p>  由于没有考虑估计偏差线性回归会出现欠拟合现象，因此在局部加权线性回归算法我们引入一些偏差来降低预测中的均方误差。</p>
<p>  局部加权线性回归算法中，我们给待预测点附近的每个点赋予一定的权重$W$，然后将最小均方误差作用于待预测点附近的这些点。这样对于每个不一样的待预测点的回归系数都是不同的。添加样本集中点的权重$W$后，回归系数的计算公式更新为<br>$$<br>\hat w &#x3D; (x^TWx)^{-1}x^TWy<br>$$<br>公式中样本集点的权重值我们使用核给待预测点附近的所有点一个权重值，这里我们使用高斯核<br>$$<br>W(i,j) &#x3D; exp\left(\frac {\mid x_i-x\mid}{-2k^2}\right)<br>$$</p>
<p>高斯核构建了一个对角元素的权重矩阵$W$，并且样本集中点的$x$距离点$x_i$越近，该点的权重$W(i,i)$将会越大，并且高斯核引入一个指定参数$k$来决定对附近的点赋予多大的权重。</p>
<h2 id="算法实现-1"><a href="#算法实现-1" class="headerlink" title="算法实现"></a>算法实现</h2><hr>
<h3 id="Python-1"><a href="#Python-1" class="headerlink" title="Python"></a>Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 样本集数据矩阵</span></span><br><span class="line">xMat = mat([[...],[...]])</span><br><span class="line"><span class="comment"># 样本集数据对应的样本类别</span></span><br><span class="line">yMat = mat([[...],[...]])</span><br><span class="line"><span class="comment"># 待预测样本点</span></span><br><span class="line">testPoint = [...]</span><br><span class="line"><span class="comment"># 高斯核自定义参数</span></span><br><span class="line">k = <span class="number">0.01</span></span><br><span class="line"><span class="comment"># 样本集尺寸</span></span><br><span class="line">m = shape(xMat)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 初始化所有样本集数据点的权重对角线矩阵</span></span><br><span class="line">weights = mat(eye((m)))</span><br><span class="line"><span class="comment"># 使用循环对每个数据点赋予一个权重</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    <span class="comment"># 根据高斯核函数公式进行计算</span></span><br><span class="line">    diffMat = testPoint - xMat[j,:]</span><br><span class="line">    weights[j,j] = exp(diffMat*diffMat.T/(-<span class="number">2.0</span>*k**<span class="number">2</span>))</span><br><span class="line"><span class="comment"># 根据回归系数计算公式计算回归系数</span></span><br><span class="line">xTx = xMat.T * (weights * xMat)</span><br><span class="line"><span class="comment"># 检测是否可以对矩阵求逆</span></span><br><span class="line"><span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"><span class="comment"># 根据回归系数计算公式计算回归系数</span></span><br><span class="line">ws = xTx.I * (xMat.T * (weights * yMat))</span><br><span class="line"><span class="comment"># 对待预测样本进行结果预测</span></span><br><span class="line">y = testPoint * ws</span><br></pre></td></tr></table></figure>

<center><img data-src="/2017/05/16/%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%9B%9E%E5%BD%92/QQ20170516-161658@2x.png" class="" title="局部加权线性回归结果"></center>

<p>  观察上图的算法训练结果可知，局部加权线性回归算法比线性回归算法的预测结果合理了很多并且解决了欠拟合的情况，但是由于自定义参数$k$的可变性，所以在实际算法中需要修改参数$k$的值进行多次训练找到一个最佳拟合曲线，同时由于$k$取值的不同，可能还会出现欠拟合和过拟合的情形，欠拟合在线性回归中讲过，过拟合产生的原因是算法考虑了太多的噪声点而导致的。</p>
<h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><hr>
<p>  试想如果我们获取到的算法训练样本集中样本特征数量比我们样本集大小还大，那么我们在使用回归系数求解公式中的求逆运算$(x^Tx)^{-1}$会出问题，因为输入数据的矩阵不是满秩矩阵，而非满秩矩阵在求逆时会出问题。<code>岭回归</code>就是解决这个问题的。</p>
<p>  岭回归就是在矩阵$x^Tx$上加一个$\lambda I$使矩阵非奇异，这样就可以对$x^Tx+\lambda I$顺利求逆。其中矩阵$I$是一个$m\times m$的单位矩阵，并且除对角线元素全为1外其他元素都为0，所以回归公式被更新为<br>$$<br>\hat w &#x3D; (x^Tx+\lambda I)^{-1}x^Ty<br>$$<br>其中参数$\lambda$为自定义参数，对于不同的$\lambda$算法的预测误差也是不同的。</p>
<h2 id="算法实现-2"><a href="#算法实现-2" class="headerlink" title="算法实现"></a>算法实现</h2><hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算回归系数函数</span></span><br><span class="line"><span class="comment"># xMat：样本集</span></span><br><span class="line"><span class="comment"># yMat：样本集对应类别矩阵</span></span><br><span class="line"><span class="comment"># lam：岭回归中的自定义参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ridgeRegres</span>(<span class="params">xMat,yMat,lam=<span class="number">0.2</span></span>):</span><br><span class="line">    <span class="comment"># 下面完全根据公式计算回归系数</span></span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    denom = xTx + eye(shape(xMat)[<span class="number">1</span>])*lam</span><br><span class="line">    <span class="comment"># 避免lam参数为0使的不可逆</span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(denom) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = denom.I * (xMat.T*yMat)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"><span class="comment"># 岭回归测试函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ridgeTest</span>(<span class="params">xArr,yArr</span>):</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat=mat(yArr).T</span><br><span class="line">    <span class="comment"># 接下来的计算是为了实现样本集数据标准化</span></span><br><span class="line">    <span class="comment"># 计算平均值</span></span><br><span class="line">    yMean = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yMean</span><br><span class="line">    xMeans = mean(xMat,<span class="number">0</span>)</span><br><span class="line">    xVar = var(xMat,<span class="number">0</span>)</span><br><span class="line">    xMat = (xMat - xMeans)/xVar</span><br><span class="line">    <span class="comment"># 使用不同的lam测试次数</span></span><br><span class="line">    numTestPts = <span class="number">30</span></span><br><span class="line">    wMat = zeros((numTestPts,shape(xMat)[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># 循环，带入不同的lam计算回归系数，为了更方便观察，lam以指数级变化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTestPts):</span><br><span class="line">        ws = ridgeRegres(xMat,yMat,exp(i-<span class="number">10</span>))</span><br><span class="line">        wMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> wMat</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/05/15/AdaBoost%E5%85%83%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/15/AdaBoost%E5%85%83%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">AdaBoost元算法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2017-05-15 15:30:39 / 修改时间：17:16:54" itemprop="dateCreated datePublished" datetime="2017-05-15T15:30:39+08:00">2017-05-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.2k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  在机器学习中，当我们处理分类判定问题中，如果我们使用一种算法对目标样本做出判断后虽然分类结果是算法根据给定的所有数据样本集做出的最优结果，但是往往我们会对结果感觉持一定的怀疑态度，即这个分类确定就是该目标样本的最终结果？</p>
<p>  带着上面的问题我们就要想怎么才能得出一个更令我们信服的结果是接下来要探讨的问题。</p>
<h2 id="元算法"><a href="#元算法" class="headerlink" title="元算法"></a>元算法</h2><hr>
<p>  我们将各种算法的结果进行分组和整合，然后从中科学的选取一个结果作为我们的最终分类判定结果，这就是<code>元算法</code>。</p>
<p>  在元算法的实现中，我们可以使用不同的算法分类器组合起来得出组合结果，这种原算法的组合方式多种多样，可以是一种算法在不同参数调优下的实现，也可以是不同算法的组合实现，还可以是数据集不同部分分配给不同分类器的组合等等。</p>
<h2 id="bagging：随机抽样数据的分类器构建"><a href="#bagging：随机抽样数据的分类器构建" class="headerlink" title="bagging：随机抽样数据的分类器构建"></a>bagging：随机抽样数据的分类器构建</h2><hr>
<p>  <code>自举汇聚法</code>也叫做<code>bagging</code>方法，实现原理为从原始数据集中有放回的抽取S次后得到S个新的数据集，并且新数据集的大小和原始数据集的大小相等，因为是有放回随机抽取的缘故，所以在新数据集中可以有重复的样本和在原始数据集中存在的一些样本在新数据集中找不到。</p>
<p>  当我们获取到了S个样本集后，分别将一个机器学习算法作用域每个新的样本集进行类别判定，在分类结果中选取一个出现次数做多的类别作为最后的分类结果，该方法相当于由一个分类器投票结果。</p>
<h2 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h2><hr>
<p>  boosting算法和bagging算法类似，不同的是boosting算法引入了一个分类器权重的概念，即在所有的算法分类器都会有一个结果权重，每个权重的定义都是在上次迭代中根据分类器分类错误的样本数据进行重新的分类器训练。boosting分类的结果是基于所有分类器的加权求和结果，每个权重代表的是其对应分类器在上一轮迭代中的成功率。</p>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><hr>
<p>  <code>AdaBoost</code>算法是由多个弱分类器构建一个强分类器。</p>
<p>  AdaBoost算法的实现原理是给所有训练样本数据赋予一个初始权重值，这些权重值构成一个向量D，并且使所有权重值相加结果为1，然后在训练数据上训练出一个弱分类器并计算该分类器的错误率<br>$$<br>\epsilon &#x3D; \frac {未正确分类的样本数}{所有样本数目}<br>$$<br>使用该错误率计算该分类器的对决策结果的权重<br>$$<br>\alpha &#x3D; \frac 12ln\left(\frac{1-\epsilon}{\epsilon}\right)<br>$$<br>在下一次进行分类器训练前，对向量D更新，即更新每个数据样本的权重值，更新策略为如果这次该样本通过训练的分类器获得正确的分类，那么我们减小该样本的权重<br>$$<br>D_i^{(t+1)} &#x3D; \frac{D_i^te^{-\alpha}}{Sum(D)}<br>$$<br>而如果该样本被分类器错分，那么我们增加该样本的权重<br>$$<br>D_i^{(t+1)} &#x3D; \frac{D_i^te^{\alpha}}{Sum(D)}<br>$$<br>这样一直迭代获取新的分类器并且保存每次迭代后获取的分类器和分类器信息直到算法训练的分类器错误率为0，这样我们就获得了一个分类器组。</p>
<p>  做目标样本分类判定的时候，由于我们已经有一个分类器组，所以分别将待求结果样本输入到所有分类器中，并通过该分类器的权重计算出最后的样本分类。</p>
<h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># 构造一个简单的数据集做算法实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadSimpData</span>():</span><br><span class="line">    datMat = matrix([[ <span class="number">1.</span> ,  <span class="number">2.1</span>],</span><br><span class="line">        [ <span class="number">2.</span> ,  <span class="number">1.1</span>],</span><br><span class="line">        [ <span class="number">1.3</span>,  <span class="number">1.</span> ],</span><br><span class="line">        [ <span class="number">1.</span> ,  <span class="number">1.</span> ],</span><br><span class="line">        [ <span class="number">2.</span> ,  <span class="number">1.</span> ]])</span><br><span class="line">    classLabels = [<span class="number">1.0</span>, <span class="number">1.0</span>, -<span class="number">1.0</span>, -<span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">    <span class="keyword">return</span> datMat,classLabels</span><br><span class="line"><span class="comment"># 使用单层决策数来构造弱分类器</span></span><br><span class="line"><span class="comment"># 单层决策树是只使用样本数据的一个特征做决策树构建</span></span><br><span class="line"><span class="comment"># 因为是弱分类器，所以通过简单的特征值比大小方式确定样本分类</span></span><br><span class="line"><span class="comment"># 如果特征值符合给定的阈值和条件，那么确定该样本分类</span></span><br><span class="line"><span class="comment"># 分类为二分类问题，由1和-1表示</span></span><br><span class="line"><span class="comment"># 全面性考虑，所以条件判断分别判定大于和小于</span></span><br><span class="line"><span class="comment"># dataMatrix：输入样本数据集</span></span><br><span class="line"><span class="comment"># dimen：选取的特征</span></span><br><span class="line"><span class="comment"># threshVal：选定的阈值，集样本特征值和该值做比较</span></span><br><span class="line"><span class="comment"># threshIneq：比较条件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stumpClassify</span>(<span class="params">dataMatrix,dimen,threshVal,threshIneq</span>):</span><br><span class="line">    <span class="comment"># 样本数据集的预测类别</span></span><br><span class="line">    retArray = ones((shape(dataMatrix)[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 根据条件做预测判定</span></span><br><span class="line">    <span class="keyword">if</span> threshIneq == <span class="string">&#x27;lt&#x27;</span>:</span><br><span class="line">        retArray[dataMatrix[:,dimen] &lt;= threshVal] = -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        retArray[dataMatrix[:,dimen] &gt; threshVal] = -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> retArray</span><br><span class="line"><span class="comment"># 构建单层决策树</span></span><br><span class="line"><span class="comment"># 根据递归方式构建分类器，并用梯度的方式确定所有可能的阈值</span></span><br><span class="line"><span class="comment"># dataArr：样本数据集</span></span><br><span class="line"><span class="comment"># classLabels：样本真实类别</span></span><br><span class="line"><span class="comment"># D：样本的权重向量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">buildStump</span>(<span class="params">dataArr,classLabels,D</span>):</span><br><span class="line">    dataMatrix = mat(dataArr)</span><br><span class="line">    labelMat = mat(classLabels).T</span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    <span class="comment"># 定义阈值范围</span></span><br><span class="line">    numSteps = <span class="number">10.0</span></span><br><span class="line">    <span class="comment"># 存储分类器信息的变量</span></span><br><span class="line">    bestStump = &#123;&#125;</span><br><span class="line">    <span class="comment"># 最佳分类器分类结果</span></span><br><span class="line">    bestClasEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 最小的错误率，初始化为无限大的错误率</span></span><br><span class="line">    minError = inf</span><br><span class="line">    <span class="comment"># 遍历所有样本特征，即选择没一个特征训练决策树，然后比较错误率</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="comment"># 获取样本特征值的最小值</span></span><br><span class="line">        rangeMin = dataMatrix[:,i].<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 获取样本特征值的最大值</span></span><br><span class="line">        rangeMax = dataMatrix[:,i].<span class="built_in">max</span>()</span><br><span class="line">        <span class="comment"># 设置阈值选择的步长，即计算一个阈值做决策树</span></span><br><span class="line">        stepSize = (rangeMax-rangeMin)/numSteps</span><br><span class="line">        <span class="comment"># 循环确定阈值的值</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(-<span class="number">1</span>,<span class="built_in">int</span>(numSteps)+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 确定所有的预测比较条件</span></span><br><span class="line">            <span class="keyword">for</span> inequal <span class="keyword">in</span> [<span class="string">&#x27;lt&#x27;</span>, <span class="string">&#x27;gt&#x27;</span>]: </span><br><span class="line">                <span class="comment"># 确定阈值，使用步长计算</span></span><br><span class="line">                threshVal = (rangeMin + <span class="built_in">float</span>(j) * stepSize)</span><br><span class="line">                <span class="comment"># 使用预测函数预测所有样本的预测结果</span></span><br><span class="line">                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)</span><br><span class="line">                <span class="comment"># 使用分类器预测结果正确与否的矩阵列表，默认都正确表示为1</span></span><br><span class="line">                errArr = mat(ones((m,<span class="number">1</span>)))</span><br><span class="line">                <span class="comment"># 将所有预测结果和真实分类不同的修改为错误分类标记0</span></span><br><span class="line">                errArr[predictedVals == labelMat] = <span class="number">0</span></span><br><span class="line">                <span class="comment"># 使用矩阵计算，并通过样本权重计算分类器错误率</span></span><br><span class="line">                weightedError = D.T*errArr</span><br><span class="line">                <span class="comment"># 选择一个错误率最小的分类器即最佳弱分类器</span></span><br><span class="line">                <span class="keyword">if</span> weightedError &lt; minError:</span><br><span class="line">                    minError = weightedError</span><br><span class="line">                    bestClasEst = predictedVals.copy()</span><br><span class="line">                    <span class="comment"># 保存分类器信息</span></span><br><span class="line">                    bestStump[<span class="string">&#x27;dim&#x27;</span>] = i</span><br><span class="line">                    bestStump[<span class="string">&#x27;thresh&#x27;</span>] = threshVal</span><br><span class="line">                    bestStump[<span class="string">&#x27;ineq&#x27;</span>] = inequal</span><br><span class="line">    <span class="keyword">return</span> bestStump,minError,bestClasEst</span><br><span class="line"><span class="comment"># 实现AdaBoost算法</span></span><br><span class="line"><span class="comment"># dataArr：样本数据</span></span><br><span class="line"><span class="comment"># classLabels：样本集对应样本分类</span></span><br><span class="line"><span class="comment"># numIt：循环次数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adaBoostTrainDS</span>(<span class="params">dataArr,classLabels,numIt=<span class="number">40</span></span>):</span><br><span class="line">    <span class="comment"># 弱分类器组，此处为单层决策树组</span></span><br><span class="line">    weakClassArr = []</span><br><span class="line">    <span class="comment"># 样本集大小</span></span><br><span class="line">    m = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 默认的样本权重，所有权重和为1</span></span><br><span class="line">    D = mat(ones((m,<span class="number">1</span>))/m)</span><br><span class="line">    <span class="comment"># 类别估计值，即对样本集通过弱分类器的判定，最后的分类结果</span></span><br><span class="line">    aggClassEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numIt):</span><br><span class="line">        <span class="comment"># 获取一个弱分类器</span></span><br><span class="line">        <span class="comment"># bestStump：该分类器信息</span></span><br><span class="line">        <span class="comment"># error：错误率</span></span><br><span class="line">        <span class="comment"># classEst：分类预测结果</span></span><br><span class="line">        bestStump,error,classEst = buildStump(dataArr,classLabels,D)</span><br><span class="line">        <span class="comment"># 通过公式计算分类器权重alpha</span></span><br><span class="line">        alpha = <span class="built_in">float</span>(<span class="number">0.5</span>*log((<span class="number">1.0</span>-error)/<span class="built_in">max</span>(error,<span class="number">1e-16</span>)))</span><br><span class="line">        <span class="comment"># 将分类器权重添加到分类器信息对象</span></span><br><span class="line">        bestStump[<span class="string">&#x27;alpha&#x27;</span>] = alpha</span><br><span class="line">        <span class="comment"># 将该弱分类器添加到分类器组</span></span><br><span class="line">        weakClassArr.append(bestStump)</span><br><span class="line">        <span class="comment"># 通过公式计算新的样本权重</span></span><br><span class="line">        expon = multiply(-<span class="number">1</span>*alpha*mat(classLabels).T,classEst)</span><br><span class="line">        D = multiply(D,exp(expon)) </span><br><span class="line">        D = D/D.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="comment"># 通过循环获取的弱分类器计算样本集的分类</span></span><br><span class="line">        aggClassEst += alpha*classEst</span><br><span class="line">        <span class="comment"># 分类错误与正确的列表</span></span><br><span class="line">        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,<span class="number">1</span>)))</span><br><span class="line">        <span class="comment"># 计算错误率</span></span><br><span class="line">        errorRate = aggErrors.<span class="built_in">sum</span>()/m</span><br><span class="line">        <span class="comment"># 如果错误率为0.则推出循环</span></span><br><span class="line">        <span class="keyword">if</span> errorRate == <span class="number">0.0</span>: </span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> weakClassArr,aggClassEst</span><br><span class="line"><span class="comment"># 通过算法训练结果，进行分类预测</span></span><br><span class="line"><span class="comment"># datToClass：待预测的目标数据</span></span><br><span class="line"><span class="comment"># classifierArr：弱分类器组</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adaClassify</span>(<span class="params">datToClass,classifierArr</span>):</span><br><span class="line">    dataMatrix = mat(datToClass)</span><br><span class="line">    m = shape(dataMatrix)[<span class="number">0</span>]</span><br><span class="line">    aggClassEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 循环所有弱分类器，并使用分类器预测函数进行分类预测</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classifierArr)):</span><br><span class="line">        <span class="comment"># 将分类器信息带入预测函数做分类预测</span></span><br><span class="line">        classEst = stumpClassify(dataMatrix,classifierArr[i][<span class="string">&#x27;dim&#x27;</span>], classifierArr[i][<span class="string">&#x27;thresh&#x27;</span>], classifierArr[i][<span class="string">&#x27;ineq&#x27;</span>])</span><br><span class="line">        <span class="comment"># 对分类结果和该分类器的权重值进行最终分类迭代判断</span></span><br><span class="line">        aggClassEst += classifierArr[i][<span class="string">&#x27;alpha&#x27;</span>]*classEst</span><br><span class="line">    <span class="comment"># 实现最终分类判定，如果大于0返回则分类为+1，否则为-1</span></span><br><span class="line">    <span class="keyword">return</span> sign(aggClassEst)</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/05/09/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/09/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="post-title-link" itemprop="url">支持向量机 (SVM)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-09 11:15:12" itemprop="dateCreated datePublished" datetime="2017-05-09T11:15:12+08:00">2017-05-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2017-05-12 22:03:40" itemprop="dateModified" datetime="2017-05-12T22:03:40+08:00">2017-05-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  支持向量机（SVM）号称最优秀的分类算法之一。SVM分类器可以得到低错误率的结果并能够对训练集之外的数据点作出很好的分类决策。</p>
<p>  如果了解过Logistic回归算法，那么对SVM算法的原理会很容易了解，其实他们是一个问题，都是去找一个决策边界函数将数据集中的所有样本分隔为两类（即二分类问题），然后计算该决策边界函数中参数的最优解。</p>
<center><img data-src="/2017/05/09/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm.png" class="" title="二分类问题分隔"></center>

<p>  上面的示意图很好的展示了我们关注的二分类问题，图中所有的实心点和空心点共同构成了我们的数据集或称样本集，实心的点代表二分类中的一个分类，而空心点则代表了另一个分类，在Logistic回归中，两个分类用1和0的值来表示，那么上图中实心的点我们可以看作是属于1分类的点，空心的点则属于0分类的点。并且我们发现，我们可以找到一条直线将所有的实心点和空心点完全分隔到该条直线的上下两端，这样的数据集我们叫做<code>线性可分</code>数据。</p>
<p>  通过观察我们知道，有直线可以将上面属于两个分类的数据点完全区分开来，并且这样的直线还不止一条，可以看出直线$H_2$和$H_3$都可以对数据集做切分，并且这样的直线还可以找到很多条。但是这么多可以做分隔的直线中，哪条才是最好的数据集分隔直线？这就是SVM算法中我们需要考虑的问题。</p>
<p>  上图中直线$H_2$只是刚刚可以将数据集切分，但是如果离$H_2$最近的点往右移动一点距离$H_2$就不满足数据切分要求，但是直线$H_3$可以最大限度的将数据集切分为上下两个类别，我们暂且把$H_3$作为最好的数据集分隔线，该条线我们称作为<code>分隔超平面</code>，当前我们考虑的只是数据点在二维平面上，如果给定的数据点集是三维的，那么用来分隔数据的就是一个平面，如果更多维的时候呢？用来分隔数据集的分隔对象我们叫做<code>超平面</code>，也就是在分类问题中的分类决策边界。</p>
<p>  如果将$H_3$作为最好的超平面，和$H_2$比较可以看出，选取最好超平面的唯一要求就是离超平面最近的数据点到超平面的距离尽可能远，这里我们称这些离超平面最近的点为<code>支持向量</code>，点到超平面的距离为<code>间隔</code>，这样可以在我们训练数据有限的情况下使得分类器尽可能的健壮和有一定的容错能力。</p>
<p>  到这里，我们知道了SVM算法的目标，求给定数据集的最佳分隔线，且数据集离分割线最近的点到分隔线的间隔最大。</p>
<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><hr>
<p>  由于二维数据集是比较基础的数据样本集，并且也比较容易理解，所以我们由二维数据集的二分类判定来研究和阐述算法，最终的结果也同样适用于多维数据集。</p>
<p>  我们了解到SVM算法的算法目标就是找到一条将数据集分隔的最佳分隔超平面，分隔超平面的函数可以写成<br>$$<br>w^Tx+b &#x3D; 0<br>$$<br>这里的函数和Logistic回归中带入到Sigmoid函数的变量一致，即<br>$$<br>0&#x3D;w_0+w_1x_1+w_2x_2 + \cdots + w_nx_n<br>$$<br>在SVM的分隔超平面中，将$w_0$看作常量$b$，所以分隔超平面函数中的系数$w$和常量$b$就是构成最佳分割超平面函数的参数，也是SVM算法中需要训练得出的解。</p>
<p>  我们现在知道了分隔超平面函数，那么找到距离分隔超平面距离最近的点A，根据最优化理论，使得点A到分隔超平面间隔最大，即该点到分隔面的法线或垂线的长度，表示为<br>$$<br>d&#x3D;\frac{|w^TA+b|}{\mid\mid w\mid\mid}<br>$$<br>同样的，因为我们考虑的是二分类问题，所以在分隔超平面的一边找到点A，那么在超平面的另一边也存在点B，而且要分别使点A和点B距离分隔超平面的距离最大则该分隔超平面在点A和点B的中线位置，我们设点A距离分隔超平面的距离为1，则点B距离分隔超平面的距离也为1，将距离分隔超平面的距离为1的所有点用线连接起来会得到两条线，相当于将分隔超平面上下移动了1个单位距离，用函数表示这两条线为<br>$$<br>w^TA_i+b&#x3D;1\<br>w^TB_i+b&#x3D;-1<br>$$</p>
<center><img data-src="/2017/05/09/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm1.png" class="" title="二分类问题分隔"></center>

<p>通过观察上图我们可以很容易理解上面我们关于最大距离的描述。并且我们发现，在SVM算法中表示两个分类的值和Logistic回归中不同，在Logistic回归中，使用1和0来表示两个不同的分类，而在SVM算法中，我们改用1和-1来表示，这样做的好处是可以更直观的描述距离问题和方便计算，这样数据集中的所有点都会满足一个条件我们叫约束条件<br>$$<br>label (w^Tx+b) \ge 1<br>$$<br>$label$表示为数据点的标签1或-1。现在回来重新看看距离计算公式<br>$$<br>d&#x3D;\frac{|w^TA+b|}{\mid\mid w\mid\mid} &#x3D; \frac{1}{\mid\mid w\mid\mid}<br>$$<br>因为$w$是一个向量，$||w||$表示为一个范数，即$w$向量的各个元素的平方和的开平方<br>$$<br>d&#x3D;\frac{|w^TA+b|}{\mid\mid w\mid\mid} &#x3D; \frac{1}{\sqrt {w^Tw}}<br>$$<br>要求点到分隔超平面的距离$d$最大，就是求$w^Tw$的值最小，这样SVM算法的优化问题由<br>$$<br>arg\quad max_{w,b}\left\lbrace min_n(label(w^Tx+b))\frac1{\mid\mid w\mid\mid}\right \rbrace<br>$$<br>转变为<br>$$<br>min(\frac 12w^Tw) \<br>s.t. \quad y(w^Tx+b) \ge 1<br>$$<br>一个带约束条件的优化问题，这里我们乘以一个0.5的系数是为了方便后面的运算，当然前提并不会对算法结果有影响。求解上述问题，我们可以使用<code>拉格朗日乘子法</code>。</p>
<h2 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h2><hr>
<p>  在数学中的最优化问题中，<code>拉格朗日乘数法</code>（以数学家约瑟夫·拉格朗日命名）是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法。这种方法可以将一个有<em>n</em>个变量与<em>k</em>个约束条件的最优化问题转换为一个解有<em>n</em> + <em>k</em>个变量的方程组的解的问题。这种方法中引入了一个或一组新的未知数，即<code>拉格朗日乘数</code>，又称<code>拉格朗日乘子</code>，或<code>拉氏乘子</code>，它们是在转换后的方程，即约束方程中作为梯度（gradient）的线性组合中各个向量的系数。</p>
<p>  比如我们要求<br>$$<br>max(f(x,y)) \<br>s.t. \quad g(x,y) &#x3D; c<br>$$<br>对不同$d_n$的值，不难想像出<br>$$<br>f \left( x, y \right)&#x3D;d_n<br>$$<br>的等高线。而方程$g$的可行集所构成的线正好是<br>$$<br>g ( x, y ) &#x3D; c<br>$$</p>
<center><img data-src="/2017/05/09/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/Lagrange_multiplier.png" class="" title="拉格朗日示意"></center>

<p>  想像我们沿着$g &#x3D; c$的可行集走；因为大部分情况下$f$的等高线和$g$的可行集线不会重合，但在有解的情况下，这两条线会相交。想像此时我们移动$g &#x3D; c$上的点，因为$f$是连续的方程，我们因此能走到$f \left( x, y \right)&#x3D;d_n$更高或更低的等高线上，也就是说$d_n$可以变大或变小。只有当$g &#x3D; c$和$f \left( x, y \right)&#x3D;d_n$相切，也就是说，此时，我们正同时沿着$g &#x3D; c$和$f \left( x, y \right)&#x3D;d_n$走，那么这个时候就会出现极值。</p>
<p>  对该优化问题求解我们引入新变量拉格朗日乘数$\alpha$，这是我们只需要下列拉格朗日函数的极值：<br>$$<br>L(x,y,\alpha) &#x3D; f(x,y) + \alpha (g(x,y)-c)<br>$$<br>根据上述推断，当出现极值时$f$和$g$的切线在某点上平行，同时也意味着两者的梯度平行，那么有<br>$$<br>\nabla \left[f(x,y) + \alpha(g(x,y)-c)\right] &#x3D; 0 \<br>s.t.\quad \alpha \neq 0<br>$$<br>求出$\alpha$的值后带入$g$和$f$中，转变为在无约束条件下的极值和对应极值点的求解，并且$L(x,y,\alpha)$在达到极值时与$f(x,y)$相等，因为$L(x,y,\alpha)$在达到极值时$g(x,y)-c$总等于0。</p>
<p>  具体例子中应用格朗拉日乘子法，求<br>$$<br>f(x,y) &#x3D; x^2y \<br>s.t. \quad x^2+y^2&#x3D;1<br>$$<br>格朗拉日乘子法极值函数为<br>$$<br>L(x,y,\alpha)&#x3D;x^2y+\alpha(x^2+y^2-1)<br>$$<br>分别对函数中变量求导<br>$$<br>\frac {\partial L(x,y,\alpha)}{\partial x} \Rightarrow 2xy +2\alpha x&#x3D;0 \<br>\frac {\partial L(x,y,\alpha)}{\partial y} \Rightarrow x^2 +2\alpha y&#x3D;0 \<br>x^2+y^2-1&#x3D;0<br>$$<br>最小值就是上面方程组的解中的一个。</p>
<p>  解释了半天格朗拉日乘子法，那用格朗拉日乘子法怎么求解我们SVM算法中的优化问题<br>$$<br>\frac 12min(w^Tw) \<br>s.t. \quad y(w^Tx+b) \ge 1<br>$$<br>然而没有我们想的那么顺利，格朗拉日乘子法并不适用我们的问题，因为格朗拉日乘子法中的约束条件时等式约束，我们的优化问题中约束条件时大于等于约束，没关系，我们使用另一个方法来求解我们的SVM算法中的优化问题，该方法是<code>KKT条件</code>，是一个广义化拉格朗日乘数，你可以理解成在拉格朗日乘数上拓展来的一个方法。</p>
<h2 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h2><hr>
<p>  KKT条件全名<code>卡罗需-库恩-塔克条件</code>（Karush-Kuhn-Tucker Conditions），是在满足一些有规则条件下，一个非线性规划问题能有最优解解法的一个必要和充分条件。这是一个广义化拉格朗日乘数的成果。</p>
<p>  KKT条件和拉格朗日乘数的概念相似，所以我们直接讲。求<br>$$<br>min\quad f(x)\<br>s.t.\quad h(x) &#x3D; 0\<br>g(x) \le  0<br>$$<br>函数$h$为等式约束条件，函数$g$为不等式约束。将优化问题转为KKT条件极值函数<br>$$<br>L(x,\alpha,\beta) &#x3D; f(x) + \sum_{i&#x3D;1}^n\alpha_ig_i(x) + \sum_{i&#x3D;1}^n\beta_ih_i(x)<br>$$<br>该KKT条件极值函数必须满足条件</p>
<ol>
<li>L对各个x求导的结果为0</li>
<li>$\beta_i \neq 0$ </li>
<li>$\alpha_i \ge 0$</li>
<li>$\alpha_ig_i(x) &#x3D; 0$</li>
<li>$g_i(x) \le 0$</li>
<li>$h_i(x) &#x3D; 0$</li>
</ol>
<p>条件1和2在拉格朗日乘数中有解释，条件5和6为为优化问题的约束条件。满足条件3是因为我们要求$f(x)$的最小值，那么KKT条件极值函数中的三项都越小越好，因为等式约束已经最小为0，不等式约束条件$g_i(x)\le0$，那么根据最小要求只有$\alpha_i \ge0$时$\alpha_ig_i(x)$才会最小。满足条件4是因为不等式条件约束在坐标系中的图形为一个扇面，当我们取值刚好在扇面的起点部分时$g_i(x)&#x3D;0$，该不等式参与约束，那么该不等式约束条件的系数$\alpha_i\ge0$，否则在其他点要么就是不满足不等式约束条件，要么就是不等式约束条件不参与约束，当不等式约束条件不参与约束时我们认为$\alpha_i&#x3D;0$，即任何时候$\alpha_ig_i(x) &#x3D; 0$。现在通过KKT条件理论，我们的优化目标成为<br>$$<br>L(x,\alpha,\beta) &#x3D; f(x) + \sum_{i&#x3D;1}^n\alpha_ig_i(x) + \sum_{i&#x3D;1}^n\beta_ih_i(x)\<br>s.t. \quad \sum_{i&#x3D;1}^n\alpha_i g_i(x)&#x3D;0\<br>\alpha_i \ge0<br>$$<br>现在看起来应该和我们SVM算法中的优化问题<br>$$<br>\frac 12 min(w^Tw) \<br>s.t. \quad y(w^Tx+b) \ge 1<br>$$<br>类似了，可以直接引入KKT条件，SVM算法中的优化问题成为<br>$$<br>\begin{align}<br>L(w,b,\alpha) &amp;&#x3D; \frac 12 w^Tw+\alpha_1g_1(x)+\cdots+\alpha_ng_n(x)\<br>&amp;&#x3D;\frac 12w^Tw+\alpha_1(1-y_i(w^Tx_i+b))+\cdots+\alpha_n(1-y_n(w^Tx_n+b))\<br>&amp;&#x3D;\frac 12w^Tw-\alpha_1(y_i(w^Tx_i+b)-1)-\cdots-\alpha_n(y_n(w^Tx_n+b)-1)\<br>&amp;&#x3D;\frac 12w^Tw-\sum_{i&#x3D;1}^n\alpha_iy_i(w^Tx_i+b)+\sum_{i&#x3D;1}^n\alpha_i\<br>\end{align}<br>$$<br>然后对目标函数$L(w,b,\alpha)$求导<br>$$<br>\frac {\partial L}{\partial w}&#x3D;w-\sum_{i&#x3D;1}^n\alpha_iy_ix_i&#x3D;0\Rightarrow w&#x3D;\sum_{i&#x3D;i}^n\alpha_iy_ix_i\<br>\frac{\partial L}{\partial b}&#x3D;-\sum_{i&#x3D;1}^n\alpha_iy_i&#x3D;0\Rightarrow \sum_{i&#x3D;1}^n\alpha_iy_i&#x3D;0<br>$$<br>然后带回函数$L(w,b,\alpha)$中<br>$$<br>\begin{align}<br>w(\alpha) &amp;&#x3D; L(w,b,\alpha)\<br>&amp;&#x3D;\frac 12 (\sum_{i&#x3D;i}^n\alpha_iy_ix_i)^T(\sum_{j&#x3D;1}^n\alpha_jy_jx_j)-\sum_{i&#x3D;1}^n\alpha_iy_i((\sum_{i&#x3D;i}^n\alpha_iy_ix_i)^Tx_i+b)+\sum_{i&#x3D;1}^n\alpha_i\<br>&amp;&#x3D;\frac 12 \sum_{i,j&#x3D;1}^n\alpha_iy_i\alpha_jy_jx_i^Tx_j-(\sum_{i,j&#x3D;1}^n\alpha_iy_i\alpha_jy_jx_i^Tx_j)+b\sum_{i&#x3D;1}^n\alpha_iy_i+\sum_{i&#x3D;1}^n\alpha_i\<br>&amp;&#x3D;-\frac12(\sum_{i,j&#x3D;1}^n\alpha_iy_i\alpha_jy_jx_i^Tx_j)+\sum_{i&#x3D;1}^n\alpha_i\<br>\end{align}<br>$$<br>最后我们SVM算法的优化目标函数被推导为<br>$$<br>max_\alpha\left[\sum_{i&#x3D;1}^m\alpha-\frac12\sum_{i,j&#x3D;1}^ny_iy_j\alpha_i\alpha_jx_i^Tx_j\right]\<br>s.t.\quad \alpha\ge0\<br>\sum_{i&#x3D;1}^n\alpha_iy_i&#x3D;0<br>$$<br>  得到了最终的优化目标函数，我们就可以开始算法实现了？理想很丰满，现实很骨感啊，事实上在平时我们接触到的数据集中很难可以找到完全线性可分的数据集，甚至有些点会越过分隔超平面错误的分类到错误的类别，这些不常规的点我们叫做离群点或噪声点，所以我们上面所做的一切工作会因为这些噪声点而失效。</p>
<p>  为了在算法中避免一些噪声点的干扰导致算法结果的出错，所以我们引入一个<code>松弛变量</code>的概念，每个数据点的松弛变量被解释为允许该数据点在一定距离上的移动，就是给了所有数据点一定的容错机会，并且这个小的移动距离不会影响我们的类别判定，被认为是可接受范围。加入松弛变量后，意味着我们放弃了对这些点的精确分类，但是这种分类损失可以使超平面不必向这些点的方向移动而得到更大的几何间隔。SVM算法的约束条件就会更新，那么新的算法优化问题为<br>$$<br>min \quad \frac 12w^Tw+C\sum_{i&#x3D;1}^n\epsilon <em>i\<br>s.t. \quad y_i(w^Tx_i+b) \ge 1-\epsilon_i\<br>\epsilon \ge 0<br>$$<br>函数中的$C$被称作<code>惩罚因子</code>，惩罚因子决定了我们对噪声点带来的损失有多重视，如果惩罚因子的值越大，噪声点对算法结果的影响越严重，因为这样表明我们不愿意放弃这些离群点。所以惩罚因子的出现只是让不可分的数据集变成可分的数据集而已。那么我们重新根据KKT条件推断优化目标函数<br>$$<br>\begin{align}<br>L(w,b,\alpha,\beta,\epsilon)<br>&amp;&#x3D;\frac 12w^Tw+C\sum</em>{i&#x3D;1}^n\epsilon <em>i+\alpha_1g^1_1(x)+\cdots+\alpha_ng^1_n(x)+\beta_1g^2_1(x)+\cdots+\beta_ng^2_n(x)\<br>&amp;&#x3D;\frac 12w^Tw+C\sum</em>{i&#x3D;1}^n\epsilon <em>i+\sum</em>{i&#x3D;1}^n\alpha_ig_i^1(x)+\sum_{i&#x3D;1}^n\beta_ig_i^2(x)\<br>&amp;&#x3D;\frac 12w^Tw+C\sum_{i&#x3D;1}^n\epsilon <em>i+\sum</em>{i&#x3D;1}^n\alpha_i(1-\epsilon_i-y_i(w^Tx_i+b))+\sum_{i&#x3D;1}^n\beta_i(-\epsilon_i)\<br>&amp;&#x3D;\frac 12w^Tw+C\sum_{i&#x3D;1}^n\epsilon <em>i-\sum</em>{i&#x3D;1}^n\alpha_i(\epsilon_i+y_i(w^Tx_i+b)-1)-\sum_{i&#x3D;1}^n\beta_i\epsilon_i\<br>&amp;&#x3D;\frac 12w^Tw+C\sum_{i&#x3D;1}^n\epsilon_i-\sum_{i&#x3D;1}^n\alpha_iy_i(w^Tx_i+b)+\sum_{i&#x3D;1}^n\alpha_i-\sum_{i&#x3D;1}^n\beta_i\epsilon_i\<br>\end{align}<br>$$<br>然后对$w,b,\epsilon$分别求导<br>$$<br>\frac {\partial L}{\partial w}&#x3D;2w-\sum_{i&#x3D;1}^n\alpha_iy_ix_i&#x3D;0\Rightarrow w&#x3D;\sum_{i&#x3D;i}^n\alpha_iy_ix_i\<br>\frac{\partial L}{\partial b}&#x3D;-\sum_{i&#x3D;1}^n\alpha_iy_i&#x3D;0\Rightarrow \sum_{i&#x3D;1}^n\alpha_iy_i&#x3D;0\<br>\frac{\partial L}{\partial \epsilon}&#x3D;0\Rightarrow C-\alpha_i-\beta_i&#x3D;0<br>$$<br>通过上面的求导结果我可以观察到惩罚因子$C$的条件<br>$$<br>0\le \alpha_i \le C<br>$$<br>将上面求导的结果带入目标函数中<br>$$<br>\begin{align}<br>w(\alpha)<br>&amp;&#x3D;L(w,b,\alpha,\beta,\epsilon)\<br>&amp;&#x3D;\frac 12w^Tw-\sum_{i&#x3D;1}^n\alpha_iy_i(w^Tx_i+b)+C\sum_{i&#x3D;1}^n\epsilon_i+\sum_{i&#x3D;1}^n\alpha_i-\sum_{i&#x3D;1}^n\beta_i\epsilon_i\<br>&amp;&#x3D;(\sum_{i&#x3D;i}^n\alpha_iy_ix_i)^T(\sum_{j&#x3D;1}^n\alpha_jy_jx_j)-\sum_{i&#x3D;1}^n\alpha_iy_i((\sum_{i&#x3D;i}^n\alpha_iy_ix_i)^Tx_i+b)+C\sum_{i&#x3D;1}^n\epsilon_i+\sum_{i&#x3D;1}^n\alpha_i-\sum_{i&#x3D;1}^n\beta_i\epsilon_i\<br>&amp;&#x3D;\frac12(\sum_{i,j&#x3D;1}^n\alpha_iy_i\alpha_jy_jx_i^Tx_j)-\sum_{i,j&#x3D;1}^n\alpha_iy_i\alpha_jy_jx_i^Tx_j+C\sum_{i&#x3D;1}^n\epsilon_i+\sum_{i&#x3D;1}^n\alpha_i-\sum_{i&#x3D;1}^n\epsilon_i(C-\alpha_i)\<br>&amp;&#x3D;-\frac12(\sum_{i,j&#x3D;1}^n\alpha_iy_i\alpha_jy_jx_i^Tx_j)+\sum_{i&#x3D;1}^n\alpha_i\<br>\end{align}<br>$$<br>发现溜达了一圈回来SVM算法中优化目标函数和没加惩罚因子时一摸一样，只是条件更新了<br>$$<br>max_\alpha\left[\sum_{i&#x3D;1}^m\alpha-\frac12\sum_{i,j&#x3D;1}^ny_iy_j\alpha_i\alpha_jx_i^Tx_j\right]\<br>s.t.\quad C\ge\alpha\ge0\<br>\sum_{i&#x3D;1}^n\alpha_iy_i&#x3D;0<br>$$</p>
<h2 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h2><hr>
<p>  从上面对KKT条件的推导中，我们最后得出了最终的带约束条件的优化目标函数，现在的问题就剩下解这个目标函数得到一组$\alpha$的值使得目标函数的取值最大。SMO算法是很好的一个计算KKT条件目标函数的方法。</p>
<p>  SMO算法的实现原理是首先对所有的$\alpha$取默认值，然后迭代去更新这些$\alpha$，SMO算法是一个逐渐被优化和逐渐完善到最优解的算法过程。我们知道在SVM算法的最终目标是找到一个超平面从数据类别的维度来最好的去分隔我们提供的数据集，SMO算法在逐渐优化的过程在SVM算法表现为分隔超平面从最差一直优化到一个最佳的分隔超平面。我们知道数据集中的一个数据点会对应一个$\alpha$的值，在SMO算法优化的某一次迭代更新中，我们假设SVM算法找到了一个分隔超平面，当然不是最佳分隔超平面，那么使用该分隔超平面分隔数据会有一部分数据点是被分到了正确的分类下面，还有一部分点是被分在了错误的分类下面或者是不能很好的做出分类判定的数据点，那么这些该次迭代就是针对这些点做点对应$\alpha$值的更新，因为我们会有很多的$\alpha$值需要更新，而往往是每更新一个$\alpha$则分隔超平面都会发生变化，分隔超平面的变化会使其他所有的$\alpha$产生变化，那么多的点我们需要怎么更新呢？SMO算法采取的策略就是选择当前迭代中被当前生成的分隔超平面分类错误数据点中的一个$\alpha$，因为修改这个错误数据点的$\alpha$值会使得其他所有的$\alpha$变化且我们一次兼顾不了那么多的$\alpha$，所以SMO算法会在剩下的数据点中在选择一个数据点的$\alpha$进行同步更新，虽然对这两个点的更新不会是最好的更新或者说因为考虑其他$\alpha$的影响所以对这两个点的更新不是最后的优化更新，但是我们可以知道这是一个慢慢变好的过程，因为至少每次都会有一个被当前迭代中生成的分隔超平面分隔错误的点被更新到正确的点，当然如果我们选取的第二个数据点刚好也是分类错误的点，那么我们就一次纠正了两个错误的数据点。这样在迭代中每次选择一个被当前迭代生成的分隔超平面分类错误的点进行更新，迭代直到找不到被分类错误的点为止，既然我们找不到任何一个被分类错误的点并且所有的数据点都满足KKT条件的目标函数和所有约束条件，那么我们就找到了那个最佳的超平面。</p>
<p>  知道了SMO算法对实现KKT条件求解的策略，我们需要讨论的就是在SMO算法实现中每个小过程的解释和推导。首先我们回顾一下最终的SVM算法目标函数和约束条件<br>$$<br>max_\alpha\left[\sum_{i&#x3D;1}^m\alpha-\frac12\sum_{i,j&#x3D;1}^ny_iy_j\alpha_i\alpha_jx_i^Tx_j\right]\<br>s.t.\quad C\ge\alpha\ge0\<br>\sum_{i&#x3D;1}^n\alpha_iy_i&#x3D;0<br>$$<br>我们在所有的数据集中找到一个数据点，然后判定该数据点是否被正确的分类。我们知道数据点如果处在正确的分类位置，那么必须满足条件<br>$$<br>f(w,b)&#x3D;y_i(w^Tx_i+b) \ge1<br>$$<br>这也是SVM算法中对所有数据点的约束条件，在KKT条件中加入系数$\alpha$后<br>$$<br>a_i(1-y_i(w^Tx_i+b)) &#x3D; 0\<br>s.t.\quad \alpha_i \ge 0\<br>1-y_i(w^T+b) \le 0<br>$$<br>所以在两个约束条件$\alpha_i \ge 0$和$1-y_i(w^T+b) \le 0$中至少有一个为0，我们在KKT条件的阐述中表明当$\alpha_i$为0的时候，则表明该数据点是边界上的点或者是被正确分类的点。在坐标系中，分别在距离分隔超平面长度为1的位置两个分类的边界则数据集中所有的数据点必须满足下面的任意条件<br>$$<br>a_i&#x3D;0 \Rightarrow y_if(w,b) \ge1\quad(正确分类)\<br>0\le\alpha_i \le C \Rightarrow y_if(w,b) &#x3D; 1\quad(在边界上)\<br>a_i&#x3D;C \Rightarrow y_if(w,b) \le 1\quad(在边界之间)<br>$$<br>当一个数据点满足上面一个条件时表示该数据点不需要做$\alpha$调整，相反需要做调整的数据点需要满足下面任意条件<br>$$<br>y_if(w,b)\le1\quad但是\quad\alpha_i \lt C\<br>y_if(w,b)\ge1\quad但是\quad\alpha_i \gt 0\<br>y_if(w,b)\le1\quad但是\quad\alpha_i&#x3D;0 或 \alpha_i&#x3D;C<br>$$<br>找到需要调整的数据点之后，根据SMO算法的求解策略，我们还需要在剩下的所有数据点中找到一个数据点作为和需要调整的数据点的同步更新那个点。</p>
<p>  现在我们找到需要调整的数据点A和剩余那部分点中的数据点B，数据点A对应$\alpha_A$，数据点B对应$\alpha_B$。还记得SVM算法的最终目标函数的约束条件么<br>$$<br>\sum_{i&#x3D;1}^n\alpha_iy_i&#x3D;0<br>$$<br>在只修改$\alpha_A$和$\alpha_B$且其他$\alpha$不变的情况下有等式<br>$$<br>\alpha_A^{new}y_A+\alpha_B^{new}y_B&#x3D;\alpha_A^{old}y_A+\alpha_B^{old}y_B&#x3D;K<br>$$<br>其中$K$只是一个常数值，因为SVM算法的分类值为1和-1中取值，所以上面式子有<br>$$<br>y_A&#x3D;y_B\quad \alpha_A^{new}+\alpha_B^{new} &#x3D; \alpha_A^{old}+\alpha_B^{old}\<br>y_A\neq y_B\quad \alpha_A^{new}-\alpha_B^{new} &#x3D; \alpha_A^{old}-\alpha_B^{old}\<br>s.t.\quad 0\le\alpha\le C<br>$$<br>通过分析可以获取$\alpha_B^{new}$的区间为<br>$$<br>y_A&#x3D; y_B\quad L&#x3D;max(0,\alpha_B^{old}+\alpha_A^{old}-C)，H&#x3D;min(C,\alpha_2^{old}+\alpha_1^{old})\<br>y_A\neq y_B\quad L&#x3D;max(0,\alpha_B^{old}-\alpha_A^{old})，H&#x3D;min(C,C+\alpha_2^{old}-\alpha_1^{old})<br>$$<br>  现在我们知道了$\alpha_B^{new}$的区间，但是我们最终是要计算$\alpha_B^{new}$的值。还记得KKT条件的目标函数<br>$$<br>w(\alpha)&#x3D;\sum_{i&#x3D;1}^n\alpha_i-\frac12(\sum_{i,j&#x3D;1}^n\alpha_iy_i\alpha_jy_jx_i^Tx_j)<br>$$<br>因为我们这里只用到数据点A和数据点B对应的$\alpha$，所以将函数分解并带入上面$\alpha_A$和$\alpha_B$的关系求导可以得到$\alpha_B^{new}$<br>$$<br>\alpha_B^{new} &#x3D; \alpha_B^{old}-\frac{y_B(E_A-E_B)}{\eta}\<br>E_i&#x3D;f(w,b)-y_i\<br>\eta&#x3D;2x_A^Tx_B-x_A^Tx_A-x_B^Tx_B<br>$$<br>然后对求出来的$\alpha_B^{new}$值使用上面计算得到的范围确定具体的值，然后带入计算$\alpha_A^{new}$。</p>
<p>  SMO算法到这一步后，我们知道了新的$\alpha$，然后可以通过之前的推导出来的$w$计算公式<br>$$<br>w&#x3D;\sum_{i&#x3D;1}^n\alpha_iy_ix_i<br>$$</p>
<p>计算出相对应$w$的值。回顾分类预测函数<br>$$<br>f(w,b)&#x3D;w^Tx+b<br>$$<br>发现我们还需要一个$b$的值，在回顾一下距离公式<br>$$<br>y(w^Tx+b)\ge1<br>$$<br>并且当前更新后的$\alpha$的值对应的数据点因为分割平面的变化成为边界点，所以有<br>$$<br>y_A(w_{new}^Tx_A+b_A^{new})&#x3D;1\<br>y_B(w_{new}^Tx_B+b_B^{new})&#x3D;1<br>$$<br>对方程求解后有<br>$$<br>b_A^{new}&#x3D;b^{old}-E_A-y_A(\alpha_A^{new}-\alpha_A^{old})x_A^Tx_A-y_B(\alpha_B^{new}-\alpha_B^{old})x_A^Tx_B\<br>b_B^{new}&#x3D;b^{old}-E_B-y_A(\alpha_A^{new}-\alpha_A^{old})x_A^Tx_B-y_B(\alpha_B^{new}-\alpha_B^{old})x_B^Tx_B<br>$$<br>结果中两个$b$的选择依据是更新$\alpha$后的数据点哪个在边界上就是哪个$b$<br>$$<br>b&#x3D; \begin{cases}<br>b_A, &amp; {if \quad 0\le \alpha_A^{new}\le C} \<br>b_B, &amp; {if \quad 0\le \alpha_B^{new}\le C}\<br>(b_A+b_B)&#x2F;2 &amp;others<br>\end{cases}<br>$$<br>  到这里我们得到了$\alpha、b$的值，针对简单线性并且带有松弛条件的SMO算法就完成了。</p>
<h2 id="启发方法"><a href="#启发方法" class="headerlink" title="启发方法"></a>启发方法</h2><hr>
<p>  这里引入启发式思想去讨论算法实现中的一些细节。</p>
<p>  为了使算法在选择$\alpha$的时候可以更快的选择出最合适的$\alpha$，所以我们在选择第一个$\alpha$时候用两种方式，一种就是在所有数据集上进行单遍扫描，另一种就是在非边界点对应的$\alpha$中实现单遍扫描，我们知道非边界的$\alpha$值代表那些不等于边界0或$C$的$\alpha$值，并且第二种方式我们需要跳过那些已知不会改变的$\alpha$的值。</p>
<p>  算法实现中，每次循环中使用哪种方式选择第一个$\alpha$的值去迭代更新取决于上一次循环的迭代中有没有$\alpha$的值被更新，如果上一次循环中没有$\alpha$的值被迭代更新，说明没有数据点处于边界中，但是并不表示我们找到的分隔超平面就是最佳分隔超平面，因为我们不知道上一次循环中取第一个$\alpha$的方式是哪种，如果是第一种，那么有可能当前的分隔超平面面就是最佳分隔超平面，但是如果是第二种则更不能确定该超平面就是我们要求的最佳超平面，所以本次循环还需要在数据集上进行单遍扫描去迭代更新$\alpha$的值；如果上一次循环中有$\alpha$的值被迭代更新，那么本次循环则优先调整上次迭代中处于数据集两个边界中的所有点对应的$\alpha$值，即$\alpha$在0和$C$区间内的数据点。这样我们就不知道什么时候我们选取到的分隔超平面就是最佳分隔超平面，所以我们会指定算法循环的次数，如果循环到一定次数就会停止算法训练，这样做的好处是如果算法在训练中不收敛或者有波动，可以避免算法一直不停的训练下去生成死循环问题。</p>
<p>  我们选取到第一个待更新的$\alpha$的值后，如果该点不满足KKT条件，那么我们选择第二个待更新的$\alpha$。第二个$\alpha$选取的方式也有两种方式，第一种方式就是在数据集中随机取一个$\alpha$，另一种方式就是通过计算所有$\alpha$对应数据点分类预测结果和真实结果误差，然后选去一个误差最大的，就是噪声最大的噪声点进行更新，即选择具有最大步长的第二个点，这样做的好处就是可以使算法更快的收敛。那么选取第二个待更新数据点的$\alpha$的值用哪种方式，这就取决于当前算法训练处于那个阶段，也就是说在当前迭代中有没有在数据边界内的噪声点，如果有那么就使用第二种方式，否则选择第一种方式。</p>
<h2 id="非线性分类应用：核函数"><a href="#非线性分类应用：核函数" class="headerlink" title="非线性分类应用：核函数"></a>非线性分类应用：核函数</h2><hr>
<p>  在SVM算法的实际应用中，我们很难找到一个可以完全线性可分的算法训练数据集，往往我们接触到的数据集都是非线性可分的，但是非线性可分的数据集也存在一个可以识别的分隔模式，所以我们要做的就是使用一种工具来捕获这种模式使得数据集支持SVM的算法。下面我们介绍叫做<code>核函数</code>的一种工具。</p>
<p>  核函数处理数据是对数据进行某种形式的转换来获取一些新的变量来表示数据，使数据更容易展示与之对应的类别，所以使用核函数来处理数据也是将数据从一个特征空间转换到另一个特征空间的的过程，经过转换后的的数据就可以在高维空间中解决线性问题，相当于我们在低维空间中解决非线性问题。</p>
<p>  在SVM算法中所有的运算都是两个向量相乘之后得到耽搁标量或数值的内积形式，在非线性数据集中，我们可以将内积德运算替换成核函数方式，这种方式被称作<code>核技巧</code>。</p>
<h2 id="径向基核函数"><a href="#径向基核函数" class="headerlink" title="径向基核函数"></a>径向基核函数</h2><hr>
<p>  径向基函数是SVM算法中常被用到的一个核函数。径向基函数采用向量作为自变量函数，并且能够基于向量距离运算输出一个标量。径向基函数的高斯版本的公式为<br>$$<br>k(x,y)&#x3D;exp\left(\frac{-\mid\mid x-y\mid\mid^2}{2\sigma^2}\right)<br>$$<br>其中$\sigma$是用户定义的用户确定到达率或者说函数值跌落到0的速度。</p>
<p>  径向基函数将数据从其特征空间映射到更高维的空间，并且我们不需要具体知道具体的维数，因为SVM算法的内积运算结果和核函数的结果都是一个表示距离的值。但是运用核函数的算法还是可控的，因为还有一个自定义的变量$\sigma$。在SVM算法实现中运用核函数的做法就是将算法中运用到内积运算的部分替换为核函数就完成了SVM算法训练数据集的转换。</p>
<h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><hr>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 核函数实现</span></span><br><span class="line"><span class="comment"># X：所有样本集数据</span></span><br><span class="line"><span class="comment"># A：将要被处理的一个样本数据</span></span><br><span class="line"><span class="comment"># kTup：径向基函数用到的参数，格式为（核函数类型，sigma）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernelTrans</span>(<span class="params">X, A, kTup</span>): </span><br><span class="line">    <span class="comment"># 样本集矩阵规格</span></span><br><span class="line">    m,n = shape(X)</span><br><span class="line">    <span class="comment"># 将样本数据处理后的初始化数据</span></span><br><span class="line">    K = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 根据核函数类型确定转换方式</span></span><br><span class="line">    <span class="keyword">if</span> kTup[<span class="number">0</span>]==<span class="string">&#x27;lin&#x27;</span>: </span><br><span class="line">        K = X * A.T</span><br><span class="line">    <span class="keyword">elif</span> kTup[<span class="number">0</span>]==<span class="string">&#x27;rbf&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            deltaRow = X[j,:] - A</span><br><span class="line">            K[j] = deltaRow*deltaRow.T</span><br><span class="line">        K = exp(K/(-<span class="number">1</span>*kTup[<span class="number">1</span>]**<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">raise</span> NameError(<span class="string">&#x27;Houston We Have a Problem -- That Kernel is not recognized&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> K</span><br><span class="line"><span class="comment"># 一个算法参数的集合结构，包括算法中用到的参数内容</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">optStruct</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,dataMatIn, classLabels, C, toler, kTup</span>):</span><br><span class="line">        <span class="comment"># 数据集矩阵</span></span><br><span class="line">        self.X = dataMatIn</span><br><span class="line">        <span class="comment"># 数据集中与数据对应的真实分类</span></span><br><span class="line">        self.labelMat = classLabels</span><br><span class="line">        <span class="comment"># 常数C，算法目标函数中约束条件中有出现</span></span><br><span class="line">        self.C = C</span><br><span class="line">        <span class="comment"># 理解为机器误差</span></span><br><span class="line">        self.tol = toler</span><br><span class="line">        <span class="comment"># 数据集的尺寸，即有多少样本在数据集中</span></span><br><span class="line">        self.m = shape(dataMatIn)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># alpha列表，smo算法最终输出的内容，一个样本对应一个alpha，初始值为0</span></span><br><span class="line">        self.alphas = mat(zeros((self.m,<span class="number">1</span>)))</span><br><span class="line">        <span class="comment"># 预测函数中出现的参数b</span></span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 一个缓存数据的属性</span></span><br><span class="line">        self.eCache = mat(zeros((self.m,<span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># 经过核函数转换过的样本集矩阵，将样本维度大小转为样本集尺寸大小相等</span></span><br><span class="line">        self.K = mat(zeros((self.m,self.m)))</span><br><span class="line">        <span class="comment"># 数据通过核函数处理填充上面的K矩阵</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.m):</span><br><span class="line">            <span class="comment"># 将数据调用核函数</span></span><br><span class="line">            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)</span><br><span class="line"><span class="comment"># 使用随机方法选择第二个alpha</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">selectJrand</span>(<span class="params">i,m</span>):</span><br><span class="line">    j=i</span><br><span class="line">    <span class="keyword">while</span> (j==i):</span><br><span class="line">        j = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>,m))</span><br><span class="line">    <span class="keyword">return</span> j</span><br><span class="line"><span class="comment"># 实现查找第二个alpha的值</span></span><br><span class="line"><span class="comment"># i：第一个alpha的矩阵索引</span></span><br><span class="line"><span class="comment"># oS：算法数据结构实体</span></span><br><span class="line"><span class="comment"># Ei：第一个alpha的预测误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">selectJ</span>(<span class="params">i, oS, Ei</span>):</span><br><span class="line">    <span class="comment"># 步长最大的alpha矩阵索引</span></span><br><span class="line">    maxK = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 临时误差差距</span></span><br><span class="line">    maxDeltaE = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 第二个alpha的预测误差</span></span><br><span class="line">    Ej = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 更新第一个alpha的误差值到缓存列表</span></span><br><span class="line">    oS.eCache[i] = [<span class="number">1</span>,Ei]</span><br><span class="line">    <span class="comment"># 获取缓存列表中所有不为0的误差值的索引</span></span><br><span class="line">    validEcacheList = nonzero(oS.eCache[:,<span class="number">0</span>].A)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 如果所有数据点中有跟实际值有误差的点，大于0的原因是排除第一个点的影响</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(validEcacheList)) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 循环有误差的所有点列表</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> validEcacheList:</span><br><span class="line">            <span class="comment"># 第二个点确定不能和第一个点是同一个点</span></span><br><span class="line">            <span class="keyword">if</span> k == i: </span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 计算当前循环到的点的新误差</span></span><br><span class="line">            Ek = calcEk(oS, k)</span><br><span class="line">            <span class="comment"># 计算步长</span></span><br><span class="line">            deltaE = <span class="built_in">abs</span>(Ei - Ek)</span><br><span class="line">            <span class="comment"># 如果当前点的步长大，则选定该点</span></span><br><span class="line">            <span class="keyword">if</span> (deltaE &gt; maxDeltaE):</span><br><span class="line">                maxK = k</span><br><span class="line">                maxDeltaE = deltaE</span><br><span class="line">                Ej = Ek</span><br><span class="line">        <span class="keyword">return</span> maxK, Ej</span><br><span class="line">    <span class="comment"># 如果不存在有误差的值则使用随机方法获取第二个alpha的值</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        j = selectJrand(i, oS.m)</span><br><span class="line">        Ej = calcEk(oS, j)</span><br><span class="line">    <span class="keyword">return</span> j, Ej</span><br><span class="line"><span class="comment"># 设置新alpha的值</span></span><br><span class="line"><span class="comment"># aj：新的值</span></span><br><span class="line"><span class="comment"># H：上限</span></span><br><span class="line"><span class="comment"># L：下限</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clipAlpha</span>(<span class="params">aj,H,L</span>):</span><br><span class="line">    <span class="keyword">if</span> aj &gt; H: </span><br><span class="line">        aj = H</span><br><span class="line">    <span class="keyword">if</span> L &gt; aj:</span><br><span class="line">        aj = L</span><br><span class="line">    <span class="keyword">return</span> aj</span><br><span class="line"><span class="comment"># 计算误差的实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcEk</span>(<span class="params">oS, k</span>):</span><br><span class="line">    <span class="comment"># 根据预测函数公式，计算处于索引k处的预测结果</span></span><br><span class="line">    <span class="comment"># 核函数推导：oS.K[:,k] = dataMatrix*dataMatrix[i,:].T)</span></span><br><span class="line">    fXk = <span class="built_in">float</span>(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)</span><br><span class="line">    <span class="comment"># 计算预测结果误差</span></span><br><span class="line">    Ek = fXk - <span class="built_in">float</span>(oS.labelMat[k])</span><br><span class="line">    <span class="keyword">return</span> Ek</span><br><span class="line"><span class="comment"># 更新缓存</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateEk</span>(<span class="params">oS, k</span>):</span><br><span class="line">    <span class="comment"># 使用最新的alpha值更新预测误差并更新缓存</span></span><br><span class="line">    Ek = calcEk(oS, k)</span><br><span class="line">    oS.eCache[k] = [<span class="number">1</span>,Ek]</span><br><span class="line"><span class="comment"># 进行选定alpha更新</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">innerL</span>(<span class="params">i, oS</span>):</span><br><span class="line">    <span class="comment"># 计算第一个alpha的误差</span></span><br><span class="line">    Ei = calcEk(oS, i)</span><br><span class="line">    <span class="comment"># 判断选取的alpha是否满足KKT条件</span></span><br><span class="line">    <span class="keyword">if</span> ((oS.labelMat[i]*Ei &lt; -oS.tol) <span class="keyword">and</span> (oS.alphas[i] &lt; oS.C)) <span class="keyword">or</span> ((oS.labelMat[i]*Ei &gt; oS.tol) <span class="keyword">and</span> (oS.alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line">        <span class="comment"># 选取第二个alpha的值</span></span><br><span class="line">        j,Ej = selectJ(i, oS, Ei)</span><br><span class="line">        <span class="comment"># 复制两个旧的alpha值，供后面计算</span></span><br><span class="line">        alphaIold = oS.alphas[i].copy()</span><br><span class="line">        alphaJold = oS.alphas[j].copy()</span><br><span class="line">        <span class="comment"># 根据alpha区间公式限定alpha的值</span></span><br><span class="line">        <span class="keyword">if</span> (oS.labelMat[i] != oS.labelMat[j]):</span><br><span class="line">            L = <span class="built_in">max</span>(<span class="number">0</span>, oS.alphas[j] - oS.alphas[i])</span><br><span class="line">            H = <span class="built_in">min</span>(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L = <span class="built_in">max</span>(<span class="number">0</span>, oS.alphas[j] + oS.alphas[i] - oS.C)</span><br><span class="line">            H = <span class="built_in">min</span>(oS.C, oS.alphas[j] + oS.alphas[i])</span><br><span class="line">        <span class="keyword">if</span> L==H: </span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 计算公式推导中的eta参数，用来计算新的alpha</span></span><br><span class="line">        <span class="comment"># 核函数推导：oS.K[i,j] = dataMatrix[i,:]*dataMatrix[j,:].T</span></span><br><span class="line">        <span class="comment"># 核函数推导：oS.K[i,i] = dataMatrix[i,:]*dataMatrix[i,:].T</span></span><br><span class="line">        <span class="comment"># 核函数推导：oS.K[j,j] = dataMatrix[j,:]*dataMatrix[j,:].T</span></span><br><span class="line">        eta = <span class="number">2.0</span> * oS.K[i,j] - oS.K[i,i] - oS.K[j,j]</span><br><span class="line">        <span class="keyword">if</span> eta &gt;= <span class="number">0</span>: </span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 通过公式计算新的alpha值</span></span><br><span class="line">        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">        <span class="comment"># 限定新alpha值的范围</span></span><br><span class="line">        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)</span><br><span class="line">        <span class="comment"># 通过新的alpha值更新预测误差</span></span><br><span class="line">        updateEk(oS, j)</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">abs</span>(oS.alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>): </span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 根据其中一个alpha的值，带入公式更新另一个alpha的值</span></span><br><span class="line">        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])</span><br><span class="line">        <span class="comment"># 更新另一个alpha值的缓存</span></span><br><span class="line">        updateEk(oS, i)</span><br><span class="line">        <span class="comment"># 计算两个alpha值的b</span></span><br><span class="line">        <span class="comment"># 核函数推导：oS.K[i,j] = dataMatrix[i,:]*dataMatrix[j,:].T</span></span><br><span class="line">        <span class="comment"># 核函数推导：oS.K[i,i] = dataMatrix[i,:]*dataMatrix[i,:].T</span></span><br><span class="line">        <span class="comment"># 核函数推导：oS.K[j,j] = dataMatrix[j,:]*dataMatrix[j,:].T</span></span><br><span class="line">        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]</span><br><span class="line">        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]</span><br><span class="line">        <span class="comment"># 根据参数b的选取条件确定算法中b的值</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> &lt; oS.alphas[i]) <span class="keyword">and</span> (oS.C &gt; oS.alphas[i]): </span><br><span class="line">            oS.b = b1</span><br><span class="line">        <span class="keyword">elif</span> (<span class="number">0</span> &lt; oS.alphas[j]) <span class="keyword">and</span> (oS.C &gt; oS.alphas[j]): </span><br><span class="line">            oS.b = b2</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            oS.b = (b1 + b2)/<span class="number">2.0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># smo算法的实现函数</span></span><br><span class="line"><span class="comment"># dataMatIn：数据集特征列表</span></span><br><span class="line"><span class="comment"># classLabels：数据集对应分类列表</span></span><br><span class="line"><span class="comment"># C：算法推导中出现的常数C，也是最终目标函数中约束条件中出现的参数C</span></span><br><span class="line"><span class="comment"># toler：容错率，理解为允许的机器误差吧</span></span><br><span class="line"><span class="comment"># maxIter：最大循环次数</span></span><br><span class="line"><span class="comment"># kTup：核函数中使用到的值，0为径向基函数中的自定义变量sigma</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">smoP</span>(<span class="params">dataMatIn, classLabels, C, toler, maxIter,kTup=(<span class="params"><span class="string">&#x27;lin&#x27;</span>, <span class="number">0</span></span>)</span>):</span><br><span class="line">    <span class="comment"># 初始化一个跟算法相关的参数（数据）结构实体</span></span><br><span class="line">    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler, kTup)</span><br><span class="line">    <span class="comment"># 记录当前循环次数</span></span><br><span class="line">    <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 选择第一个alpha用那种方式</span></span><br><span class="line">    entireSet = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否有alpha的值被更新</span></span><br><span class="line">    alphaPairsChanged = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 同时满足两种情况会继续循环迭代，</span></span><br><span class="line">    <span class="comment"># 1、循环次数不能超过设置的最大循环次数</span></span><br><span class="line">    <span class="comment"># 2、alphaPairsChanged大于0或者entireSet为真</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">iter</span> &lt; maxIter) <span class="keyword">and</span> ((alphaPairsChanged &gt; <span class="number">0</span>) <span class="keyword">or</span> (entireSet)):</span><br><span class="line">        <span class="comment"># 将alpha的是否被更新初始化为否</span></span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 如果entireSet值为真，采用所有数据集遍历的方式找到第一个alpha的值</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:</span><br><span class="line">            <span class="comment"># 遍历所有数据集</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(oS.m):</span><br><span class="line">                <span class="comment"># 完成alpha值的更新</span></span><br><span class="line">                alphaPairsChanged += innerL(i,oS)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 寻找数据集在边界上的所有数据点，使用乘法可以剔除那些分类正确的点</span></span><br><span class="line">            <span class="comment"># 该函数返回一个不为0的数据列表，列表项为之前列表项的索引值</span></span><br><span class="line">            nonBoundIs = nonzero((oS.alphas.A &gt; <span class="number">0</span>) * (oS.alphas.A &lt; C))[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 遍历边界内的点</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> nonBoundIs:</span><br><span class="line">                <span class="comment"># 完成alpha值的更新</span></span><br><span class="line">                alphaPairsChanged += innerL(i,oS)</span><br><span class="line">        <span class="comment"># 递增循环次数</span></span><br><span class="line">        <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 如果当前循环中取第一个alpha的方式是全样本集遍历，那么设置下次循环为边界内数据点遍历</span></span><br><span class="line">        <span class="keyword">if</span> entireSet: </span><br><span class="line">            entireSet = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 如果当前循环中取第一个alpha的方式是边界内数据点遍历并且没有更新到alpha，那么设置下次为全样本集遍历</span></span><br><span class="line">        <span class="keyword">elif</span> (alphaPairsChanged == <span class="number">0</span>): </span><br><span class="line">            entireSet = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 最终计算出b的值和alpha列表</span></span><br><span class="line">    <span class="keyword">return</span> oS.b,oS.alphas</span><br><span class="line"><span class="comment"># 测试算法函数，k1:径向基函数中的自定义变量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">testRbf</span>(<span class="params">k1=<span class="number">1.3</span></span>):</span><br><span class="line">    <span class="comment"># 从文件中加载训练样本集，dataArr：样本数据集的特征数据，labelArr：数据集中数据对应真实类别</span></span><br><span class="line">    dataArr,labelArr = loadDataSet(<span class="string">&#x27;testSetRBF.txt&#x27;</span>)</span><br><span class="line">    <span class="comment"># 调用smo算法函数计算参数b和alphas的值</span></span><br><span class="line">    b,alphas = smoP(dataArr, labelArr, <span class="number">200</span>, <span class="number">0.0001</span>, <span class="number">10000</span>, (<span class="string">&#x27;rbf&#x27;</span>, k1))</span><br><span class="line">    <span class="comment">### 算法测试部分 ###</span></span><br><span class="line">    datMat=mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    <span class="comment"># 所有支持向量的矩阵索引</span></span><br><span class="line">    svInd=nonzero(alphas.A&gt;<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 所有支持向量的样本数据</span></span><br><span class="line">    sVs=datMat[svInd]</span><br><span class="line">    <span class="comment"># 所有支持向量的类别数据</span></span><br><span class="line">    labelSV = labelMat[svInd];</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;there are %d Support Vectors&quot;</span> % shape(sVs)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 样本集矩阵规格</span></span><br><span class="line">    m,n = shape(datMat)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs,datMat[i,:],(<span class="string">&#x27;rbf&#x27;</span>, k1))</span><br><span class="line">        <span class="comment"># 根据分类预测函数和w求解公式和核函数推导出的预测结果计算方法</span></span><br><span class="line">        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict)!=sign(labelArr[i]): </span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;the training error rate is: %f&quot;</span> % (<span class="built_in">float</span>(errorCount)/m)</span><br><span class="line">    <span class="comment"># 测试新的样本数据</span></span><br><span class="line">    dataArr,labelArr = loadDataSet(<span class="string">&#x27;testSetRBF2.txt&#x27;</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    datMat=mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    m,n = shape(datMat)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs,datMat[i,:],(<span class="string">&#x27;rbf&#x27;</span>, k1))</span><br><span class="line">        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict)!=sign(labelArr[i]): </span><br><span class="line">            errorCount += <span class="number">1</span>    </span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;the test error rate is: %f&quot;</span> % (<span class="built_in">float</span>(errorCount)/m)</span><br></pre></td></tr></table></figure>

<p>上面算法中用到的公式有<br>$$<br>分类预测函数\quad f(w,b)&#x3D;w^Tx+b\<br>w值求解公式\quad w&#x3D;\sum_{i&#x3D;i}^n\alpha_iy_ix_i\<br>预测误差公式\quad E_i&#x3D;f(w,b)-y_i\<br>\alpha 区间判定\quad \begin{cases}y_A&#x3D; y_B\quad L&#x3D;max(0,\alpha_B^{old}+\alpha_A^{old}-C)，H&#x3D;min(C,\alpha_2^{old}+\alpha_1^{old})\<br>y_A\neq y_B\quad L&#x3D;max(0,\alpha_B^{old}-\alpha_A^{old})，H&#x3D;min(C,C+\alpha_2^{old}-\alpha_1^{old})\end{cases}\<br>\eta 求解公式\quad \eta&#x3D;2x_A^Tx_B-x_A^Tx_A-x_B^Tx_B\<br>新\alpha计算公式\quad \alpha_B^{new} &#x3D; \alpha_B^{old}-\frac{y_B(E_A-E_B)}{\eta}\<br>计算另一个\alpha公式推导\quad \alpha_A^{new}y_A+\alpha_B^{new}y_B&#x3D;\alpha_A^{old}y_A+\alpha_B^{old}y_B\<br>两个新b值的取值\quad \begin{cases}b_A^{new}&#x3D;b^{old}-E_A-y_A(\alpha_A^{new}-\alpha_A^{old})x_A^Tx_A-y_B(\alpha_B^{new}-\alpha_B^{old})x_A^Tx_B\<br>b_B^{new}&#x3D;b^{old}-E_B-y_A(\alpha_A^{new}-\alpha_A^{old})x_A^Tx_B-y_B(\alpha_B^{new}-\alpha_B^{old})x_B^Tx_B\end{cases}\<br>b值取值\quad b&#x3D; \begin{cases}<br>b_A, &amp; {if \quad 0\le \alpha_A^{new}\le C} \<br>b_B, &amp; {if \quad 0\le \alpha_B^{new}\le C}\<br>(b_A+b_B)&#x2F;2 &amp;others<br>\end{cases}<br>$$</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://vnicl.github.io/2017/05/04/Logistic%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Iceberg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="攻城狮也文艺">
      <meta itemprop="description" content="一个理想主义者 · 空想家 · LOSER">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 攻城狮也文艺">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/04/Logistic%E5%9B%9E%E5%BD%92/" class="post-title-link" itemprop="url">Logistic回归</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-04 14:27:47" itemprop="dateCreated datePublished" datetime="2017-05-04T14:27:47+08:00">2017-05-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2017-05-09 11:32:22" itemprop="dateModified" datetime="2017-05-09T11:32:22+08:00">2017-05-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.1k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  Logistic回归算法是一个求最优解的算法，算法的目标是训练一个非线性函数用于分类判定。</p>
<p>  回归的定义是把平面上的一系列点用一条线对这些点进行拟合，这个拟合的过程叫做<code>回归</code>，这条线就做<code>最佳拟合线</code>。拟合的定义是把平面上一系列点用一条线连接起来。利用Logistic回归算法进行分类是根据现有数据对分类边界建立回归公式，且算法需要找到一个最佳拟合参数用来分类判定，所以Logistic回归算法在算法训练过程中，主要就是为了找到最佳的分类回归系数。</p>
<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><hr>
<p>  为了实现算法分类判定，我们需要有一个函数能接受所有的样本特征输入，然后预测出样本类别，在二分类判定中，我们要求该函数输出0或1，这种函数被称作<code>海维塞德阶跃函数</code>或<code>单位阶跃函数</code>，<code>Sigmoid</code>函数满足我们的要求，Sigmoid函数公式为：<br>$$<br>\sigma(z) &#x3D; \frac1{1+e^{-z}}<br>$$</p>
<center><img data-src="/2017/05/04/Logistic%E5%9B%9E%E5%BD%92/sigmoid.jpg" class="" title="可以看到将横坐标尺度放大时，Sigmoid函数很像一个阶跃函数"></center>

<p>  所以为了实现Logistic回归分类器，我们可以在每一个特征上乘以一个回归系数，然后把所有结果相加并带入Sigmoid函数中得到一个0～1之间的数值，任何大于0.5的数据被分为1类，小于0.5则属于0类，即Sigmoid的输入公式为<br>$$<br>z&#x3D;w_0 + w_1x_1 + w_2x_2 + \cdots + w_ix_i<br>$$<br>这里将样本特征$x$个数定义为$i$，为了方便计算，将特征$x_0$的值设置为1，则上述公式可以表示为<br>$$<br>\begin{align}<br>z<br>&amp;&#x3D; w_0 + w_1x_1 + w_2x_2 + \cdots + w_ix_i \<br>&amp;&#x3D; w_0x_0 + w_1x_1 + w_2x_2 + \cdots + w_ix_i \<br>&amp;&#x3D; w^Tx<br>\end{align}<br>$$<br>因此可以推导出Logistic回归算法中的预测函数<br>$$<br>h(x_i) &#x3D; \sigma(w^Tx) &#x3D; \frac1{1+e^{-w^Tx}}<br>$$<br>  上面确定了分类器的函数形式，但是里面有个回归系数$w$，现在我们需要基于最优化方法来确定最佳回归系数来对样本数据做最佳拟合。</p>
<h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><hr>
<p>  在计算最佳回归系数的时候会用到<code>最大似然估计</code>方法，所以这里先介绍这个概念。</p>
<p>  现在已经拿到了很多个样本，这些样本值已经实现，最大似然估计就是去找到那个参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。通俗一点的解释就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</p>
<p>  比如一个麻袋里有白球与黑球，但是我不知道它们之间的比例，那我就有放回的抽取10次，结果我发现我抽到了8次黑球2次白球，我要求最有可能的黑白球之间的比例时，就采取最大似然估计法：我假设我抽到黑球的概率为$p$,那得出8次黑球2次白球这个结果的概率为：$P&#x3D;p^8(1-p)^2$，现在我想要得出p是多少，使得P最大的p就是我要求的结果。</p>
<p>  同理，在Logistic回归中，对单个样本分类为0或1的概率表示为<br>$$<br>p(y_1&#x3D;1|x_1) &#x3D; \sigma(z_1)<br>$$</p>
<p>$$<br>p(y_1&#x3D;0|x_1) &#x3D; 1-\sigma(z_1)<br>$$</p>
<p>用一个公式表示产生该样本的概率为<br>$$<br>p(y_1|x_1) &#x3D;\sigma(z_1)^{y_1}(1-\sigma(z_1))^{1-y_1}<br>$$<br>因为$y_1$表示0或1两个分类结果，所以$y_1$和$1-y_1$表示指定产生概率只有产生和没产生两种。根据上面描述的最大似然估计和单样本产生概率公式，可以得出生成当前样本集的最大似然估计函数，定义样本数量为$n$<br>$$<br>\begin{align}<br>L(z_i)<br>&amp;&#x3D; p(y_1|x_1)p(y_2|x_2)p(y_3|x_3) \cdots p(y_n|x_n) \<br>&amp;&#x3D; \prod_{i&#x3D;1}^np(y_i|x_i) \<br>&amp;&#x3D; \prod_{i&#x3D;1}^n(\sigma(z_i))^{y_i}(1-\sigma(z_i))^{1-y_i}<br>\end{align}<br>$$<br>为了方便计算，对$L(z_i)$取对数，这样连乘积变成了线性加总，且不会改变极值的位置（前面朴素贝叶斯算法中也有用到）<br>$$<br>\begin{align}<br>l(z_i)<br>&amp;&#x3D; logL(z_i) \<br>&amp;&#x3D;log(\sigma(z_i)^{y_i}(1-\sigma(z_i))^{1-y_i}) \<br>&amp;&#x3D; \sum_{i&#x3D;1}^n(y_ilog(\sigma(z_i))+(1-y_i)log(1-\sigma(z_i)))<br>\end{align}<br>$$<br>  现在我们知道通过计算最大似然估计函数$l(z_i)$就表示回归算法会更接近于当前样本的实际分类状态，根据公式中输入参数$z_i$的计算公式<br>$$<br>z&#x3D;w^Tx<br>$$<br>所以我们重新更新最大似然估计函数$l(z_i)$为$l(w_i)$<br>$$<br>\begin{align}<br>l(w_i)<br>&amp;&#x3D;l(z_i)  \<br>&amp;&#x3D; l(w_i^Tx_i) \<br>&amp;&#x3D; \sum_{i&#x3D;1}^n(y_ilog(\sigma(w_i^Tx_i))+(1-y_i)log(1-\sigma(w_i^Tx_i)))<br>\end{align}<br>$$<br>可以知道，在输入$x$（样本集样本）不变情况下，我们需要改变$w$的取值使得$l(w_i)$达到我们要求的最大，所以又回到训练最佳回归系数的问题。</p>
<h2 id="梯度上升算法"><a href="#梯度上升算法" class="headerlink" title="梯度上升算法"></a>梯度上升算法</h2><hr>
<p>  梯度上升算法是计算极值的一个有效方法，因为我们需要计算最大似然估计函数$l(w_i)$的最大值，所以我们采用沿着$l(w_i)$的梯度上升方向迭代，直到达到某个停止条件或设定的迭代次数，然后影响$l(w_i)$取值的$w_i$即是我们要求的最佳回归系数。下面先介绍梯度的概念。</p>
<p>  梯度是一个立体概念，通俗点讲就好比有一些层峦叠嶂的山，而你要从山脚走到山顶，常规的走法就是你会先选择往比当前位置海拔更高一点的方向走，最后一直走到山顶结束，这就是梯度上升（当然还有梯度下降）。现在可以在脑海中构造梯度的图形，因为$w_i$取值的迭代变化，所以会产生很多大小不同的$l(w_i)$，把这些$l(w_i)$看成在立体坐标系中一系列点，把这些点连接起来之后就会生成一个梯度图像，这时候找到最大的$l(w_i)$取值就和上山是一个问题。</p>
<p>  梯度上升算法到达每个点后都会重新估计移动的方向，并沿新的梯度方向移动，这样循环移动直到满足停止条件。</p>
<center><img data-src="/2017/05/04/Logistic%E5%9B%9E%E5%BD%92/tidu.png" class="" title="梯度概念"></center >

<p>  梯度上升算法的迭代公式为<br>$$<br>w:&#x3D;w+\alpha \nabla_wf(w)<br>$$<br>公式中$w$即最佳回归系数，$\alpha$表示为上升算法中的移动步长即移动量的大小。在数学中，函数$f(x,y)$的梯度表示为<br>$$<br>\nabla f(x, y) &#x3D;<br>\begin{pmatrix}<br>     \frac {\partial f(x, y)}{\partial x} \<br>     \frac {\partial f(x, y)}{\partial y}<br>\end{pmatrix}<br>$$<br>即沿$x$的方向移动$\frac {\partial f(x, y)}{\partial x}$，沿$y$的方向移动$\frac {\partial f(x, y)}{\partial y}$。所以梯度上升算法的迭代公式推导为<br>$$<br>w:&#x3D;w+\alpha \frac {\partial f(w)}{\partial w}<br>$$<br>在Logistic回归中，$f(w)$函数为上面的最大似然估计函数$l(w_i)$<br>$$<br>l(w_i) &#x3D; \sum_{i&#x3D;1}^n(y_ilog(\sigma(w_i^Tx_i))+(1-y_i)log(1-\sigma(w_i^Tx_i)))<br>$$<br>所以公式推导为<br>$$<br>\begin{align}<br>\frac {\partial f(w_i)}{\partial w_i}<br>&amp;&#x3D;\frac {\partial}{\partial w_i} l(w_i)\<br>&amp;&#x3D;\sum_{i&#x3D;1}^n(y_i \frac1{\sigma(w_i^Tx_i)}\frac {\partial}{\partial w_i}\sigma(w_i^Tx_i)-(1-y_i)\frac 1{1-\sigma(w_i^Tx_i)} \frac \partial{\partial w_i}\sigma(w_i^Tx_i)) \<br>&amp;&#x3D;\sum_{i&#x3D;1}^n(y_i \frac1{\sigma(w_i^Tx_i)}-(1-y_i)\frac 1{1-\sigma(w_i^Tx_i)})\frac \partial{\partial w_i}\sigma(w_i^Tx_i) \<br>&amp;&#x3D;\sum_{i&#x3D;1}^n(y_i \frac1{\sigma(w_i^Tx_i)}-(1-y_i)\frac 1{1-\sigma(w_i^Tx_i)}) \sigma(w_i^Tx_i)(1-\sigma(w_i^Tx_i))\frac \partial{\partial w_i} w_i^Tx_i\<br>&amp;&#x3D;\sum_{i&#x3D;1}^n(y_i(1-\sigma(w_i^Tx_i))-(1-y_i)\sigma(w_i^Tx_i))x_i \<br>&amp;&#x3D;\sum_{i&#x3D;1}^n(y_i-\sigma(w_i^Tx_i))x_i<br>\end{align}<br>$$<br>即将输入值$x_i$看作常量的情况下对$w_i$求偏导。所以算法迭代公式更新为<br>$$<br>\begin{align}<br>w: &amp;&#x3D;w+\alpha \frac {\partial f(w)}{\partial w} \<br>&amp;&#x3D;w + \alpha \sum_{i&#x3D;1}^n(y_i-\sigma(w_i^Tx_i))x_i<br>\end{align}<br>$$</p>
<h2 id="梯度上升算法的实现"><a href="#梯度上升算法的实现" class="headerlink" title="梯度上升算法的实现"></a>梯度上升算法的实现</h2><hr>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>  算法实现中会使用到矩阵运算，所以先导入<code>NumPy</code>函数库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>

<p>  首先我们需要构造一个样本集数据的列表和一个和样本集样本对应的分类列表<br>$$<br>dataSet &#x3D;<br>\begin{pmatrix}<br>1 &amp; 2 &amp; 3 \<br>\cdots &amp; \cdots &amp; \cdots \<br>1 &amp; 4 &amp; 1 \<br>\end{pmatrix},<br>dataLabel &#x3D; \begin{pmatrix}<br>1,\cdots,0<br>\end{pmatrix}<br>$$<br>为了方便计算，并且构造的数据符合预测函数$\sigma(w^Tx)$需要的输入要求，所以样本集中开始的特征值始终为1，即$w_0&#x3D;1$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataSet = array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], ...,[<span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>]])</span><br><span class="line">dataLabel = [<span class="number">1</span>, ..., <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>  接下来我们做算法训练求最佳回归系数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将样本集数据dataSet转换为NumPy中的矩阵格式</span></span><br><span class="line">dataMat = mat(dataSet)</span><br></pre></td></tr></table></figure>

<p>$$<br>dataMat &#x3D;<br>\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \<br>\cdots &amp; \cdots &amp; \cdots \<br>1 &amp; 4 &amp; 1 \<br>\end{bmatrix}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 样本集矩阵的规格</span></span><br><span class="line">m, n = shape(dataMat)</span><br><span class="line"><span class="comment"># 设置在梯度迭代中的移动步长，即每次迭代的移动距离</span></span><br><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line"><span class="comment"># 设置最大迭代次数</span></span><br><span class="line">maxCycles = <span class="number">500</span></span><br><span class="line"><span class="comment"># 创建一个初始的回归系数，默认为1，回归系数的尺寸和样本集中样本的特征数量相同</span></span><br><span class="line">weights = ones((n, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>$$<br>weights&#x3D;\begin{bmatrix}<br>1 \<br>1 \<br>1<br>\end{bmatrix}<br>$$</p>
<p>  接下来我们实现梯度上升的迭代求出最佳回归系数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(maxCycles):</span><br><span class="line">    h = <span class="number">1.0</span>/(<span class="number">1</span>+exp(-dataMat*weights))</span><br><span class="line">    weights = weights + alpha * (mat(dataLabel).transpose() - h) * dataMat.transpose()</span><br></pre></td></tr></table></figure>

<p>其中<code>1.0/(1+exp(-dataMat*weights))</code>是迭代过程中，使用迭代当前的回归系数和样本特征值输入到预测函数$\frac1{1+e^{-w^Tx}}$做结果预测，且$w^Tx&#x3D;w_0 + w_1x_1 + w_2x_2 + \cdots + w_ix_i$，计算$w^Tx$使用矩阵计算很容易实现，过程如下<br>$$<br>\begin{align}<br>dataMat \times weights<br>&amp;&#x3D;<br>\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \<br>\cdots &amp; \cdots &amp; \cdots \<br>1 &amp; 4 &amp; 1 \<br>\end{bmatrix}<br>\times<br>\begin{bmatrix}<br>1 \<br>1 \<br>1<br>\end{bmatrix}<br>\<br>&amp;&#x3D;<br>\begin{bmatrix}<br>1\times1+2\times1+3\times1 \<br>\cdots  \<br>1\times1+4\times1+1\times1 \<br>\end{bmatrix}<br>\<br>\end{align}<br>$$<br>其中<code>weights + alpha * (mat(dataLabel).transpose() - h) * dataMat.transpose()</code>则完全是根据梯度迭代公式计算<br>$$<br>w: &#x3D; w + \alpha (y_i-\sigma(w_i^Tx_i))x_i^T<br>$$<br>迭代完成之后<code>weights</code>的值即为我们所求的最佳回归系数。</p>
<h2 id="算法收敛"><a href="#算法收敛" class="headerlink" title="算法收敛"></a>算法收敛</h2><hr>
<p>  收敛是判断一个优化算法好坏的方法，就是说该算法的参数是否达到了稳定值。通俗的讲就是在做最优化算法的时候，算法要找到一个最优解需要一个过程，在Logistic回归算法计算最佳回归系数的时候，回归系数会由一个最差的回归系数慢慢变成一个最佳回归系数，并且由于一般情况下数据集中的样本并不是线性可分的，在算法训练过程中回归系数的值还会有数据的波动或高频波动产生，但是回归系数的值在迭代更新一定次数后会趋于一个稳定值，尽管还会有稍许数据波动产生也是可以接受范围，这个时候我们就说算法已经收敛，这个趋于稳定的值就是我们所说的最佳回归系数。</p>
<p>  在算法训练后观察优化算法收敛的状态可以让我们找到如何优化和完善算法的方向和评估算法的好坏。为了可以直观的看到算法收敛，我们需要在坐标系中，将$x$轴作为系数更新次数，$y$轴表示为某个特征系数的值，就会绘制出一条算法收敛示意图。</p>
<p>  知道了收敛是判断一个优化算法的好坏，因此我们在做优化和改善算法的时候，我们的目标就是使得优化算法可以更好的收敛，即我们需要使收敛速度更快和数据波动或高频波动更少。</p>
<p>  收敛速度的可控因素是算法训练中$\alpha$的值，即控制梯度上升中每次移动的步长。最好的方法是在梯度上升中使得回归系数的每次迭代更新中$\alpha$的值随着迭代次数的增加不断下降，即将梯度上升中的移动速度慢慢放缓。因为刚开始的迭代中系数的波动都比较大且不是我们需要计算的最佳回归系数状态，所以可以加快通过这个波动的速度。不过值得注意的是，需要保证$\alpha$的值永远不会减小到0，也就是需要保证在每次迭代更新后的新数据任然具有一定的影响。</p>
<h2 id="随机梯度上升算法"><a href="#随机梯度上升算法" class="headerlink" title="随机梯度上升算法"></a>随机梯度上升算法</h2><hr>
<p>  梯度上升算法中，在更新回归系数的迭代过程中，因为矩阵相乘的关系每次迭代都会遍历整个数据集，如果定义训练数据集中有$m$个样本，一个数据样本中有$n$个特征，算法训练中被迭代了$t$次，那么梯度上升算法的运算量为<br>$$<br>c &#x3D; n \times m \times t<br>$$<br>如果样本集中样本数量非常大的时候，梯度上升算法的运算复杂度就会很高，并且直接影响算法训练的效率。</p>
<p>  基于上述原因，所以我们引入随机梯度上升算法，随机梯度上升算法是梯度上升算法的优化版本。随机梯度上升算法和梯度上升算法的不同在于梯度上升算法是每次迭代都将样本集中所有样本用于更新回归系数计算，而随机梯度上升算法是每次迭代都只使用样本集中的一条没有参与计算的新样本进行计算，是增量式的更新，这是一个<code>在线学习</code>的算法，而梯度上升算法是一个<code>批处理</code>的算法。正是随机梯度上升算法这个不同，大大降低了算法训练过程的计算复杂度和训练效率。</p>
<h2 id="随机梯度上升算法的实现"><a href="#随机梯度上升算法的实现" class="headerlink" title="随机梯度上升算法的实现"></a>随机梯度上升算法的实现</h2><hr>
<h3 id="Python-1"><a href="#Python-1" class="headerlink" title="Python"></a>Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># 构造样本集，这里需使用NumPy数组</span></span><br><span class="line">dataSet = array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], ...,[<span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment"># 样本集对应的样本类别</span></span><br><span class="line">dataLabel = [<span class="number">1</span>, ..., <span class="number">0</span>]</span><br><span class="line"><span class="comment"># 样本集数据的规格</span></span><br><span class="line">m, n = shape(dataSet)</span><br><span class="line"><span class="comment"># 创建一个初始的回归系数，默认为1，回归系数的尺寸和样本集中样本的特征数量相同</span></span><br><span class="line">weights = ones((n, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 设置循环迭代的次数</span></span><br><span class="line">maxCycles = <span class="number">200</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(maxCycles):</span><br><span class="line">    dataIndex = <span class="built_in">range</span>(m)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="comment"># 每次迭代的移动步进距离，逐渐减小且不可能等于0</span></span><br><span class="line">        alpha = <span class="number">4</span>/(<span class="number">1.0</span>+k+i)+<span class="number">0.01</span></span><br><span class="line">        randIndex = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, <span class="built_in">len</span>(dataIndex)))</span><br><span class="line">        <span class="comment"># 这里没有使用矩阵乘法，所有需要用sum函数计算sigmoid的输入值</span></span><br><span class="line">        h = <span class="number">1.0</span> / (<span class="number">1</span> + exp(-<span class="built_in">sum</span>(dataSet[randIndex]*weights)))</span><br><span class="line">        weights = weights + alpha * (classLabel[randIndex] - h) * dataSet[randIndex]</span><br><span class="line">        <span class="keyword">del</span>(dataIndex[randIndex])</span><br></pre></td></tr></table></figure>

<p>算法实现中<code>alpha = 4/(1.0+k+i)+0.01</code>可以根据实际样本集和训练情况做修改，<code>+0.01</code>为保证<code>alpha</code>不会等于0。在算法训练中采用增量更新，每次随机获取数据集中的一个样本输入并删除，保证不会重复输入相同的样本。</p>
<h2 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h2><hr>
<p>  Logistic回归算法在二分类的分类预测中，因为算法的过程是对已知分类的样本集做拟合的过程，所以最终可以生成一条线叫做最佳拟合线，也叫决策边界，这条线将所有的样本内容一分为二，一部分属于分类1，则另一部分属于分类2。为了更直观的展示算法训练效果，那么如何确定和在坐标系中画出这条线呢？</p>
<p>  我们已经通过算法训练出了最佳回归系数，并且通过Sigmoid函数的曲线可以看出，预测函数的边界在0的位置，在坐标系中，横坐标0两边的部分分别表示两个不同的分类，所以设定边界函数为<br>$$<br>0&#x3D;w_0+w_1x_1 + w_2x_2 + \cdots + w_nx_n<br>$$<br>由于$x_0&#x3D;0$，所以$0&#x3D;w_0+w_1x_1+w_2x_2$，解出$x_2$和$x_1$的关系为<br>$$<br>x_1 &#x3D; \frac {-w_0-w_1 * x_2}{w_2}<br>$$<br>在坐标系中，将一系列$(x_1,x_2)$的点用线连接起来，这条线即算法在该样本集中的决策边界。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2009 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Iceberg</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9taXN0Lw==">NexT.Mist</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.umd.js" integrity="sha256-+2+qOqR8CKoHh/AsVR9k2qaDBKWjYNC2nozhYmv5j9k=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
